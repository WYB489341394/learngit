{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Summary of the file.\\n\\n    功能：\\n        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以KNN方法划分并获得准确率，通过混淆矩阵标示结果。\\n\\n    输出：\\n        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "''' Summary of the file.\n",
    "\n",
    "    功能：\n",
    "        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以KNN方法划分并获得准确率，通过混淆矩阵标示结果。\n",
    "\n",
    "    输出：\n",
    "        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# # GIST特征值\\nsubfeatures=pd.read_csv(r'E:\\test\\\\process_gist\\\\gist_f_train_full.csv',header=None)\\nlabels = pd.read_csv(r'E:\\test\\\\process_gist\\\\CNO_full.txt',header=None)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实验室的15000条恶意样本，分为十个家族\n",
    "# # LBP特征值\n",
    "subfeatures=pd.read_csv(r'F:\\virtus_test\\Simhash_LBP\\feature_train_full.csv',header=None)\n",
    "labels = pd.read_csv(r'F:\\virtus_test\\Simhash_LBP\\CNO_full.txt',header=None)\n",
    "\n",
    "'''\n",
    "# # GIST特征值\n",
    "subfeatures=pd.read_csv(r'E:\\test\\process_gist\\gist_f_train_full.csv',header=None)\n",
    "labels = pd.read_csv(r'E:\\test\\process_gist\\CNO_full.txt',header=None)\n",
    "'''\n",
    "# # text段代码提取二进制代码，生成图像的LBP特征\n",
    "# subfeatures=pd.read_csv('/home/stack/Data/Output/lbp_only_text/feature_train_full.csv',header=None)\n",
    "# labels = pd.read_csv('/home/stack/Data/Output/lbp_only_text/CNO_full.txt',header=None)\n",
    "\n",
    "# text段 GIST特征\n",
    "##subfeatures=pd.read_csv(r'E:\\test\\process_gist\\gist_f_train_full.csv',header=None)\n",
    "##labels = pd.read_csv(r'E:\\test\\process_gist\\CNO_full.txt',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6    \\\n",
      "1479   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1480   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1481   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1482   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1483   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1484   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1485   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1486   0.001674  0.000186  0.000558  0.000062  0.000016  0.000000  0.000031   \n",
      "1487   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1488   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1489   0.003751  0.000418  0.001457  0.000155  0.000263  0.000062  0.000108   \n",
      "1490   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1491   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1492   0.001720  0.000170  0.000605  0.000062  0.000047  0.000000  0.000062   \n",
      "1493   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1494   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1495   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1496   0.009703  0.001038  0.002387  0.000341  0.000728  0.000279  0.000310   \n",
      "1497   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1498   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1499   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1500   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1501   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1502   0.001612  0.000186  0.000248  0.000047  0.000077  0.000000  0.000062   \n",
      "1503   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1504   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1505   0.004464  0.000325  0.001411  0.000186  0.000202  0.000047  0.000140   \n",
      "1506   0.001612  0.000186  0.000248  0.000047  0.000077  0.000000  0.000062   \n",
      "1507   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1508   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "14716  0.001131  0.000093  0.000108  0.000016  0.000124  0.000000  0.000108   \n",
      "14717  0.000248  0.000000  0.000016  0.000031  0.000016  0.000000  0.000000   \n",
      "14718  0.001504  0.000140  0.000233  0.000031  0.000155  0.000062  0.000031   \n",
      "14719  0.001845  0.000186  0.000356  0.000031  0.000077  0.000000  0.000031   \n",
      "14720  0.001348  0.000233  0.000248  0.000016  0.000124  0.000031  0.000000   \n",
      "14721  0.001457  0.000124  0.000202  0.000047  0.000155  0.000031  0.000031   \n",
      "14722  0.000853  0.000108  0.000233  0.000016  0.000016  0.000000  0.000016   \n",
      "14723  0.000930  0.000108  0.000170  0.000031  0.000077  0.000016  0.000031   \n",
      "14724  0.001457  0.000124  0.000170  0.000031  0.000062  0.000047  0.000031   \n",
      "14725  0.001147  0.000062  0.000140  0.000031  0.000155  0.000000  0.000016   \n",
      "14726  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "14727  0.001627  0.000186  0.000202  0.000016  0.000077  0.000031  0.000016   \n",
      "14728  0.001147  0.000062  0.000140  0.000031  0.000155  0.000000  0.000016   \n",
      "14729  0.000093  0.000000  0.000000  0.000000  0.000016  0.000031  0.000000   \n",
      "14730  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "14731  0.000992  0.000093  0.000124  0.000016  0.000124  0.000016  0.000031   \n",
      "14732  0.000728  0.000031  0.000108  0.000031  0.000047  0.000016  0.000031   \n",
      "14733  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "14734  0.001457  0.000124  0.000170  0.000031  0.000062  0.000047  0.000031   \n",
      "14735  0.000170  0.000016  0.000016  0.000016  0.000000  0.000000  0.000000   \n",
      "14736  0.000170  0.000016  0.000016  0.000016  0.000000  0.000000  0.000000   \n",
      "14737  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "14738  0.001457  0.000124  0.000170  0.000031  0.000062  0.000047  0.000031   \n",
      "14739  0.001240  0.000186  0.000170  0.000016  0.000077  0.000031  0.000000   \n",
      "14740  0.001333  0.000140  0.000233  0.000031  0.000093  0.000000  0.000000   \n",
      "14741  0.001504  0.000140  0.000233  0.000031  0.000155  0.000062  0.000031   \n",
      "14742  0.000140  0.000031  0.000093  0.000016  0.000031  0.000000  0.000000   \n",
      "14743  0.000356  0.000031  0.000093  0.000047  0.000031  0.000000  0.000031   \n",
      "14744  0.001380  0.000124  0.000512  0.000031  0.000093  0.000016  0.000047   \n",
      "14745  0.001147  0.000093  0.000248  0.000016  0.000108  0.000000  0.000016   \n",
      "\n",
      "            7         8         9    ...       246  247  248  249  250  251  \\\n",
      "1479   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1480   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1481   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1482   0.000000  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1483   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1484   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1485   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1486   0.000016  0.000217  0.000031  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1487   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1488   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1489   0.000047  0.000589  0.000108  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1490   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1491   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1492   0.000000  0.000263  0.000047  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1493   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1494   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1495   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1496   0.000124  0.001287  0.000170  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1497   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1498   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1499   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1500   0.000000  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1501   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1502   0.000016  0.000310  0.000031  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1503   0.000000  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1504   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1505   0.000108  0.000574  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1506   0.000016  0.000310  0.000031  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1507   0.000000  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "1508   0.000000  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "...         ...       ...       ...  ...       ...  ...  ...  ...  ...  ...   \n",
      "14716  0.000000  0.000295  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14717  0.000000  0.000016  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14718  0.000031  0.000155  0.000016  ...  0.000016  0.0  0.0  0.0  0.0  0.0   \n",
      "14719  0.000000  0.000140  0.000062  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14720  0.000000  0.000170  0.000077  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14721  0.000016  0.000233  0.000016  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14722  0.000000  0.000186  0.000093  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14723  0.000016  0.000108  0.000016  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14724  0.000016  0.000155  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14725  0.000000  0.000170  0.000031  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14726  0.000000  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14727  0.000031  0.000155  0.000062  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14728  0.000000  0.000170  0.000031  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14729  0.000000  0.000016  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14730  0.000000  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14731  0.000000  0.000233  0.000031  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14732  0.000016  0.000077  0.000031  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14733  0.000000  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14734  0.000016  0.000155  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14735  0.000016  0.000047  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14736  0.000016  0.000047  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14737  0.000000  0.000000  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14738  0.000016  0.000155  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14739  0.000000  0.000170  0.000062  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14740  0.000031  0.000140  0.000031  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14741  0.000031  0.000155  0.000016  ...  0.000016  0.0  0.0  0.0  0.0  0.0   \n",
      "14742  0.000000  0.000124  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14743  0.000000  0.000108  0.000000  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14744  0.000016  0.000124  0.000062  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "14745  0.000016  0.000108  0.000031  ...  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "       252       253  254      255  \n",
      "1479   0.0  0.000000  0.0  0.98359  \n",
      "1480   0.0  0.000000  0.0  0.98359  \n",
      "1481   0.0  0.000000  0.0  0.98359  \n",
      "1482   0.0  0.000000  0.0  1.00000  \n",
      "1483   0.0  0.000000  0.0  0.98359  \n",
      "1484   0.0  0.000000  0.0  0.98359  \n",
      "1485   0.0  0.000000  0.0  0.98359  \n",
      "1486   0.0  0.000000  0.0  0.99386  \n",
      "1487   0.0  0.000000  0.0  0.98359  \n",
      "1488   0.0  0.000000  0.0  0.98359  \n",
      "1489   0.0  0.000000  0.0  0.98472  \n",
      "1490   0.0  0.000000  0.0  0.98359  \n",
      "1491   0.0  0.000000  0.0  0.98359  \n",
      "1492   0.0  0.000000  0.0  0.99346  \n",
      "1493   0.0  0.000000  0.0  0.98359  \n",
      "1494   0.0  0.000000  0.0  0.98359  \n",
      "1495   0.0  0.000000  0.0  0.98359  \n",
      "1496   0.0  0.000000  0.0  0.96261  \n",
      "1497   0.0  0.000000  0.0  0.98359  \n",
      "1498   0.0  0.000000  0.0  0.98359  \n",
      "1499   0.0  0.000000  0.0  0.98359  \n",
      "1500   0.0  0.000000  0.0  1.00000  \n",
      "1501   0.0  0.000000  0.0  0.98359  \n",
      "1502   0.0  0.000000  0.0  0.99402  \n",
      "1503   0.0  0.000000  0.0  1.00000  \n",
      "1504   0.0  0.000000  0.0  0.98359  \n",
      "1505   0.0  0.000000  0.0  0.98359  \n",
      "1506   0.0  0.000000  0.0  0.99402  \n",
      "1507   0.0  0.000000  0.0  1.00000  \n",
      "1508   0.0  0.000000  0.0  1.00000  \n",
      "...    ...       ...  ...      ...  \n",
      "14716  0.0  0.000000  0.0  0.99640  \n",
      "14717  0.0  0.000000  0.0  0.99940  \n",
      "14718  0.0  0.000000  0.0  0.99422  \n",
      "14719  0.0  0.000000  0.0  0.99378  \n",
      "14720  0.0  0.000000  0.0  0.99492  \n",
      "14721  0.0  0.000016  0.0  0.99467  \n",
      "14722  0.0  0.000000  0.0  0.99679  \n",
      "14723  0.0  0.000000  0.0  0.99656  \n",
      "14724  0.0  0.000000  0.0  0.99504  \n",
      "14725  0.0  0.000000  0.0  0.99586  \n",
      "14726  0.0  0.000000  0.0  1.00000  \n",
      "14727  0.0  0.000000  0.0  0.99428  \n",
      "14728  0.0  0.000000  0.0  0.99586  \n",
      "14729  0.0  0.000000  0.0  0.99957  \n",
      "14730  0.0  0.000000  0.0  1.00000  \n",
      "14731  0.0  0.000000  0.0  0.99640  \n",
      "14732  0.0  0.000000  0.0  0.99755  \n",
      "14733  0.0  0.000000  0.0  1.00000  \n",
      "14734  0.0  0.000000  0.0  0.99504  \n",
      "14735  0.0  0.000000  0.0  0.99935  \n",
      "14736  0.0  0.000000  0.0  0.99935  \n",
      "14737  0.0  0.000000  0.0  1.00000  \n",
      "14738  0.0  0.000000  0.0  0.99504  \n",
      "14739  0.0  0.000000  0.0  0.99521  \n",
      "14740  0.0  0.000000  0.0  0.99519  \n",
      "14741  0.0  0.000000  0.0  0.99422  \n",
      "14742  0.0  0.000000  0.0  0.99901  \n",
      "14743  0.0  0.000000  0.0  0.99870  \n",
      "14744  0.0  0.000000  0.0  0.99495  \n",
      "14745  0.0  0.000000  0.0  0.99552  \n",
      "\n",
      "[13267 rows x 256 columns]\n"
     ]
    }
   ],
   "source": [
    "# 删除数据量较小的数据\n",
    "result = pd.concat([subfeatures, labels], axis=1,ignore_index=True)\n",
    "'''\n",
    "print(result)\n",
    "print(result.apply(lambda x: x.sum(), axis=1))\n",
    "print(labels.apply(lambda x: x.sum(), axis=1))\n",
    "print(labels.apply(lambda x: x.sum(), axis=1)==result.apply(lambda x: x.sum(), axis=1))\n",
    "'''\n",
    "list=[]\n",
    "Q = result.apply(lambda x: x.sum(), axis=1)\n",
    "# W = labels.apply(lambda x: x.sum(), axis=1)\n",
    "for t in range(len(result)):\n",
    "    if Q[t] <= 2.5:\n",
    "        list.append(t)\n",
    "# print(list,len(list))  #打印出较小数据量的标号和个数\n",
    "result=result.drop(labels=list,axis=0)\n",
    "\n",
    "subfeatures=result.iloc[:,:-1]\n",
    "labels=result.iloc[:,-1]\n",
    "print (subfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subfeatures.fillna('0')\n",
    "\n",
    "\n",
    "# 平均准确率归零\n",
    "avgscore = 0\n",
    "recallscore = 0\n",
    "precisionscore = 0\n",
    "N = 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.92      0.78      0.84       154\n",
      "           3       0.43      0.91      0.59       145\n",
      "           4       1.00      0.98      0.99       158\n",
      "           5       0.98      0.92      0.95       136\n",
      "           6       0.94      0.99      0.96       150\n",
      "           7       0.94      0.91      0.93       150\n",
      "           8       0.98      0.29      0.45       147\n",
      "           9       0.93      0.88      0.90       145\n",
      "          10       0.95      0.84      0.89       142\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1327\n",
      "   macro avg       0.90      0.83      0.83      1327\n",
      "weighted avg       0.90      0.83      0.84      1327\n",
      "\n",
      "0.8342125094197438\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "[[120  28   0   0   0   4   0   2   0]\n",
      " [  5 132   0   0   0   3   1   1   3]\n",
      " [  0   3 155   0   0   0   0   0   0]\n",
      " [  0   2   0 125   8   0   0   0   1]\n",
      " [  0   2   0   0 148   0   0   0   0]\n",
      " [  2   5   0   0   0 137   0   6   0]\n",
      " [  0 104   0   0   0   0  43   0   0]\n",
      " [  4   7   0   1   1   2   0 128   2]\n",
      " [  0  21   0   1   0   0   0   1 119]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.88      0.81      0.84       136\n",
      "           3       0.42      0.89      0.57       136\n",
      "           4       1.00      0.99      1.00       138\n",
      "           5       0.99      0.91      0.95       151\n",
      "           6       0.91      0.99      0.95       149\n",
      "           7       0.91      0.90      0.90       158\n",
      "           8       0.96      0.33      0.49       153\n",
      "           9       0.91      0.85      0.88       157\n",
      "          10       0.98      0.82      0.89       149\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1327\n",
      "   macro avg       0.89      0.83      0.83      1327\n",
      "weighted avg       0.89      0.83      0.83      1327\n",
      "\n",
      "0.8296910324039186\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "[[110  19   0   0   0   3   1   3   0]\n",
      " [  3 121   0   0   0   5   1   3   3]\n",
      " [  0   1 137   0   0   0   0   0   0]\n",
      " [  0   0   0 137  14   0   0   0   0]\n",
      " [  0   1   0   0 148   0   0   0   0]\n",
      " [  2   7   0   0   0 142   0   7   0]\n",
      " [  0 103   0   0   0   0  50   0   0]\n",
      " [  9   9   0   0   0   5   0 134   0]\n",
      " [  1  24   0   1   0   1   0   0 122]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.45      0.93      0.61       168\n",
      "           3       0.82      0.54      0.65       134\n",
      "           4       0.99      0.96      0.98       162\n",
      "           5       0.98      0.91      0.94       138\n",
      "           6       0.92      0.98      0.95       150\n",
      "           7       0.95      0.88      0.92       161\n",
      "           8       0.94      0.24      0.39       136\n",
      "           9       0.89      0.86      0.88       137\n",
      "          10       0.97      0.91      0.94       141\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      1327\n",
      "   macro avg       0.88      0.80      0.80      1327\n",
      "weighted avg       0.87      0.81      0.81      1327\n",
      "\n",
      "0.8123587038432555\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "[[157   6   0   0   0   2   0   3   0]\n",
      " [ 53  72   0   1   0   1   1   4   2]\n",
      " [  6   0 156   0   0   0   0   0   0]\n",
      " [  0   2   0 125  11   0   0   0   0]\n",
      " [  2   0   0   1 147   0   0   0   0]\n",
      " [  7   5   0   0   0 142   0   5   2]\n",
      " [103   0   0   0   0   0  33   0   0]\n",
      " [ 10   2   1   1   0   4   1 118   0]\n",
      " [  9   1   0   0   1   0   0   2 128]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.95      0.76      0.85       148\n",
      "           3       0.82      0.55      0.66       149\n",
      "           4       0.99      0.99      0.99       158\n",
      "           5       0.64      0.99      0.78       147\n",
      "           6       0.97      0.43      0.60       138\n",
      "           7       0.94      0.91      0.93       165\n",
      "           8       0.53      0.99      0.69       129\n",
      "           9       0.87      0.82      0.84       152\n",
      "          10       0.95      0.82      0.88       141\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      1327\n",
      "   macro avg       0.85      0.81      0.80      1327\n",
      "weighted avg       0.86      0.81      0.81      1327\n",
      "\n",
      "0.8093443858327054\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "[[113   5   0   0   0   4  21   5   0]\n",
      " [  1  82   0   0   0   0  55   6   5]\n",
      " [  0   0 157   0   0   0   1   0   0]\n",
      " [  0   0   0 145   0   0   2   0   0]\n",
      " [  0   0   0  78  60   0   0   0   0]\n",
      " [  2   4   0   0   0 150   3   6   0]\n",
      " [  0   1   0   0   0   0 128   0   0]\n",
      " [  3   7   1   1   0   5  10 124   1]\n",
      " [  0   1   0   2   2   0  20   1 115]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.40      0.93      0.56       148\n",
      "           3       0.83      0.54      0.65       145\n",
      "           4       1.00      0.97      0.99       142\n",
      "           5       0.99      0.91      0.95       148\n",
      "           6       0.92      0.99      0.95       159\n",
      "           7       0.95      0.90      0.93       133\n",
      "           8       0.94      0.32      0.47       152\n",
      "           9       0.90      0.89      0.89       132\n",
      "          10       0.97      0.82      0.89       168\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1327\n",
      "   macro avg       0.88      0.81      0.81      1327\n",
      "weighted avg       0.88      0.80      0.81      1327\n",
      "\n",
      "0.8048229088168802\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "[[137   2   0   0   0   3   2   4   0]\n",
      " [ 56  78   0   1   1   1   1   3   4]\n",
      " [  4   0 138   0   0   0   0   0   0]\n",
      " [  0   0   0 135  12   1   0   0   0]\n",
      " [  2   0   0   0 157   0   0   0   0]\n",
      " [  7   2   0   0   0 120   0   4   0]\n",
      " [104   0   0   0   0   0  48   0   0]\n",
      " [ 11   3   0   0   0   1   0 117   0]\n",
      " [ 19   9   0   0   0   0   0   2 138]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.92      0.76      0.84       161\n",
      "           3       0.82      0.51      0.63       137\n",
      "           4       1.00      0.99      0.99       155\n",
      "           5       0.98      0.91      0.95       140\n",
      "           6       0.89      1.00      0.94       122\n",
      "           7       0.94      0.89      0.92       170\n",
      "           8       0.57      1.00      0.73       148\n",
      "           9       0.89      0.85      0.87       140\n",
      "          10       0.97      0.84      0.90       154\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1327\n",
      "   macro avg       0.89      0.86      0.86      1327\n",
      "weighted avg       0.89      0.86      0.86      1327\n",
      "\n",
      "0.8620949510173324\n",
      "Confusion matrix, without normalization\n",
      "\n",
      "[[123   0   0   0   0   3  27   8   0]\n",
      " [  5  70   0   0   1   3  51   3   4]\n",
      " [  0   0 153   0   0   0   2   0   0]\n",
      " [  1   0   0 128  11   0   0   0   0]\n",
      " [  0   0   0   0 122   0   0   0   0]\n",
      " [  3   6   0   0   0 152   6   3   0]\n",
      " [  0   0   0   0   0   0 148   0   0]\n",
      " [  1   5   0   1   2   4   8 119   0]\n",
      " [  0   4   0   1   1   0  18   1 129]]\n",
      "train...\n",
      "test...\n"
     ]
    }
   ],
   "source": [
    "# 进行十次KNN测试\n",
    "for i in range(N):\n",
    "    # 以10%的比例进行交叉验证\n",
    "    # X_train, X_test, y_train, y_test = cross_validation.train_test_split(subfeatures, features_labels, test_size=0.1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subfeatures, labels, test_size=0.1)\n",
    "\n",
    "    # 进行训练\n",
    "    print('train...')\n",
    "    # 进行KNN训练,距离为1\n",
    "    neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "    neigh.fit(X_train, y_train)\n",
    "\n",
    "    # 预试\n",
    "    print('test...')\n",
    "    c_test = neigh.predict(X_test)\n",
    "\n",
    "    # print(y_test)\n",
    "    # print(c_test)\n",
    "\n",
    "    # 计算预测划分准确率\n",
    "    print('accurary...')\n",
    "    score = neigh.score(X_test, y_test)\n",
    "    print(classification_report(y_test,c_test))\n",
    "    avgscore = avgscore + score\n",
    "    recallscore = recallscore + recall_score(y_test,c_test,average=\"macro\")\n",
    "    precisionscore = precisionscore + precision_score(y_test,c_test,average=\"macro\")\n",
    "    print(score)\n",
    "\n",
    "    # 通过混淆矩阵进行结果标示\n",
    "    cm = confusion_matrix(y_test, c_test)\n",
    "    np.set_printoptions(threshold=10000)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization\\n')\n",
    "    print(str(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出N次的平均准确率\n",
    "avgscore = avgscore / N\n",
    "recallscore = recallscore / N\n",
    "precisionscore = precisionscore / N\n",
    "\n",
    "print('avgscore....')\n",
    "print(avgscore)\n",
    "print('False positive rate')\n",
    "print(1-precisionscore)\n",
    "print('false negative rate')\n",
    "print(1-recallscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
