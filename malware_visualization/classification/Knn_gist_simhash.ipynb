{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Summary of the file.\\n\\n    功能：\\n        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以KNN方法划分并获得准确率，通过混淆矩阵标示结果。\\n\\n    输出：\\n        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "''' Summary of the file.\n",
    "\n",
    "    功能：\n",
    "        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以KNN方法划分并获得准确率，通过混淆矩阵标示结果。\n",
    "\n",
    "    输出：\n",
    "        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验室的15000条恶意样本，分为十个家族\n",
    "\n",
    "# # GIST特征值\n",
    "subfeatures = pd.read_csv(r'F:\\virtus_test\\Simhash_Gist\\gist_f_train_full.csv',header=None)\n",
    "labels = pd.read_csv(r'F:\\virtus_test\\Simhash_Gist\\CNO_full.txt',header=None)\n",
    "\n",
    "# # text段代码提取二进制代码，生成图像的LBP特征\n",
    "# subfeatures=pd.read_csv('/home/stack/Data/Output/lbp_only_text/feature_train_full.csv',header=None)\n",
    "# labels = pd.read_csv('/home/stack/Data/Output/lbp_only_text/CNO_full.txt',header=None)\n",
    "\n",
    "# text段 GIST特征\n",
    "# #subfeatures=pd.read_csv(r'E:\\test\\process_gist\\gist_f_train_full.csv',header=None)\n",
    "# #labels = pd.read_csv(r'E:\\test\\process_gist\\CNO_full.txt',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6    \\\n",
      "1      0.000011  0.000129  0.000012  0.000013  0.000190  0.011638  0.000046   \n",
      "3      0.085982  0.040875  0.021009  0.007505  0.091562  0.046841  0.023623   \n",
      "5      0.047440  0.016571  0.000198  0.000226  0.029116  0.015923  0.000131   \n",
      "8      0.048793  0.000449  0.000058  0.000162  0.046696  0.007090  0.000088   \n",
      "9      0.090414  0.069236  0.015272  0.027969  0.108240  0.064971  0.028188   \n",
      "10     0.105230  0.055067  0.045520  0.043418  0.146050  0.082876  0.059477   \n",
      "12     0.106120  0.052204  0.025907  0.030567  0.106140  0.064704  0.054260   \n",
      "14     0.021522  0.000341  0.000092  0.005975  0.018221  0.012244  0.000068   \n",
      "17     0.000052  0.000663  0.000042  0.000023  0.000109  0.011774  0.000069   \n",
      "21     0.073294  0.031559  0.000907  0.001161  0.033077  0.018886  0.000125   \n",
      "23     0.034921  0.000640  0.008101  0.006527  0.045931  0.007843  0.000342   \n",
      "24     0.054843  0.017804  0.021770  0.000366  0.026341  0.012451  0.020339   \n",
      "27     0.015568  0.008779  0.000222  0.007207  0.023940  0.007822  0.000156   \n",
      "28     0.014477  0.000388  0.000121  0.000169  0.008625  0.009821  0.000071   \n",
      "35     0.009722  0.010958  0.000220  0.000192  0.040150  0.015223  0.000325   \n",
      "36     0.006725  0.001589  0.000039  0.000037  0.000045  0.000018  0.000020   \n",
      "42     0.065576  0.023021  0.000537  0.007310  0.053085  0.016383  0.019325   \n",
      "43     0.076206  0.028472  0.013910  0.014231  0.061812  0.041494  0.011432   \n",
      "44     0.051666  0.021962  0.000145  0.009109  0.036048  0.041747  0.000266   \n",
      "48     0.106120  0.052204  0.025907  0.030567  0.106140  0.064704  0.054260   \n",
      "49     0.081347  0.041241  0.033649  0.016655  0.088113  0.044449  0.019023   \n",
      "50     0.049125  0.009870  0.000121  0.000140  0.037119  0.011620  0.000187   \n",
      "53     0.000565  0.011056  0.000064  0.000059  0.018768  0.000731  0.000097   \n",
      "55     0.067826  0.031716  0.008185  0.020591  0.072527  0.033175  0.015271   \n",
      "56     0.095037  0.036306  0.012970  0.007856  0.085954  0.036720  0.016928   \n",
      "57     0.012356  0.013008  0.000089  0.000091  0.000188  0.000194  0.000054   \n",
      "58     0.031085  0.008855  0.000380  0.007510  0.022150  0.009404  0.020555   \n",
      "63     0.053784  0.019715  0.000376  0.005506  0.041130  0.021092  0.007336   \n",
      "65     0.101920  0.048134  0.029147  0.023865  0.101900  0.056331  0.029307   \n",
      "71     0.067826  0.031716  0.008185  0.020591  0.072527  0.033175  0.015271   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "14711  0.027949  0.010702  0.000154  0.000241  0.026767  0.020819  0.000046   \n",
      "14713  0.008533  0.000058  0.000015  0.000081  0.007145  0.000133  0.000025   \n",
      "14714  0.032600  0.006772  0.001068  0.006106  0.026379  0.016189  0.000110   \n",
      "14715  0.016825  0.000150  0.000150  0.000247  0.000245  0.000074  0.000063   \n",
      "14716  0.029318  0.000309  0.000177  0.000292  0.032729  0.009719  0.000357   \n",
      "14717  0.019109  0.000119  0.000118  0.000204  0.000140  0.000060  0.000050   \n",
      "14718  0.027949  0.010702  0.000154  0.000241  0.026767  0.020819  0.000046   \n",
      "14719  0.027909  0.001007  0.000109  0.000281  0.033534  0.023921  0.000051   \n",
      "14720  0.036238  0.000663  0.000207  0.000222  0.022120  0.021694  0.008042   \n",
      "14721  0.032760  0.006189  0.000134  0.000210  0.021019  0.019801  0.000043   \n",
      "14722  0.021856  0.000454  0.000079  0.000165  0.024799  0.026449  0.000060   \n",
      "14723  0.025536  0.000636  0.000101  0.000198  0.014470  0.015480  0.000071   \n",
      "14724  0.029723  0.007952  0.000122  0.000195  0.021815  0.014326  0.000043   \n",
      "14725  0.032116  0.000618  0.000090  0.000231  0.017416  0.012625  0.000050   \n",
      "14727  0.027619  0.007065  0.001445  0.000269  0.022107  0.018690  0.006996   \n",
      "14728  0.032116  0.000618  0.000090  0.000231  0.017416  0.012625  0.000050   \n",
      "14729  0.006779  0.000051  0.000018  0.000084  0.000157  0.000011  0.000007   \n",
      "14731  0.024574  0.000765  0.000090  0.000185  0.014533  0.020711  0.000060   \n",
      "14732  0.027141  0.000437  0.000085  0.000172  0.009343  0.016085  0.000062   \n",
      "14734  0.029723  0.007952  0.000122  0.000195  0.021815  0.014326  0.000043   \n",
      "14735  0.016825  0.000150  0.000150  0.000247  0.000245  0.000074  0.000063   \n",
      "14736  0.016825  0.000150  0.000150  0.000247  0.000245  0.000074  0.000063   \n",
      "14738  0.029723  0.007952  0.000122  0.000195  0.021815  0.014326  0.000043   \n",
      "14739  0.020614  0.006521  0.000071  0.000159  0.027041  0.020863  0.000050   \n",
      "14740  0.026531  0.007973  0.000184  0.008164  0.017272  0.019603  0.000064   \n",
      "14741  0.027949  0.010702  0.000154  0.000241  0.026767  0.020819  0.000046   \n",
      "14742  0.006034  0.000178  0.000124  0.000180  0.011371  0.011035  0.000090   \n",
      "14743  0.022438  0.000101  0.000096  0.000147  0.011350  0.000097  0.000064   \n",
      "14744  0.030505  0.006186  0.000294  0.000194  0.014117  0.018035  0.007121   \n",
      "14745  0.026862  0.009670  0.000129  0.000269  0.024332  0.018500  0.000036   \n",
      "\n",
      "            7         8         9    ...       502       503       504  \\\n",
      "1      0.000023  0.000039  0.000209  ...  0.000188  0.000007  0.000299   \n",
      "3      0.013533  0.024425  0.012715  ...  0.026862  0.016864  0.035761   \n",
      "5      0.000250  0.000833  0.000781  ...  0.000327  0.000732  0.002077   \n",
      "8      0.000220  0.009911  0.000372  ...  0.000189  0.000659  0.013990   \n",
      "9      0.036200  0.041733  0.014951  ...  0.023332  0.035918  0.032092   \n",
      "10     0.030726  0.062322  0.020363  ...  0.050765  0.033769  0.046558   \n",
      "12     0.026473  0.034075  0.004622  ...  0.044696  0.028257  0.031096   \n",
      "14     0.001564  0.000654  0.000542  ...  0.000406  0.005033  0.001603   \n",
      "17     0.000028  0.000010  0.000287  ...  0.000281  0.000011  0.000038   \n",
      "21     0.006789  0.001120  0.007006  ...  0.000445  0.007047  0.001979   \n",
      "23     0.015700  0.010239  0.009276  ...  0.001807  0.018214  0.017054   \n",
      "24     0.000255  0.000874  0.000368  ...  0.019161  0.001285  0.000842   \n",
      "27     0.002000  0.000620  0.000614  ...  0.000544  0.006227  0.001884   \n",
      "28     0.000063  0.000230  0.000112  ...  0.000169  0.000272  0.000046   \n",
      "35     0.000175  0.001268  0.000649  ...  0.000826  0.000793  0.004916   \n",
      "36     0.000020  0.000036  0.000020  ...  0.000011  0.000012  0.000010   \n",
      "42     0.008498  0.009535  0.000944  ...  0.020944  0.013558  0.015856   \n",
      "43     0.017455  0.009667  0.009426  ...  0.017825  0.017613  0.014131   \n",
      "44     0.001196  0.000919  0.001156  ...  0.002155  0.005689  0.001368   \n",
      "48     0.026473  0.034075  0.004622  ...  0.044696  0.028257  0.031096   \n",
      "49     0.012340  0.028111  0.024457  ...  0.018858  0.016200  0.025420   \n",
      "50     0.000202  0.008295  0.008723  ...  0.000272  0.000721  0.009397   \n",
      "53     0.000105  0.000495  0.000213  ...  0.000042  0.000075  0.001323   \n",
      "55     0.007736  0.023849  0.013999  ...  0.020807  0.015783  0.032103   \n",
      "56     0.009549  0.020103  0.020933  ...  0.028060  0.020085  0.018434   \n",
      "57     0.000043  0.000099  0.000080  ...  0.000027  0.000037  0.000013   \n",
      "58     0.008573  0.007789  0.000260  ...  0.022050  0.016828  0.018070   \n",
      "63     0.001523  0.009591  0.007291  ...  0.012053  0.005878  0.012795   \n",
      "65     0.014130  0.037454  0.003673  ...  0.028023  0.020952  0.031361   \n",
      "71     0.007736  0.023849  0.013999  ...  0.020807  0.015783  0.032103   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "14711  0.000064  0.000623  0.000430  ...  0.000188  0.000605  0.000786   \n",
      "14713  0.000046  0.000290  0.000010  ...  0.000028  0.000182  0.000038   \n",
      "14714  0.001464  0.000693  0.000682  ...  0.000473  0.005186  0.000612   \n",
      "14715  0.000084  0.000128  0.000058  ...  0.000095  0.000275  0.000071   \n",
      "14716  0.000162  0.000571  0.000243  ...  0.001468  0.002030  0.000732   \n",
      "14717  0.000070  0.000073  0.000050  ...  0.000084  0.000243  0.000063   \n",
      "14718  0.000064  0.000623  0.000430  ...  0.000188  0.000605  0.000786   \n",
      "14719  0.000065  0.000765  0.000627  ...  0.000196  0.000567  0.000686   \n",
      "14720  0.000227  0.000547  0.000267  ...  0.015050  0.001600  0.000631   \n",
      "14721  0.000054  0.000452  0.000541  ...  0.000182  0.000365  0.000536   \n",
      "14722  0.000068  0.000419  0.000318  ...  0.000256  0.000301  0.000910   \n",
      "14723  0.000066  0.000417  0.000177  ...  0.000121  0.000554  0.000050   \n",
      "14724  0.000050  0.000442  0.000253  ...  0.000175  0.000374  0.000507   \n",
      "14725  0.000070  0.000565  0.000183  ...  0.000123  0.000499  0.000053   \n",
      "14727  0.000102  0.000506  0.000400  ...  0.010535  0.000670  0.000695   \n",
      "14728  0.000070  0.000565  0.000183  ...  0.000123  0.000499  0.000053   \n",
      "14729  0.000023  0.000067  0.000004  ...  0.000023  0.000156  0.000004   \n",
      "14731  0.000061  0.000522  0.000329  ...  0.000143  0.000499  0.000132   \n",
      "14732  0.000071  0.000308  0.000141  ...  0.000163  0.000260  0.000060   \n",
      "14734  0.000050  0.000442  0.000253  ...  0.000175  0.000374  0.000507   \n",
      "14735  0.000084  0.000128  0.000058  ...  0.000095  0.000275  0.000071   \n",
      "14736  0.000084  0.000128  0.000058  ...  0.000095  0.000275  0.000071   \n",
      "14738  0.000050  0.000442  0.000253  ...  0.000175  0.000374  0.000507   \n",
      "14739  0.000067  0.000617  0.000426  ...  0.000177  0.000555  0.000574   \n",
      "14740  0.000545  0.000578  0.000309  ...  0.000493  0.001765  0.000691   \n",
      "14741  0.000064  0.000623  0.000430  ...  0.000188  0.000605  0.000786   \n",
      "14742  0.000089  0.000279  0.000175  ...  0.000211  0.000438  0.000698   \n",
      "14743  0.000087  0.000340  0.000039  ...  0.000064  0.000317  0.000542   \n",
      "14744  0.000170  0.000342  0.000395  ...  0.012715  0.000780  0.000549   \n",
      "14745  0.000077  0.000577  0.000378  ...  0.000161  0.000341  0.000604   \n",
      "\n",
      "            505       506       507       508       509       510       511  \n",
      "1      0.000703  0.000028  0.000005  0.000010  0.000004  0.000001  0.000003  \n",
      "3      0.019030  0.003486  0.013007  0.030220  0.024300  0.023935  0.006006  \n",
      "5      0.001635  0.000293  0.009732  0.000421  0.000189  0.000347  0.006363  \n",
      "8      0.001652  0.000225  0.007695  0.015311  0.005524  0.000318  0.005244  \n",
      "9      0.021769  0.015602  0.015613  0.033571  0.011543  0.016102  0.009725  \n",
      "10     0.021763  0.014818  0.023718  0.043511  0.010529  0.029337  0.017198  \n",
      "12     0.008935  0.007652  0.016561  0.021731  0.006077  0.013341  0.012924  \n",
      "14     0.001324  0.000231  0.007469  0.000873  0.010255  0.000346  0.004921  \n",
      "17     0.000028  0.000006  0.000004  0.000005  0.000002  0.000003  0.000004  \n",
      "21     0.014666  0.000187  0.000232  0.001381  0.000785  0.000202  0.000444  \n",
      "23     0.015643  0.001478  0.006110  0.001029  0.003982  0.011195  0.000104  \n",
      "24     0.000811  0.000657  0.000275  0.001102  0.000605  0.000152  0.000201  \n",
      "27     0.001376  0.000281  0.009386  0.000415  0.000152  0.000323  0.006162  \n",
      "28     0.000027  0.000028  0.000035  0.000298  0.000053  0.000017  0.000113  \n",
      "35     0.003668  0.030339  0.005964  0.000084  0.000150  0.000185  0.000030  \n",
      "36     0.000034  0.000038  0.000014  0.000645  0.000708  0.000111  0.000029  \n",
      "42     0.003339  0.004410  0.011811  0.021109  0.001301  0.000362  0.004222  \n",
      "43     0.014192  0.003508  0.007671  0.011506  0.001128  0.000369  0.005384  \n",
      "44     0.002140  0.000271  0.000175  0.018861  0.005548  0.000127  0.000414  \n",
      "48     0.008935  0.007652  0.016561  0.021731  0.006077  0.013341  0.012924  \n",
      "49     0.020273  0.008980  0.011772  0.014614  0.005748  0.007459  0.013876  \n",
      "50     0.010984  0.007286  0.000331  0.000599  0.002313  0.000888  0.000086  \n",
      "53     0.000202  0.000020  0.000064  0.000016  0.000026  0.000024  0.000010  \n",
      "55     0.024032  0.001985  0.011915  0.020704  0.000813  0.000336  0.005606  \n",
      "56     0.028149  0.005513  0.015827  0.020884  0.010217  0.014823  0.007093  \n",
      "57     0.000006  0.000011  0.000014  0.000061  0.000133  0.000057  0.000017  \n",
      "58     0.000680  0.002184  0.015411  0.001210  0.000503  0.000378  0.006376  \n",
      "63     0.009767  0.005263  0.007601  0.001787  0.001922  0.000838  0.005336  \n",
      "65     0.010836  0.020465  0.015581  0.023732  0.009061  0.022926  0.014672  \n",
      "71     0.024032  0.001985  0.011915  0.020704  0.000813  0.000336  0.005606  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "14711  0.000457  0.000036  0.000119  0.000639  0.000349  0.000065  0.000152  \n",
      "14713  0.000009  0.000009  0.000020  0.000014  0.000017  0.000006  0.000005  \n",
      "14714  0.000479  0.000049  0.000124  0.000491  0.000180  0.000051  0.000119  \n",
      "14715  0.000077  0.000037  0.000022  0.001099  0.000188  0.000064  0.000473  \n",
      "14716  0.000743  0.010183  0.006273  0.000613  0.000201  0.000069  0.000077  \n",
      "14717  0.000070  0.000035  0.000021  0.001032  0.000182  0.000061  0.000422  \n",
      "14718  0.000457  0.000036  0.000119  0.000639  0.000349  0.000065  0.000152  \n",
      "14719  0.000566  0.000045  0.000084  0.001167  0.000339  0.000095  0.000216  \n",
      "14720  0.000398  0.000222  0.000146  0.000642  0.000175  0.000045  0.000149  \n",
      "14721  0.000487  0.000030  0.000089  0.000689  0.000238  0.000059  0.000153  \n",
      "14722  0.000672  0.000060  0.000133  0.000397  0.000101  0.000027  0.000128  \n",
      "14723  0.000038  0.000020  0.000049  0.000633  0.000189  0.000063  0.000146  \n",
      "14724  0.000572  0.000030  0.000091  0.000667  0.000237  0.000059  0.000162  \n",
      "14725  0.000042  0.000019  0.000063  0.000666  0.000251  0.000080  0.000154  \n",
      "14727  0.000470  0.000034  0.000118  0.010986  0.000265  0.000043  0.000160  \n",
      "14728  0.000042  0.000019  0.000063  0.000666  0.000251  0.000080  0.000154  \n",
      "14729  0.000006  0.000007  0.000008  0.000013  0.000019  0.000008  0.000004  \n",
      "14731  0.000340  0.000022  0.000048  0.000633  0.000183  0.000060  0.000146  \n",
      "14732  0.000036  0.000021  0.000041  0.000607  0.000153  0.000050  0.000179  \n",
      "14734  0.000572  0.000030  0.000091  0.000667  0.000237  0.000059  0.000162  \n",
      "14735  0.000077  0.000037  0.000022  0.001099  0.000188  0.000064  0.000473  \n",
      "14736  0.000077  0.000037  0.000022  0.001099  0.000188  0.000064  0.000473  \n",
      "14738  0.000572  0.000030  0.000091  0.000667  0.000237  0.000059  0.000162  \n",
      "14739  0.000351  0.000042  0.000131  0.000511  0.000130  0.000034  0.000142  \n",
      "14740  0.000486  0.000036  0.000091  0.000644  0.000233  0.000055  0.000155  \n",
      "14741  0.000457  0.000036  0.000119  0.000639  0.000349  0.000065  0.000152  \n",
      "14742  0.000540  0.000054  0.000112  0.000061  0.000049  0.000016  0.000017  \n",
      "14743  0.000043  0.000031  0.000100  0.000351  0.000051  0.000020  0.000133  \n",
      "14744  0.000465  0.000156  0.000108  0.000646  0.000196  0.000060  0.000144  \n",
      "14745  0.000490  0.000043  0.000083  0.000609  0.000240  0.000063  0.000125  \n",
      "\n",
      "[11974 rows x 512 columns] \n",
      " 1         1\n",
      "3         1\n",
      "5         1\n",
      "8         1\n",
      "9         1\n",
      "10        1\n",
      "12        1\n",
      "14        1\n",
      "17        1\n",
      "21        1\n",
      "23        1\n",
      "24        1\n",
      "27        1\n",
      "28        1\n",
      "35        1\n",
      "36        1\n",
      "42        1\n",
      "43        1\n",
      "44        1\n",
      "48        1\n",
      "49        1\n",
      "50        1\n",
      "53        1\n",
      "55        1\n",
      "56        1\n",
      "57        1\n",
      "58        1\n",
      "63        1\n",
      "65        1\n",
      "71        1\n",
      "         ..\n",
      "14711    10\n",
      "14713    10\n",
      "14714    10\n",
      "14715    10\n",
      "14716    10\n",
      "14717    10\n",
      "14718    10\n",
      "14719    10\n",
      "14720    10\n",
      "14721    10\n",
      "14722    10\n",
      "14723    10\n",
      "14724    10\n",
      "14725    10\n",
      "14727    10\n",
      "14728    10\n",
      "14729    10\n",
      "14731    10\n",
      "14732    10\n",
      "14734    10\n",
      "14735    10\n",
      "14736    10\n",
      "14738    10\n",
      "14739    10\n",
      "14740    10\n",
      "14741    10\n",
      "14742    10\n",
      "14743    10\n",
      "14744    10\n",
      "14745    10\n",
      "Name: 512, Length: 11974, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 对全NaN列行进行清除\n",
    "result = pd.concat([subfeatures, labels], axis=1,ignore_index=True)\n",
    "# print(result)\n",
    "result=result.dropna(how='all',axis=1)\n",
    "result=result.dropna(how='any',axis=0)\n",
    "result=result.reindex()\n",
    "# print(result)\n",
    "subfeatures=result.iloc[:,:-1]\n",
    "labels=result.iloc[:,-1]\n",
    "print(subfeatures,'\\n',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.71      0.73        63\n",
      "           2       0.94      0.96      0.95       125\n",
      "           3       0.88      0.82      0.85        97\n",
      "           4       0.99      1.00      1.00       168\n",
      "           5       1.00      0.93      0.97       152\n",
      "           6       0.94      1.00      0.97       152\n",
      "           7       0.94      0.93      0.94       128\n",
      "           8       0.91      1.00      0.95        48\n",
      "           9       0.95      0.97      0.96       130\n",
      "          10       0.97      0.98      0.97       135\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.93      0.93      0.93      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      "0.9449081803005008\n",
      "Confusion matrix, without normalization\n",
      "[[ 45   5   5   0   0   0   1   3   2   2]\n",
      " [  3 120   1   0   0   0   1   0   0   0]\n",
      " [  7   1  80   1   0   0   3   1   2   2]\n",
      " [  0   0   0 168   0   0   0   0   0   0]\n",
      " [  0   0   0   0 142   9   0   1   0   0]\n",
      " [  0   0   0   0   0 152   0   0   0   0]\n",
      " [  1   2   4   0   0   0 119   0   2   0]\n",
      " [  0   0   0   0   0   0   0  48   0   0]\n",
      " [  1   0   1   0   0   0   2   0 126   0]\n",
      " [  3   0   0   0   0   0   0   0   0 132]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.59      0.63        82\n",
      "           2       0.92      0.94      0.93       139\n",
      "           3       0.86      0.91      0.88        86\n",
      "           4       0.99      1.00      1.00       141\n",
      "           5       0.97      0.91      0.94       136\n",
      "           6       0.93      1.00      0.96       154\n",
      "           7       0.94      0.93      0.93       163\n",
      "           8       0.98      1.00      0.99        51\n",
      "           9       0.86      0.87      0.87       117\n",
      "          10       0.95      0.94      0.94       129\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1198\n",
      "   macro avg       0.91      0.91      0.91      1198\n",
      "weighted avg       0.92      0.92      0.92      1198\n",
      "\n",
      "0.9190317195325542\n",
      "Confusion matrix, without normalization\n",
      "[[ 48   8   8   1   4   0   1   0   8   4]\n",
      " [  7 131   1   0   0   0   0   0   0   0]\n",
      " [  2   2  78   0   0   0   0   1   2   1]\n",
      " [  0   0   0 141   0   0   0   0   0   0]\n",
      " [  0   0   0   0 124  12   0   0   0   0]\n",
      " [  0   0   0   0   0 154   0   0   0   0]\n",
      " [  5   1   0   0   0   0 151   0   6   0]\n",
      " [  0   0   0   0   0   0   0  51   0   0]\n",
      " [  2   0   2   0   0   0   9   0 102   2]\n",
      " [  6   0   2   0   0   0   0   0   0 121]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.76      0.80        67\n",
      "           2       0.92      0.98      0.95       123\n",
      "           3       0.91      0.88      0.89        98\n",
      "           4       0.99      1.00      1.00       157\n",
      "           5       0.98      0.91      0.94       131\n",
      "           6       0.91      1.00      0.95       148\n",
      "           7       0.94      0.96      0.95       148\n",
      "           8       0.98      0.97      0.97        59\n",
      "           9       0.98      0.91      0.94       151\n",
      "          10       0.94      0.98      0.96       116\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.94      0.93      0.94      1198\n",
      "weighted avg       0.95      0.94      0.94      1198\n",
      "\n",
      "0.9449081803005008\n",
      "Confusion matrix, without normalization\n",
      "[[ 51   3   3   0   0   1   2   1   2   4]\n",
      " [  1 121   1   0   0   0   0   0   0   0]\n",
      " [  2   5  86   0   1   1   1   0   0   2]\n",
      " [  0   0   0 157   0   0   0   0   0   0]\n",
      " [  0   0   0   0 119  12   0   0   0   0]\n",
      " [  0   0   0   0   0 148   0   0   0   0]\n",
      " [  1   1   2   0   0   0 142   0   1   1]\n",
      " [  0   1   1   0   0   0   0  57   0   0]\n",
      " [  4   1   1   1   1   0   6   0 137   0]\n",
      " [  1   0   1   0   0   0   0   0   0 114]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.71      0.71        78\n",
      "           2       0.86      0.98      0.92       113\n",
      "           3       0.84      0.80      0.82       106\n",
      "           4       1.00      1.00      1.00       155\n",
      "           5       0.98      0.94      0.96       155\n",
      "           6       0.94      0.98      0.96       136\n",
      "           7       0.97      0.95      0.96       157\n",
      "           8       0.93      0.98      0.95        43\n",
      "           9       0.97      0.89      0.93       134\n",
      "          10       0.92      0.95      0.93       121\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1198\n",
      "   macro avg       0.91      0.92      0.91      1198\n",
      "weighted avg       0.93      0.93      0.93      1198\n",
      "\n",
      "0.9257095158597662\n",
      "Confusion matrix, without normalization\n",
      "[[ 55   7   7   0   2   0   1   0   0   6]\n",
      " [  2 111   0   0   0   0   0   0   0   0]\n",
      " [  8   7  85   0   0   0   1   2   2   1]\n",
      " [  0   0   0 155   0   0   0   0   0   0]\n",
      " [  0   0   0   0 145   9   0   1   0   0]\n",
      " [  1   0   2   0   0 133   0   0   0   0]\n",
      " [  2   3   1   0   0   0 149   0   2   0]\n",
      " [  1   0   0   0   0   0   0  42   0   0]\n",
      " [  7   1   2   0   1   0   1   0 119   3]\n",
      " [  1   0   4   0   0   0   1   0   0 115]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.81      0.82        78\n",
      "           2       0.95      0.95      0.95       123\n",
      "           3       0.85      0.89      0.87        91\n",
      "           4       1.00      1.00      1.00       163\n",
      "           5       1.00      0.94      0.97       145\n",
      "           6       0.94      1.00      0.97       154\n",
      "           7       0.97      0.93      0.95       151\n",
      "           8       0.98      1.00      0.99        43\n",
      "           9       0.91      0.95      0.93       128\n",
      "          10       0.98      0.97      0.98       122\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1198\n",
      "   macro avg       0.94      0.94      0.94      1198\n",
      "weighted avg       0.95      0.95      0.95      1198\n",
      "\n",
      "0.9490818030050083\n",
      "Confusion matrix, without normalization\n",
      "[[ 63   2   7   0   0   0   1   0   3   2]\n",
      " [  4 117   0   0   0   0   0   0   2   0]\n",
      " [  6   1  81   0   0   0   0   1   2   0]\n",
      " [  0   0   0 163   0   0   0   0   0   0]\n",
      " [  0   0   0   0 137   8   0   0   0   0]\n",
      " [  0   0   0   0   0 154   0   0   0   0]\n",
      " [  0   3   3   0   0   0 140   0   5   0]\n",
      " [  0   0   0   0   0   0   0  43   0   0]\n",
      " [  0   0   4   0   0   0   3   0 121   0]\n",
      " [  3   0   0   0   0   1   0   0   0 118]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.74      0.74        74\n",
      "           2       0.94      0.91      0.93       136\n",
      "           3       0.89      0.89      0.89        96\n",
      "           4       1.00      1.00      1.00       163\n",
      "           5       1.00      0.87      0.93       164\n",
      "           6       0.87      0.99      0.93       145\n",
      "           7       0.93      0.96      0.94       141\n",
      "           8       0.92      0.98      0.95        45\n",
      "           9       0.89      0.92      0.91       116\n",
      "          10       0.97      0.91      0.94       118\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1198\n",
      "   macro avg       0.91      0.92      0.91      1198\n",
      "weighted avg       0.93      0.92      0.92      1198\n",
      "\n",
      "0.9240400667779632\n",
      "Confusion matrix, without normalization\n",
      "[[ 55   4   5   0   0   1   3   1   2   3]\n",
      " [  6 124   2   0   0   0   1   2   1   0]\n",
      " [  4   2  85   0   0   0   1   1   3   0]\n",
      " [  0   0   0 163   0   0   0   0   0   0]\n",
      " [  0   0   0   0 143  21   0   0   0   0]\n",
      " [  1   0   0   0   0 144   0   0   0   0]\n",
      " [  1   0   2   0   0   0 135   0   3   0]\n",
      " [  0   0   1   0   0   0   0  44   0   0]\n",
      " [  3   2   0   0   0   0   4   0 107   0]\n",
      " [  5   0   1   0   0   0   1   0   4 107]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.71      0.75        80\n",
      "           2       0.94      0.96      0.95       143\n",
      "           3       0.84      0.85      0.85        82\n",
      "           4       1.00      0.99      1.00       163\n",
      "           5       0.98      0.90      0.94       138\n",
      "           6       0.92      1.00      0.96       157\n",
      "           7       0.94      0.92      0.93       148\n",
      "           8       0.95      1.00      0.98        41\n",
      "           9       0.93      0.93      0.93       123\n",
      "          10       0.94      0.97      0.96       123\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1198\n",
      "   macro avg       0.92      0.92      0.92      1198\n",
      "weighted avg       0.93      0.93      0.93      1198\n",
      "\n",
      "0.9332220367278798\n",
      "Confusion matrix, without normalization\n",
      "[[ 57   3   7   0   0   1   4   0   1   7]\n",
      " [  5 137   0   0   0   0   0   1   0   0]\n",
      " [  6   2  70   0   0   0   1   1   2   0]\n",
      " [  1   0   0 162   0   0   0   0   0   0]\n",
      " [  0   0   1   0 124  13   0   0   0   0]\n",
      " [  0   0   0   0   0 157   0   0   0   0]\n",
      " [  2   3   3   0   0   0 136   0   4   0]\n",
      " [  0   0   0   0   0   0   0  41   0   0]\n",
      " [  1   1   1   0   1   0   4   0 115   0]\n",
      " [  1   0   1   0   1   0   0   0   1 119]]\n",
      "train...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.76      0.77        87\n",
      "           2       0.95      0.94      0.94       125\n",
      "           3       0.87      0.92      0.89        83\n",
      "           4       0.99      1.00      1.00       169\n",
      "           5       0.99      0.92      0.96       154\n",
      "           6       0.90      1.00      0.95       121\n",
      "           7       0.93      0.95      0.94       149\n",
      "           8       1.00      1.00      1.00        46\n",
      "           9       0.95      0.93      0.94       140\n",
      "          10       0.97      0.95      0.96       124\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.93      0.94      0.93      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      "0.9398998330550918\n",
      "Confusion matrix, without normalization\n",
      "[[ 66   6   5   0   0   1   1   0   5   3]\n",
      " [  6 117   1   0   0   0   1   0   0   0]\n",
      " [  4   0  76   1   0   0   0   0   1   1]\n",
      " [  0   0   0 169   0   0   0   0   0   0]\n",
      " [  0   0   0   0 142  12   0   0   0   0]\n",
      " [  0   0   0   0   0 121   0   0   0   0]\n",
      " [  3   0   4   0   0   0 141   0   1   0]\n",
      " [  0   0   0   0   0   0   0  46   0   0]\n",
      " [  2   0   0   0   0   0   8   0 130   0]\n",
      " [  3   0   1   0   1   0   1   0   0 118]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.70      0.76        83\n",
      "           2       0.90      0.98      0.94       125\n",
      "           3       0.93      0.87      0.90       102\n",
      "           4       0.98      1.00      0.99       137\n",
      "           5       1.00      0.95      0.97       161\n",
      "           6       0.94      1.00      0.97       142\n",
      "           7       0.95      0.97      0.96       151\n",
      "           8       1.00      1.00      1.00        49\n",
      "           9       0.90      0.88      0.89       112\n",
      "          10       0.94      0.96      0.95       136\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.94      0.93      0.93      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      "0.9398998330550918\n",
      "Confusion matrix, without normalization\n",
      "[[ 58  10   1   1   0   1   1   0   6   5]\n",
      " [  2 122   1   0   0   0   0   0   0   0]\n",
      " [  3   2  89   0   0   0   3   0   3   2]\n",
      " [  0   0   0 137   0   0   0   0   0   0]\n",
      " [  0   0   0   0 153   8   0   0   0   0]\n",
      " [  0   0   0   0   0 142   0   0   0   0]\n",
      " [  1   1   0   0   0   0 147   0   2   0]\n",
      " [  0   0   0   0   0   0   0  49   0   0]\n",
      " [  4   0   3   1   0   0   3   0  99   2]\n",
      " [  2   0   2   1   0   0   1   0   0 130]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.79      0.70        53\n",
      "           2       0.94      0.91      0.92       129\n",
      "           3       0.93      0.86      0.89       114\n",
      "           4       0.99      0.99      0.99       152\n",
      "           5       0.98      0.96      0.97       154\n",
      "           6       0.95      1.00      0.98       140\n",
      "           7       0.93      0.96      0.94       138\n",
      "           8       0.96      1.00      0.98        46\n",
      "           9       0.91      0.91      0.91       139\n",
      "          10       0.98      0.90      0.94       133\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.92      0.93      0.92      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      "0.9357262103505843\n",
      "Confusion matrix, without normalization\n",
      "[[ 42   2   1   0   0   1   2   0   3   2]\n",
      " [  6 117   1   0   0   0   1   0   4   0]\n",
      " [  6   3  98   0   1   0   2   2   2   0]\n",
      " [  0   0   0 151   0   0   0   0   1   0]\n",
      " [  0   0   0   0 148   6   0   0   0   0]\n",
      " [  0   0   0   0   0 140   0   0   0   0]\n",
      " [  1   2   2   0   0   0 132   0   1   0]\n",
      " [  0   0   0   0   0   0   0  46   0   0]\n",
      " [  4   0   2   0   0   0   5   0 127   1]\n",
      " [  8   0   1   1   2   0   0   0   1 120]]\n"
     ]
    }
   ],
   "source": [
    "# 平均准确率归零\n",
    "avgscore = 0\n",
    "recallscore = 0\n",
    "precisionscore = 0\n",
    "N = 10\n",
    "\n",
    "# 进行十次KNN测试\n",
    "for i in range(N):\n",
    "    # 以10%的比例进行交叉验证\n",
    "    #X_train, X_test, y_train, y_test = cross_validation.train_test_split(subfeatures, features_labels, test_size=0.1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subfeatures, labels, test_size=0.1)\n",
    "\n",
    "    # 进行训练\n",
    "    print('train...')\n",
    "    # 进行KNN训练,距离为1\n",
    "    neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "    neigh.fit(X_train, y_train)\n",
    "\n",
    "    # 预试\n",
    "    print('test...')\n",
    "    c_test = neigh.predict(X_test)\n",
    "\n",
    "    # print(y_test)\n",
    "    # print(c_test)\n",
    "\n",
    "    # 计算预测划分准确率\n",
    "    print('accurary...')\n",
    "    score = neigh.score(X_test, y_test)\n",
    "    print(classification_report(y_test,c_test))\n",
    "    avgscore = avgscore + score\n",
    "    recallscore = recallscore + recall_score(y_test,c_test,average=\"macro\")\n",
    "    precisionscore = precisionscore + precision_score(y_test,c_test,average=\"macro\")\n",
    "    print(score)\n",
    "\n",
    "    # 通过混淆矩阵进行结果标示\n",
    "    cm = confusion_matrix(y_test, c_test)\n",
    "    np.set_printoptions(threshold=10000)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(str(cm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgscore....\n",
      "0.9356427378964941\n",
      "False positive rate\n",
      "0.0739616897089459\n",
      "false negative rate\n",
      "0.07296649520946463\n"
     ]
    }
   ],
   "source": [
    "# 输出N次的平均准确率\n",
    "avgscore = avgscore / N\n",
    "recallscore = recallscore / N\n",
    "precisionscore = precisionscore / N\n",
    "\n",
    "print('avgscore....')\n",
    "print(avgscore)\n",
    "print('False positive rate')\n",
    "print(1-precisionscore)\n",
    "print('false negative rate')\n",
    "print(1-recallscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
