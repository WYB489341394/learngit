{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Summary of the file.\\n    \\n    功能：\\n        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以随机森林方法划分并获得准确率，通过混淆矩阵标示结果。\\n    \\n    输出：\\n        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report,precision_score,recall_score\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "''' Summary of the file.\n",
    "    \n",
    "    功能：\n",
    "        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以随机森林方法划分并获得准确率，通过混淆矩阵标示结果。\n",
    "    \n",
    "    输出：\n",
    "        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LBP特征值\n",
    "subfeatures = pd.read_csv(r'F:\\virtus_test\\Simhash_Gist\\gist_f_train_full.csv',header=None)\n",
    "labels = pd.read_csv(r'F:\\virtus_test\\Simhash_Gist\\CNO_full.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 7, 11, 13, 15, 16, 18, 19, 20, 22, 25, 26, 29, 30, 31, 32, 33, 34, 37, 38, 39, 40, 41, 45, 46, 47, 51, 52, 54, 59, 60, 61, 62, 64, 66, 67, 68, 69, 70, 72, 74, 75, 78, 83, 84, 85, 87, 88, 91, 92, 93, 94, 96, 98, 99, 102, 103, 104, 106, 108, 109, 112, 114, 116, 118, 119, 120, 122, 123, 124, 126, 127, 128, 132, 133, 134, 135, 137, 140, 141, 143, 145, 146, 147, 148, 153, 154, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 169, 170, 171, 173, 175, 181, 182, 183, 184, 187, 188, 189, 193, 195, 196, 197, 198, 199, 202, 204, 205, 206, 207, 209, 211, 216, 218, 220, 223, 224, 226, 229, 230, 231, 233, 237, 238, 242, 243, 244, 246, 247, 253, 255, 256, 257, 259, 260, 261, 262, 263, 265, 267, 269, 270, 272, 276, 277, 279, 280, 281, 282, 283, 284, 287, 288, 289, 293, 295, 297, 298, 299, 302, 303, 305, 307, 312, 315, 316, 319, 321, 322, 323, 324, 325, 329, 331, 333, 336, 340, 343, 345, 349, 351, 352, 353, 354, 356, 357, 358, 359, 360, 362, 364, 365, 368, 375, 376, 377, 378, 379, 380, 381, 382, 384, 386, 387, 390, 393, 395, 396, 397, 401, 402, 404, 405, 406, 407, 408, 410, 414, 416, 418, 419, 420, 422, 423, 424, 425, 426, 432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 446, 449, 450, 451, 453, 456, 457, 459, 461, 463, 464, 466, 467, 468, 469, 470, 473, 475, 477, 478, 479, 481, 482, 485, 487, 489, 492, 493, 494, 495, 496, 501, 502, 508, 509, 512, 513, 515, 516, 517, 518, 519, 520, 522, 524, 525, 526, 530, 532, 537, 538, 539, 540, 541, 544, 545, 547, 548, 550, 555, 556, 558, 559, 560, 561, 563, 567, 568, 569, 571, 572, 573, 574, 576, 577, 578, 581, 583, 584, 587, 588, 594, 597, 599, 600, 601, 604, 605, 606, 610, 611, 612, 613, 615, 618, 619, 623, 625, 626, 627, 628, 632, 633, 634, 635, 636, 639, 641, 642, 644, 645, 647, 648, 655, 657, 658, 659, 662, 663, 664, 667, 668, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 685, 686, 689, 691, 692, 695, 696, 697, 704, 705, 707, 708, 709, 710, 712, 718, 721, 722, 723, 725, 726, 729, 731, 732, 736, 737, 739, 741, 742, 745, 747, 748, 749, 756, 759, 760, 762, 764, 766, 768, 770, 772, 773, 774, 776, 778, 779, 781, 786, 787, 789, 790, 793, 794, 797, 798, 799, 800, 801, 802, 811, 812, 814, 816, 817, 819, 822, 823, 824, 825, 826, 827, 829, 830, 831, 832, 833, 839, 843, 844, 845, 847, 854, 858, 859, 860, 861, 866, 870, 874, 875, 877, 881, 883, 885, 887, 888, 889, 891, 892, 893, 894, 897, 898, 899, 902, 905, 906, 908, 911, 913, 918, 921, 923, 925, 926, 927, 929, 930, 933, 937, 939, 943, 945, 948, 951, 952, 953, 957, 959, 960, 961, 967, 968, 970, 972, 973, 976, 978, 979, 980, 981, 984, 985, 987, 988, 990, 994, 996, 997, 999, 1001, 1003, 1006, 1007, 1014, 1016, 1017, 1019, 1020, 1024, 1025, 1026, 1028, 1034, 1039, 1042, 1045, 1047, 1050, 1053, 1054, 1055, 1057, 1058, 1060, 1073, 1076, 1078, 1081, 1083, 1084, 1085, 1094, 1095, 1097, 1099, 1104, 1105, 1106, 1107, 1108, 1109, 1111, 1112, 1116, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1128, 1133, 1134, 1136, 1137, 1140, 1151, 1152, 1153, 1157, 1158, 1159, 1160, 1161, 1162, 1164, 1166, 1168, 1170, 1172, 1174, 1175, 1176, 1177, 1178, 1179, 1181, 1183, 1186, 1187, 1188, 1189, 1191, 1193, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1203, 1204, 1205, 1206, 1210, 1211, 1213, 1214, 1216, 1217, 1218, 1221, 1222, 1224, 1225, 1228, 1229, 1230, 1232, 1235, 1236, 1239, 1240, 1244, 1245, 1246, 1251, 1252, 1254, 1256, 1258, 1262, 1264, 1265, 1267, 1268, 1269, 1270, 1271, 1272, 1276, 1278, 1279, 1282, 1284, 1286, 1287, 1289, 1290, 1292, 1295, 1298, 1302, 1303, 1305, 1312, 1316, 1319, 1320, 1327, 1329, 1331, 1333, 1334, 1336, 1337, 1338, 1340, 1341, 1342, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1353, 1354, 1356, 1357, 1365, 1367, 1369, 1371, 1373, 1374, 1375, 1379, 1381, 1382, 1384, 1388, 1389, 1390, 1393, 1394, 1395, 1396, 1399, 1400, 1401, 1402, 1404, 1405, 1406, 1409, 1410, 1412, 1413, 1415, 1417, 1418, 1419, 1420, 1421, 1423, 1424, 1426, 1428, 1429, 1430, 1431, 1433, 1436, 1437, 1439, 1440, 1443, 1444, 1446, 1447, 1448, 1449, 1450, 1454, 1460, 1462, 1469, 1472, 1474, 1482, 1500, 1503, 1507, 1508, 1511, 1520, 1545, 1555, 1556, 1596, 1602, 1604, 1608, 1613, 1616, 1622, 1625, 1627, 1630, 1649, 1650, 1660, 1666, 1667, 1670, 1680, 1682, 1683, 1696, 1697, 1709, 1713, 1716, 1724, 1729, 1734, 1741, 1765, 1771, 1793, 1797, 1805, 1813, 1817, 1821, 1827, 1845, 1851, 1852, 1859, 1868, 1871, 1882, 1889, 1893, 1898, 1900, 1902, 1910, 1916, 1941, 1943, 1954, 1957, 1979, 1982, 1988, 2009, 2014, 2027, 2030, 2059, 2063, 2064, 2071, 2072, 2073, 2074, 2079, 2084, 2107, 2122, 2124, 2125, 2130, 2140, 2155, 2171, 2193, 2211, 2214, 2224, 2250, 2274, 2283, 2297, 2299, 2309, 2313, 2339, 2341, 2343, 2345, 2346, 2351, 2368, 2383, 2392, 2400, 2418, 2425, 2426, 2434, 2440, 2442, 2459, 2462, 2463, 2468, 2473, 2475, 2487, 2491, 2492, 2494, 2497, 2502, 2505, 2506, 2515, 2516, 2521, 2529, 2541, 2546, 2550, 2581, 2584, 2587, 2596, 2599, 2611, 2630, 2631, 2633, 2634, 2643, 2671, 2689, 2694, 2710, 2733, 2734, 2740, 2761, 2768, 2770, 2805, 2816, 2819, 2827, 2830, 2832, 2839, 2841, 2850, 2851, 2859, 2868, 2874, 2876, 2894, 2896, 2908, 2913, 2916, 2932, 2934, 2954, 2976, 2977, 2979, 2980, 2985, 2986, 2989, 2991, 2992, 2993, 2995, 2998, 2999, 3000, 3001, 3005, 3009, 3010, 3012, 3014, 3015, 3016, 3020, 3022, 3025, 3029, 3031, 3032, 3033, 3035, 3037, 3039, 3044, 3052, 3053, 3054, 3058, 3059, 3060, 3063, 3064, 3068, 3069, 3073, 3077, 3079, 3084, 3086, 3088, 3095, 3096, 3097, 3098, 3103, 3104, 3107, 3110, 3111, 3117, 3123, 3125, 3126, 3128, 3130, 3133, 3136, 3138, 3144, 3152, 3157, 3161, 3163, 3164, 3165, 3166, 3167, 3171, 3174, 3179, 3182, 3185, 3186, 3187, 3188, 3190, 3192, 3195, 3196, 3197, 3202, 3203, 3207, 3208, 3209, 3211, 3216, 3220, 3223, 3224, 3229, 3231, 3232, 3233, 3235, 3236, 3237, 3239, 3240, 3242, 3244, 3247, 3250, 3251, 3253, 3256, 3260, 3267, 3271, 3272, 3275, 3283, 3284, 3295, 3297, 3298, 3299, 3300, 3301, 3305, 3307, 3311, 3312, 3315, 3320, 3322, 3323, 3324, 3325, 3327, 3333, 3335, 3338, 3339, 3350, 3353, 3354, 3356, 3362, 3368, 3370, 3371, 3376, 3377, 3381, 3382, 3384, 3385, 3392, 3396, 3398, 3406, 3409, 3411, 3412, 3414, 3416, 3417, 3419, 3420, 3425, 3426, 3427, 3431, 3433, 3436, 3437, 3438, 3439, 3441, 3443, 3446, 3448, 3450, 3453, 3454, 3455, 3457, 3458, 3459, 3464, 3471, 3473, 3476, 3478, 3482, 3484, 3488, 3489, 3494, 3498, 3501, 3502, 3507, 3509, 3510, 3511, 3512, 3519, 3520, 3524, 3527, 3528, 3537, 3541, 3544, 3548, 3550, 3556, 3561, 3563, 3567, 3569, 3573, 3574, 3576, 3578, 3579, 3584, 3585, 3588, 3595, 3596, 3597, 3598, 3600, 3608, 3609, 3610, 3612, 3613, 3614, 3617, 3618, 3621, 3622, 3623, 3625, 3626, 3627, 3629, 3630, 3631, 3635, 3640, 3641, 3642, 3643, 3645, 3646, 3650, 3654, 3658, 3660, 3661, 3665, 3668, 3669, 3672, 3674, 3676, 3678, 3679, 3691, 3703, 3706, 3708, 3709, 3712, 3713, 3714, 3716, 3722, 3724, 3727, 3732, 3734, 3738, 3744, 3745, 3746, 3750, 3756, 3758, 3770, 3777, 3788, 3790, 3791, 3794, 3796, 3799, 3800, 3808, 3815, 3821, 3822, 3823, 3828, 3831, 3836, 3851, 3852, 3854, 3856, 3858, 3861, 3870, 3874, 3876, 3878, 3881, 3886, 3893, 3896, 3897, 3901, 3903, 3910, 3927, 3934, 3946, 3952, 3955, 3975, 3983, 3988, 3993, 3994, 4000, 4006, 4009, 4035, 4045, 4053, 4058, 4061, 4062, 4063, 4075, 4076, 4077, 4080, 4081, 4084, 4088, 4091, 4092, 4094, 4098, 4099, 4100, 4101, 4102, 4105, 4118, 4122, 4124, 4126, 4139, 4151, 4158, 4162, 4163, 4164, 4165, 4167, 4168, 4179, 4186, 4187, 4189, 4190, 4195, 4196, 4200, 4204, 4205, 4206, 4207, 4208, 4210, 4213, 4216, 4218, 4219, 4220, 4221, 4225, 4226, 4230, 4231, 4232, 4234, 4238, 4239, 4243, 4244, 4248, 4251, 4254, 4259, 4260, 4262, 4263, 4264, 4267, 4268, 4269, 4270, 4271, 4274, 4277, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4290, 4293, 4294, 4295, 4296, 4299, 4301, 4303, 4304, 4306, 4307, 4309, 4315, 4317, 4319, 4321, 4323, 4325, 4328, 4329, 4332, 4333, 4334, 4342, 4344, 4345, 4348, 4349, 4350, 4351, 4352, 4353, 4356, 4359, 4366, 4367, 4368, 4369, 4370, 4371, 4377, 4380, 4382, 4383, 4384, 4388, 4400, 4401, 4461, 4494, 4511, 4516, 4532, 4637, 4660, 4667, 4679, 4908, 4932, 4934, 5002, 5099, 5154, 5311, 5682, 5715, 5750, 5782, 6065, 6267, 6321, 6521, 6558, 6848, 6861, 6988, 7012, 7104, 7478, 7574, 7691, 7766, 7869, 8595, 8640, 8703, 8728, 8900, 8930, 8936, 8948, 8950, 8958, 8967, 8969, 8972, 8983, 8990, 8993, 8998, 9000, 9014, 9018, 9019, 9119, 9186, 9467, 9474, 9626, 9760, 9771, 9816, 9840, 10029, 10107, 10243, 10349, 10363, 10365, 10388, 10389, 10390, 10392, 10393, 10394, 10395, 10398, 10399, 10400, 10401, 10403, 10404, 10405, 10406, 10407, 10408, 10409, 10410, 10412, 10414, 10416, 10420, 10422, 10423, 10425, 10427, 10429, 10430, 10431, 10432, 10434, 10435, 10436, 10437, 10438, 10439, 10440, 10441, 10442, 10445, 10448, 10449, 10451, 10452, 10453, 10455, 10456, 10457, 10458, 10460, 10461, 10462, 10463, 10466, 10468, 10469, 10470, 10471, 10474, 10475, 10477, 10480, 10482, 10485, 10486, 10487, 10489, 10490, 10491, 10493, 10494, 10495, 10496, 10497, 10498, 10499, 10500, 10501, 10502, 10504, 10506, 10508, 10509, 10510, 10512, 10513, 10514, 10516, 10517, 10518, 10519, 10520, 10521, 10522, 10526, 10528, 10530, 10532, 10533, 10534, 10535, 10536, 10537, 10541, 10542, 10543, 10546, 10548, 10549, 10550, 10552, 10553, 10554, 10557, 10563, 10565, 10566, 10567, 10568, 10569, 10570, 10573, 10574, 10575, 10577, 10579, 10580, 10581, 10582, 10584, 10585, 10588, 10590, 10592, 10593, 10594, 10595, 10596, 10597, 10599, 10600, 10601, 10602, 10603, 10605, 10606, 10607, 10608, 10609, 10610, 10611, 10612, 10613, 10614, 10615, 10616, 10618, 10619, 10621, 10622, 10623, 10626, 10628, 10629, 10632, 10633, 10634, 10635, 10636, 10637, 10639, 10641, 10643, 10644, 10645, 10646, 10647, 10650, 10651, 10653, 10654, 10656, 10657, 10659, 10660, 10661, 10662, 10665, 10667, 10668, 10669, 10672, 10673, 10677, 10679, 10681, 10682, 10683, 10684, 10685, 10687, 10688, 10690, 10692, 10693, 10694, 10695, 10696, 10697, 10700, 10701, 10703, 10705, 10707, 10708, 10710, 10713, 10715, 10716, 10717, 10718, 10720, 10721, 10723, 10724, 10726, 10727, 10729, 10730, 10732, 10733, 10737, 10738, 10740, 10742, 10743, 10744, 10745, 10746, 10747, 10748, 10749, 10750, 10751, 10752, 10753, 10755, 10756, 10757, 10758, 10759, 10761, 10762, 10763, 10764, 10765, 10767, 10771, 10772, 10773, 10774, 10776, 10777, 10778, 10779, 10780, 10781, 10782, 10783, 10785, 10787, 10790, 10791, 10792, 10793, 10794, 10795, 10796, 10797, 10798, 10799, 10800, 10801, 10802, 10803, 10805, 10807, 10808, 10809, 10810, 10811, 10813, 10814, 10817, 10818, 10819, 10821, 10826, 10827, 10828, 10830, 10831, 10832, 10833, 10834, 10835, 10836, 10837, 10839, 10840, 10841, 10842, 10843, 10844, 10845, 10847, 10848, 10849, 10850, 10851, 10853, 10854, 10855, 10856, 10858, 10860, 10861, 10862, 10864, 10865, 10866, 10867, 10868, 10869, 10870, 10872, 10873, 10874, 10875, 10876, 10878, 10879, 10880, 10881, 10882, 10883, 10884, 10885, 10886, 10887, 10888, 10889, 10891, 10892, 10894, 10895, 10896, 10897, 10898, 10899, 10900, 10901, 10902, 10903, 10905, 10907, 10908, 10909, 10910, 10911, 10912, 10913, 10914, 10915, 10916, 10920, 10921, 10922, 10923, 10924, 10927, 10928, 10929, 10931, 10933, 10934, 10936, 10937, 10939, 10940, 10941, 10942, 10944, 10951, 10952, 10953, 10955, 10956, 10958, 10959, 10960, 10961, 10964, 10965, 10966, 10967, 10969, 10970, 10971, 10973, 10974, 10975, 10978, 10979, 10981, 10982, 10983, 10984, 10985, 10986, 10987, 10988, 10991, 10992, 10993, 10994, 10996, 10997, 10998, 10999, 11000, 11002, 11003, 11004, 11005, 11006, 11008, 11009, 11010, 11013, 11014, 11015, 11016, 11017, 11018, 11019, 11020, 11021, 11023, 11025, 11026, 11028, 11030, 11031, 11033, 11035, 11036, 11037, 11041, 11043, 11044, 11045, 11047, 11048, 11049, 11050, 11051, 11053, 11054, 11055, 11056, 11059, 11061, 11062, 11064, 11066, 11067, 11068, 11069, 11070, 11073, 11074, 11075, 11076, 11077, 11078, 11079, 11081, 11083, 11085, 11086, 11087, 11088, 11091, 11092, 11093, 11095, 11100, 11101, 11102, 11104, 11110, 11111, 11112, 11113, 11114, 11115, 11117, 11118, 11119, 11120, 11121, 11122, 11124, 11125, 11126, 11129, 11131, 11134, 11135, 11136, 11137, 11138, 11139, 11140, 11141, 11142, 11146, 11149, 11150, 11152, 11155, 11157, 11158, 11159, 11161, 11162, 11165, 11168, 11169, 11170, 11171, 11172, 11173, 11174, 11176, 11177, 11178, 11180, 11181, 11182, 11185, 11188, 11189, 11190, 11192, 11194, 11196, 11199, 11202, 11203, 11204, 11205, 11206, 11207, 11209, 11210, 11211, 11212, 11213, 11214, 11216, 11217, 11218, 11219, 11220, 11221, 11222, 11223, 11224, 11225, 11226, 11228, 11229, 11231, 11233, 11234, 11235, 11236, 11237, 11238, 11239, 11240, 11241, 11242, 11245, 11248, 11249, 11250, 11251, 11254, 11255, 11256, 11258, 11259, 11261, 11262, 11263, 11266, 11267, 11268, 11269, 11270, 11271, 11272, 11273, 11274, 11275, 11276, 11277, 11278, 11280, 11283, 11284, 11286, 11290, 11291, 11292, 11293, 11294, 11296, 11297, 11299, 11300, 11302, 11303, 11304, 11305, 11306, 11308, 11309, 11310, 11311, 11312, 11313, 11316, 11318, 11319, 11320, 11321, 11322, 11323, 11324, 11325, 11326, 11328, 11329, 11331, 11332, 11333, 11334, 11337, 11338, 11339, 11341, 11342, 11347, 11348, 11349, 11350, 11354, 11355, 11356, 11357, 11358, 11361, 11363, 11364, 11365, 11367, 11368, 11369, 11370, 11372, 11374, 11375, 11376, 11377, 11382, 11383, 11384, 11386, 11388, 11390, 11391, 11392, 11394, 11395, 11396, 11397, 11398, 11399, 11401, 11403, 11405, 11406, 11407, 11409, 11410, 11411, 11413, 11414, 11415, 11419, 11420, 11421, 11422, 11424, 11426, 11427, 11428, 11429, 11431, 11432, 11433, 11434, 11436, 11438, 11439, 11440, 11443, 11444, 11445, 11446, 11450, 11451, 11454, 11455, 11456, 11458, 11459, 11460, 11461, 11462, 11464, 11465, 11466, 11467, 11468, 11469, 11470, 11471, 11472, 11476, 11477, 11478, 11479, 11480, 11481, 11482, 11484, 11485, 11489, 11491, 11492, 11493, 11495, 11496, 11498, 11500, 11502, 11504, 11505, 11506, 11507, 11510, 11511, 11512, 11513, 11514, 11515, 11516, 11517, 11521, 11522, 11523, 11525, 11526, 11527, 11528, 11529, 11531, 11532, 11533, 11534, 11535, 11537, 11538, 11540, 11541, 11542, 11543, 11545, 11546, 11548, 11549, 11550, 11551, 11552, 11553, 11556, 11557, 11558, 11561, 11562, 11563, 11564, 11565, 11566, 11567, 11568, 11570, 11571, 11573, 11577, 11581, 11582, 11584, 11585, 11586, 11588, 11589, 11590, 11591, 11592, 11593, 11594, 11595, 11598, 11600, 11601, 11602, 11603, 11604, 11607, 11609, 11610, 11611, 11612, 11613, 11614, 11615, 11616, 11617, 11618, 11619, 11620, 11621, 11622, 11623, 11625, 11626, 11630, 11631, 11633, 11635, 11637, 11638, 11639, 11640, 11643, 11644, 11645, 11646, 11647, 11648, 11650, 11651, 11652, 11654, 11656, 11657, 11658, 11659, 11661, 11663, 11664, 11665, 11666, 11668, 11671, 11672, 11673, 11674, 11676, 11677, 11678, 11680, 11683, 11685, 11686, 11687, 11688, 11689, 11690, 11691, 11692, 11695, 11696, 11699, 11700, 11701, 11702, 11703, 11704, 11705, 11706, 11708, 11710, 11711, 11713, 11714, 11715, 11720, 11721, 11724, 11725, 11726, 11728, 11729, 11730, 11731, 11733, 11735, 11737, 11738, 11739, 11740, 11741, 11742, 11743, 11744, 11745, 11746, 11747, 11748, 11750, 11751, 11752, 11755, 11756, 11757, 11758, 11759, 11762, 11763, 11765, 11767, 11768, 11769, 11770, 11773, 11774, 11775, 11776, 11777, 11778, 11779, 11780, 11781, 11783, 11784, 11785, 11787, 11788, 11789, 11792, 11793, 11794, 11795, 11796, 11797, 11798, 11799, 11800, 11801, 11803, 11804, 11805, 11806, 11808, 11809, 11810, 11812, 11816, 11817, 11818, 11819, 11820, 11822, 11823, 11824, 11825, 11829, 11830, 11831, 11833, 11834, 11836, 11840, 11841, 11843, 11844, 11845, 11846, 11848, 11852, 11853, 11854, 11855, 11856, 11857, 11858, 11859, 11860, 11861, 11862, 11863, 11864, 11866, 11867, 11868, 11869, 11870, 11875, 11877, 11878, 11879, 11880, 11881, 11882, 11885, 11886, 11887, 11890, 11916, 11930, 11933, 11962, 11967, 11986, 11995, 11997, 12018, 12026, 12027, 12044, 12066, 12074, 12119, 12155, 12170, 12177, 12295, 12360, 12422, 12424, 12460, 12472, 12487, 12502, 12528, 12585, 12608, 12669, 12670, 12784, 12804, 12822, 12850, 12851, 12939, 12952, 12988, 13008, 13129, 13136, 13240, 13254, 13261, 13265, 13268, 13286, 13293, 13297, 13309, 13320, 13333, 13334, 13337, 13349, 13351, 13352, 13369, 13383, 13387, 13390, 13392, 13394, 13403, 13407, 13427, 13439, 13470, 13496, 13520, 13521, 13540, 13542, 13544, 13545, 13546, 13547, 13564, 13565, 13572, 13573, 13583, 13590, 13593, 13603, 13608, 13625, 13628, 13635, 13638, 13640, 13647, 13648, 13703, 13704, 13720, 13737, 13746, 13760, 13767, 13784, 13788, 13805, 13810, 13814, 13841, 13859, 13864, 13866, 13867, 13880, 13887, 13890, 13894, 13907, 13944, 13955, 13973, 13984, 13988, 13995, 13997, 14002, 14008, 14021, 14034, 14052, 14053, 14058, 14062, 14064, 14066, 14073, 14087, 14088, 14094, 14102, 14109, 14127, 14136, 14138, 14146, 14154, 14155, 14162, 14179, 14184, 14185, 14191, 14201, 14208, 14210, 14226, 14232, 14243, 14254, 14258, 14268, 14269, 14272, 14293, 14296, 14301, 14308, 14335, 14337, 14340, 14359, 14363, 14368, 14376, 14381, 14394, 14399, 14402, 14403, 14410, 14415, 14426, 14427, 14439, 14440, 14467, 14472, 14475, 14477, 14486, 14501, 14513, 14514, 14524, 14535, 14538, 14541, 14553, 14556, 14559, 14567, 14572, 14577, 14584, 14588, 14595, 14602, 14603, 14612, 14628, 14629, 14635, 14636, 14638, 14639, 14643, 14644, 14654, 14661, 14663, 14679, 14697, 14699, 14703, 14706, 14712, 14726, 14730, 14733, 14737] 2772\n"
     ]
    }
   ],
   "source": [
    "# 对全0行进行清除\n",
    "result = pd.concat([subfeatures, labels], axis=1,ignore_index=True)\n",
    "'''\n",
    "print(result)\n",
    "print(result.apply(lambda x: x.sum(), axis=1))\n",
    "print(labels.apply(lambda x: x.sum(), axis=1))\n",
    "print(labels.apply(lambda x: x.sum(), axis=1)==result.apply(lambda x: x.sum(), axis=1))\n",
    "'''\n",
    "list=[]\n",
    "Q = result.apply(lambda x: x.sum(), axis=1)\n",
    "W = labels.apply(lambda x: x.sum(), axis=1)\n",
    "for t in range(len(result)):\n",
    "    if Q[t]==W[t]:\n",
    "        list.append(t)\n",
    "print(list,len(list))  #打印出全零行的标号和个数\n",
    "result=result.drop(labels=list,axis=0)\n",
    "\n",
    "subfeatures=result.iloc[:,:-1]\n",
    "labels=result.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均准确率归零\n",
    "avgscore = 0\n",
    "recallscore = 0\n",
    "precisionscore = 0\n",
    "N = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train...\n",
      "test...\n",
      "0.9357262103505843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.73      0.76        71\n",
      "           2       0.97      0.96      0.96       143\n",
      "           3       0.83      0.86      0.84        83\n",
      "           4       0.98      1.00      0.99       150\n",
      "           5       1.00      0.89      0.94       170\n",
      "           6       0.88      0.98      0.93       149\n",
      "           7       0.98      0.95      0.97       162\n",
      "           8       0.98      0.98      0.98        43\n",
      "           9       0.90      0.95      0.92       116\n",
      "          10       0.95      0.97      0.96       111\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.93      0.93      0.93      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      " initial test  labels size\n",
      "Confusion matrix, without normalization\n",
      "[[ 52   3   9   1   0   1   0   0   2   3]\n",
      " [  3 137   1   0   0   0   0   1   1   0]\n",
      " [  3   0  71   1   0   0   2   0   3   3]\n",
      " [  0   0   0 150   0   0   0   0   0   0]\n",
      " [  1   0   0   0 151  18   0   0   0   0]\n",
      " [  0   0   3   0   0 146   0   0   0   0]\n",
      " [  1   0   2   0   0   0 154   0   5   0]\n",
      " [  0   0   0   0   0   0   0  42   1   0]\n",
      " [  4   1   0   0   0   0   1   0 110   0]\n",
      " [  1   0   0   1   0   1   0   0   0 108]]\n",
      "train...\n",
      "test...\n",
      "0.9382303839732888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.66      0.71        71\n",
      "           2       0.93      0.98      0.96       124\n",
      "           3       0.88      0.86      0.87        97\n",
      "           4       1.00      1.00      1.00       127\n",
      "           5       0.99      0.90      0.94       152\n",
      "           6       0.91      0.99      0.95       147\n",
      "           7       0.96      0.96      0.96       158\n",
      "           8       0.98      0.98      0.98        51\n",
      "           9       0.93      0.95      0.94       131\n",
      "          10       0.96      0.97      0.96       140\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.93      0.93      0.93      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      " initial test  labels size\n",
      "Confusion matrix, without normalization\n",
      "[[ 47   4   7   0   2   0   2   0   4   5]\n",
      " [  0 122   0   0   0   0   0   1   1   0]\n",
      " [  5   2  83   0   0   0   3   0   3   1]\n",
      " [  0   0   0 127   0   0   0   0   0   0]\n",
      " [  0   0   1   0 137  14   0   0   0   0]\n",
      " [  1   0   1   0   0 145   0   0   0   0]\n",
      " [  2   2   0   0   0   0 152   0   2   0]\n",
      " [  0   1   0   0   0   0   0  50   0   0]\n",
      " [  3   0   1   0   0   0   2   0 125   0]\n",
      " [  3   0   1   0   0   0   0   0   0 136]]\n",
      "train...\n",
      "test...\n",
      "0.9340567612687813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.74      0.72        69\n",
      "           2       0.95      0.95      0.95       132\n",
      "           3       0.85      0.85      0.85        84\n",
      "           4       0.99      0.99      0.99       151\n",
      "           5       1.00      0.89      0.94       160\n",
      "           6       0.90      1.00      0.94       128\n",
      "           7       0.95      0.93      0.94       138\n",
      "           8       0.94      0.98      0.96        47\n",
      "           9       0.95      0.95      0.95       146\n",
      "          10       0.98      0.96      0.97       143\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1198\n",
      "   macro avg       0.92      0.92      0.92      1198\n",
      "weighted avg       0.94      0.93      0.93      1198\n",
      "\n",
      " initial test  labels size\n",
      "Confusion matrix, without normalization\n",
      "[[ 51   5   7   1   0   0   1   1   0   3]\n",
      " [  3 125   1   0   0   0   1   2   0   0]\n",
      " [  7   1  71   0   0   0   3   0   2   0]\n",
      " [  0   0   0 150   0   0   0   0   1   0]\n",
      " [  1   0   2   0 143  14   0   0   0   0]\n",
      " [  0   0   0   0   0 128   0   0   0   0]\n",
      " [  5   0   1   0   0   0 129   0   3   0]\n",
      " [  0   1   0   0   0   0   0  46   0   0]\n",
      " [  3   0   2   0   0   0   2   0 139   0]\n",
      " [  3   0   0   0   0   1   0   0   2 137]]\n",
      "train...\n",
      "test...\n",
      "0.9415692821368948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.74      0.74        62\n",
      "           2       0.96      0.97      0.97       143\n",
      "           3       0.95      0.80      0.87       102\n",
      "           4       0.99      1.00      1.00       161\n",
      "           5       1.00      0.95      0.97       139\n",
      "           6       0.95      0.99      0.97       143\n",
      "           7       0.92      0.96      0.94       141\n",
      "           8       0.96      0.98      0.97        54\n",
      "           9       0.89      0.91      0.90       128\n",
      "          10       0.94      0.98      0.96       125\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.93      0.93      0.93      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      " initial test  labels size\n",
      "Confusion matrix, without normalization\n",
      "[[ 46   4   2   1   0   0   2   0   2   5]\n",
      " [  2 139   0   0   0   0   1   1   0   0]\n",
      " [  7   1  82   0   0   0   2   1   7   2]\n",
      " [  0   0   0 161   0   0   0   0   0   0]\n",
      " [  0   0   0   0 132   7   0   0   0   0]\n",
      " [  0   0   1   0   0 142   0   0   0   0]\n",
      " [  0   0   1   0   0   0 135   0   5   0]\n",
      " [  0   1   0   0   0   0   0  53   0   0]\n",
      " [  4   0   0   0   0   0   7   0 116   1]\n",
      " [  3   0   0   0   0   0   0   0   0 122]]\n",
      "train...\n",
      "test...\n",
      "0.9440734557595993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.75      0.75        60\n",
      "           2       0.93      0.95      0.94       131\n",
      "           3       0.87      0.88      0.87        99\n",
      "           4       0.99      0.99      0.99       164\n",
      "           5       1.00      0.92      0.96       142\n",
      "           6       0.94      1.00      0.97       150\n",
      "           7       0.96      0.95      0.96       140\n",
      "           8       0.96      1.00      0.98        44\n",
      "           9       0.96      0.97      0.96       133\n",
      "          10       0.95      0.93      0.94       135\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.93      0.93      0.93      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      " initial test  labels size\n",
      "Confusion matrix, without normalization\n",
      "[[ 45   5   5   0   0   0   1   0   1   3]\n",
      " [  2 124   2   0   0   0   1   1   1   0]\n",
      " [  5   2  87   0   0   0   1   1   0   3]\n",
      " [  0   0   1 163   0   0   0   0   0   0]\n",
      " [  1   0   1   0 131   9   0   0   0   0]\n",
      " [  0   0   0   0   0 150   0   0   0   0]\n",
      " [  2   2   0   0   0   0 133   0   3   0]\n",
      " [  0   0   0   0   0   0   0  44   0   0]\n",
      " [  0   0   1   0   0   0   2   0 129   1]\n",
      " [  5   0   3   1   0   0   0   0   1 125]]\n",
      "train...\n",
      "test...\n",
      "0.9373956594323873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.77      0.79        94\n",
      "           2       0.96      0.96      0.96       133\n",
      "           3       0.91      0.80      0.85       102\n",
      "           4       0.99      0.99      0.99       151\n",
      "           5       1.00      0.93      0.96       128\n",
      "           6       0.94      1.00      0.97       154\n",
      "           7       0.92      0.96      0.94       140\n",
      "           8       0.92      0.97      0.95        36\n",
      "           9       0.94      0.93      0.93       135\n",
      "          10       0.92      0.98      0.95       125\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.93      0.93      0.93      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      " initial test  labels size\n",
      "Confusion matrix, without normalization\n",
      "[[ 72   2   5   1   0   1   1   1   4   7]\n",
      " [  2 128   0   0   0   0   1   1   1   0]\n",
      " [ 10   3  82   0   0   1   2   1   1   2]\n",
      " [  1   0   0 150   0   0   0   0   0   0]\n",
      " [  1   0   1   0 119   7   0   0   0   0]\n",
      " [  0   0   0   0   0 154   0   0   0   0]\n",
      " [  1   1   1   0   0   0 135   0   1   1]\n",
      " [  0   0   0   0   0   0   0  35   1   0]\n",
      " [  2   0   1   0   0   0   6   0 125   1]\n",
      " [  0   0   0   1   0   0   1   0   0 123]]\n",
      "train...\n",
      "test...\n",
      "0.9440734557595993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.73      0.76        75\n",
      "           2       0.94      0.98      0.96       134\n",
      "           3       0.87      0.86      0.86        90\n",
      "           4       1.00      1.00      1.00       139\n",
      "           5       1.00      0.91      0.95       158\n",
      "           6       0.92      1.00      0.96       156\n",
      "           7       0.95      0.97      0.96       144\n",
      "           8       0.98      1.00      0.99        45\n",
      "           9       0.97      0.93      0.95       123\n",
      "          10       0.96      0.98      0.97       134\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.94      0.93      0.94      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      " initial test  labels size\n",
      "Confusion matrix, without normalization\n",
      "[[ 55   7   7   0   0   1   0   0   1   4]\n",
      " [  0 131   1   0   0   0   2   0   0   0]\n",
      " [  8   1  77   0   0   0   1   1   0   2]\n",
      " [  0   0   0 139   0   0   0   0   0   0]\n",
      " [  1   0   1   0 144  12   0   0   0   0]\n",
      " [  0   0   0   0   0 156   0   0   0   0]\n",
      " [  1   0   1   0   0   0 139   0   3   0]\n",
      " [  0   0   0   0   0   0   0  45   0   0]\n",
      " [  3   1   0   0   0   0   5   0 114   0]\n",
      " [  1   0   2   0   0   0   0   0   0 131]]\n",
      "train...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test...\n",
      "0.9432387312186978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.77      0.76        70\n",
      "           2       0.95      0.95      0.95       125\n",
      "           3       0.90      0.92      0.91        85\n",
      "           4       1.00      0.99      1.00       147\n",
      "           5       1.00      0.91      0.96       151\n",
      "           6       0.92      1.00      0.96       161\n",
      "           7       0.96      0.93      0.95       140\n",
      "           8       0.98      0.98      0.98        45\n",
      "           9       0.90      0.93      0.92       138\n",
      "          10       0.99      0.96      0.98       136\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.94      0.94      0.93      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      " initial test  labels size\n",
      "Confusion matrix, without normalization\n",
      "[[ 54   3   5   0   0   0   0   0   7   1]\n",
      " [  2 119   1   0   0   0   1   1   1   0]\n",
      " [  3   0  78   0   0   0   1   0   3   0]\n",
      " [  1   0   0 146   0   0   0   0   0   0]\n",
      " [  0   0   0   0 138  13   0   0   0   0]\n",
      " [  0   0   0   0   0 161   0   0   0   0]\n",
      " [  3   2   2   0   0   0 130   0   3   0]\n",
      " [  0   1   0   0   0   0   0  44   0   0]\n",
      " [  5   0   1   0   0   0   3   0 129   0]\n",
      " [  4   0   0   0   0   1   0   0   0 131]]\n",
      "train...\n",
      "test...\n",
      "0.9332220367278798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.72      0.76        72\n",
      "           2       0.91      0.98      0.94       113\n",
      "           3       0.81      0.85      0.83        87\n",
      "           4       1.00      1.00      1.00       158\n",
      "           5       0.99      0.90      0.94       154\n",
      "           6       0.91      1.00      0.95       153\n",
      "           7       0.93      0.92      0.93       140\n",
      "           8       1.00      0.98      0.99        52\n",
      "           9       0.96      0.90      0.93       141\n",
      "          10       0.93      0.98      0.95       128\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1198\n",
      "   macro avg       0.93      0.92      0.92      1198\n",
      "weighted avg       0.93      0.93      0.93      1198\n",
      "\n",
      " initial test  labels size\n",
      "Confusion matrix, without normalization\n",
      "[[ 52   7   6   0   1   0   0   0   0   6]\n",
      " [  1 111   0   0   0   0   1   0   0   0]\n",
      " [  8   0  74   0   0   0   1   0   3   1]\n",
      " [  0   0   0 158   0   0   0   0   0   0]\n",
      " [  0   0   1   0 138  15   0   0   0   0]\n",
      " [  0   0   0   0   0 153   0   0   0   0]\n",
      " [  2   2   5   0   0   0 129   0   2   0]\n",
      " [  0   1   0   0   0   0   0  51   0   0]\n",
      " [  0   1   4   0   0   0   7   0 127   2]\n",
      " [  2   0   1   0   0   0   0   0   0 125]]\n",
      "train...\n",
      "test...\n",
      "0.9432387312186978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.68      0.74        71\n",
      "           2       0.96      0.99      0.97       138\n",
      "           3       0.87      0.85      0.86        86\n",
      "           4       1.00      1.00      1.00       152\n",
      "           5       1.00      0.89      0.94       150\n",
      "           6       0.90      1.00      0.95       148\n",
      "           7       0.97      0.92      0.94       157\n",
      "           8       1.00      1.00      1.00        61\n",
      "           9       0.92      0.99      0.95       112\n",
      "          10       0.92      0.99      0.95       123\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.94      0.93      0.93      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      " initial test  labels size\n",
      "Confusion matrix, without normalization\n",
      "[[ 48   4   8   0   0   0   2   0   0   9]\n",
      " [  0 136   1   0   0   0   0   0   1   0]\n",
      " [  5   0  73   0   0   0   2   0   4   2]\n",
      " [  0   0   0 152   0   0   0   0   0   0]\n",
      " [  0   0   0   0 134  16   0   0   0   0]\n",
      " [  0   0   0   0   0 148   0   0   0   0]\n",
      " [  4   1   2   0   0   0 145   0   5   0]\n",
      " [  0   0   0   0   0   0   0  61   0   0]\n",
      " [  0   0   0   0   0   0   1   0 111   0]\n",
      " [  1   0   0   0   0   0   0   0   0 122]]\n"
     ]
    }
   ],
   "source": [
    "# 进行十次随机森林测试\n",
    "for i in range(N):\n",
    "    # 以10%的比例进行交叉验证\n",
    "    # X_train, X_test, y_train, y_test = cross_validation.train_test_split(subfeatures,features_labels,test_size=0.1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subfeatures, labels, test_size=0.1)\n",
    "\n",
    "    # 进行训练\n",
    "    print('train...')\n",
    "    # 进行随机森林训练,30课树，不限制进程数\n",
    "    srf = RF(n_estimators=30, n_jobs=-1)\n",
    "    srf.fit(X_train, y_train)\n",
    "\n",
    "    # 预试\n",
    "    print(\"test...\")\n",
    "    c_test = srf.predict(X_test)\n",
    "\n",
    "\n",
    "    # 计算预测划分准确率\n",
    "    score = srf.score(X_test, y_test)\n",
    "    print(score)\n",
    "    print(classification_report(y_test,c_test))\n",
    "    # print(\"c_test\")\n",
    "    # print(c_test)\n",
    "    # print('y_test')\n",
    "    # print(y_test)\n",
    "\n",
    "    avgscore = avgscore + score\n",
    "    recallscore = recallscore + recall_score(y_test,c_test,average=\"macro\")\n",
    "    precisionscore = precisionscore + precision_score(y_test,c_test,average=\"macro\")\n",
    "    print(\" initial test  labels size\")\n",
    "\n",
    "    # 通过混淆矩阵进行结果标示\n",
    "    cm = confusion_matrix(y_test, c_test)\n",
    "    np.set_printoptions(threshold=10000)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(str(cm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgscore....\n",
      "0.9394824707846411\n",
      "False positive rate\n",
      "0.06964328342466497\n",
      "false negative rate\n",
      "0.07075885635784418\n"
     ]
    }
   ],
   "source": [
    "# 输出N次的平均准确率\n",
    "avgscore = avgscore / N\n",
    "recallscore = recallscore / N\n",
    "precisionscore = precisionscore / N\n",
    "\n",
    "print('avgscore....')\n",
    "print(avgscore)\n",
    "print('False positive rate')\n",
    "print(1-precisionscore)\n",
    "print('false negative rate')\n",
    "print(1-recallscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
