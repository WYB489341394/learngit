{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Summary of the file.\\n\\n    功能：\\n        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以KNN方法划分并获得准确率，通过混淆矩阵标示结果。\\n\\n    输出：\\n        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "import pylab as pl\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score\n",
    "import math\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "''' Summary of the file.\n",
    "\n",
    "    功能：\n",
    "        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以KNN方法划分并获得准确率，通过混淆矩阵标示结果。\n",
    "\n",
    "    输出：\n",
    "        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBP_block\n",
    "subfeatures=pd.read_csv(r'E:\\test\\process_lpb_block2\\lbp_kmeans_hist_feature.csv',header=None)\n",
    "labels = pd.read_csv(r'E:\\test\\process_lpb_block2\\ClassNo_full.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把先提取的家族10的特征挪到最后\n",
    "result = pd.concat([subfeatures, labels], axis=1,ignore_index=True)\n",
    "# print(result)\n",
    "result2 = result.iloc[:1431]\n",
    "list = []\n",
    "for i in range(len(result)):\n",
    "    if result.iloc[i,-1] == 10:\n",
    "        list.append(i)\n",
    "result = result.drop(list)\n",
    "result = pd.concat([result,result2],axis=0,ignore_index=True)\n",
    "subfeatures = result.iloc[:,:-1]\n",
    "labels = result.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 平均准确率归零\n",
    "avgscore = 0\n",
    "recallscore = 0\n",
    "precisionscore = 0\n",
    "N = 10\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.51      0.57       125\n",
      "           2       0.90      0.90      0.90       143\n",
      "           3       0.72      0.74      0.73       116\n",
      "           4       0.88      0.82      0.85       138\n",
      "           5       0.94      0.99      0.97       153\n",
      "           6       0.93      0.99      0.96       130\n",
      "           7       0.82      0.91      0.87       149\n",
      "           8       1.00      0.99      0.99        70\n",
      "           9       0.82      0.78      0.80       153\n",
      "          10       0.88      0.92      0.90       138\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1315\n",
      "   macro avg       0.85      0.85      0.85      1315\n",
      "weighted avg       0.85      0.85      0.85      1315\n",
      "\n",
      "0.8539923954372624\n",
      "Confusion matrix, without normalization\n",
      "[[ 64   6  12   6   5   3   6   0  12  11]\n",
      " [  4 128   4   0   0   1   4   0   1   1]\n",
      " [  8   2  86   1   2   2  10   0   1   4]\n",
      " [  5   1   6 113   0   1   2   0  10   0]\n",
      " [  0   0   1   0 152   0   0   0   0   0]\n",
      " [  0   0   0   0   1 129   0   0   0   0]\n",
      " [  4   2   3   1   0   1 136   0   1   1]\n",
      " [  0   1   0   0   0   0   0  69   0   0]\n",
      " [  8   2   7   7   1   1   7   0 119   1]\n",
      " [  8   0   1   0   0   1   0   0   1 127]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.48      0.49       122\n",
      "           2       0.91      0.86      0.89       125\n",
      "           3       0.72      0.74      0.73       116\n",
      "           4       0.89      0.83      0.86       167\n",
      "           5       0.95      0.98      0.96       134\n",
      "           6       1.00      0.99      1.00       156\n",
      "           7       0.84      0.90      0.87       147\n",
      "           8       0.97      0.97      0.97        76\n",
      "           9       0.76      0.74      0.75       144\n",
      "          10       0.80      0.86      0.83       128\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1315\n",
      "   macro avg       0.83      0.84      0.83      1315\n",
      "weighted avg       0.84      0.84      0.84      1315\n",
      "\n",
      "0.8365019011406845\n",
      "Confusion matrix, without normalization\n",
      "[[ 59   4  15   4   1   0   5   0  15  19]\n",
      " [  7 108   4   0   0   0   4   0   1   1]\n",
      " [ 13   1  86   0   5   0   3   1   6   1]\n",
      " [ 13   0   3 139   0   0   0   0  10   2]\n",
      " [  0   1   0   2 131   0   0   0   0   0]\n",
      " [  0   0   0   0   0 155   1   0   0   0]\n",
      " [  4   1   5   2   0   0 132   1   1   1]\n",
      " [  0   2   0   0   0   0   0  74   0   0]\n",
      " [ 11   0   4   7   1   0  12   0 106   3]\n",
      " [ 10   2   2   2   0   0   1   0   1 110]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.56      0.57       118\n",
      "           2       0.92      0.91      0.91       144\n",
      "           3       0.65      0.68      0.66        96\n",
      "           4       0.91      0.83      0.87       152\n",
      "           5       0.95      1.00      0.97       151\n",
      "           6       0.99      0.99      0.99       140\n",
      "           7       0.89      0.90      0.90       147\n",
      "           8       0.97      1.00      0.98        83\n",
      "           9       0.80      0.79      0.80       151\n",
      "          10       0.85      0.87      0.86       133\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1315\n",
      "   macro avg       0.85      0.85      0.85      1315\n",
      "weighted avg       0.86      0.86      0.86      1315\n",
      "\n",
      "0.8585551330798479\n",
      "Confusion matrix, without normalization\n",
      "[[ 66   5  15   5   2   1   2   1  10  11]\n",
      " [  3 131   4   0   1   0   1   1   1   2]\n",
      " [ 11   3  65   2   2   0   4   1   7   1]\n",
      " [ 10   0   5 126   0   0   0   0   9   2]\n",
      " [  0   0   0   0 151   0   0   0   0   0]\n",
      " [  0   0   1   0   0 139   0   0   0   0]\n",
      " [  5   1   5   1   0   0 133   0   0   2]\n",
      " [  0   0   0   0   0   0   0  83   0   0]\n",
      " [  9   1   3   5   3   0   9   0 119   2]\n",
      " [ 11   2   2   0   0   0   0   0   2 116]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.52      0.58       112\n",
      "           2       0.89      0.85      0.87       133\n",
      "           3       0.70      0.72      0.71       119\n",
      "           4       0.83      0.88      0.86       142\n",
      "           5       0.95      0.99      0.97       158\n",
      "           6       0.97      1.00      0.99       145\n",
      "           7       0.88      0.91      0.89       145\n",
      "           8       0.97      0.99      0.98        76\n",
      "           9       0.81      0.77      0.79       139\n",
      "          10       0.86      0.90      0.88       146\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1315\n",
      "   macro avg       0.85      0.85      0.85      1315\n",
      "weighted avg       0.85      0.86      0.86      1315\n",
      "\n",
      "0.8585551330798479\n",
      "Confusion matrix, without normalization\n",
      "[[ 58   3  12  12   2   0   1   1  12  11]\n",
      " [  5 113   9   0   1   0   2   0   2   1]\n",
      " [ 10   7  86   3   0   2   5   0   3   3]\n",
      " [  1   1   3 125   3   2   0   0   6   1]\n",
      " [  0   0   0   0 157   0   0   0   0   1]\n",
      " [  0   0   0   0   0 145   0   0   0   0]\n",
      " [  3   1   5   2   1   0 132   0   0   1]\n",
      " [  0   1   0   0   0   0   0  75   0   0]\n",
      " [  3   1   6   8   1   0   9   1 107   3]\n",
      " [  9   0   2   0   1   0   1   0   2 131]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.58      0.59       125\n",
      "           2       0.89      0.84      0.87       127\n",
      "           3       0.73      0.75      0.74       122\n",
      "           4       0.84      0.83      0.84       147\n",
      "           5       0.96      0.99      0.97       161\n",
      "           6       0.98      0.99      0.99       138\n",
      "           7       0.85      0.91      0.88       138\n",
      "           8       0.96      0.99      0.97        72\n",
      "           9       0.81      0.78      0.80       156\n",
      "          10       0.90      0.87      0.88       129\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1315\n",
      "   macro avg       0.85      0.85      0.85      1315\n",
      "weighted avg       0.85      0.85      0.85      1315\n",
      "\n",
      "0.8517110266159695\n",
      "Confusion matrix, without normalization\n",
      "[[ 73   3  14   9   0   2   4   2   8  10]\n",
      " [  9 107   8   0   1   0   2   0   0   0]\n",
      " [ 12   5  91   3   3   0   2   1   4   1]\n",
      " [  4   1   1 122   2   1   4   0  11   1]\n",
      " [  1   0   0   0 159   0   0   0   0   1]\n",
      " [  1   0   0   0   0 137   0   0   0   0]\n",
      " [  4   1   3   2   0   0 126   0   2   0]\n",
      " [  1   0   0   0   0   0   0  71   0   0]\n",
      " [  8   2   6   8   0   0  10   0 122   0]\n",
      " [  9   1   1   1   1   0   1   0   3 112]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.57      0.63       141\n",
      "           2       0.86      0.86      0.86       132\n",
      "           3       0.65      0.72      0.68       102\n",
      "           4       0.86      0.78      0.82       134\n",
      "           5       0.93      1.00      0.96       134\n",
      "           6       0.95      1.00      0.97       156\n",
      "           7       0.89      0.90      0.89       160\n",
      "           8       0.94      0.99      0.96        68\n",
      "           9       0.76      0.75      0.75       143\n",
      "          10       0.89      0.90      0.89       145\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1315\n",
      "   macro avg       0.84      0.85      0.84      1315\n",
      "weighted avg       0.84      0.84      0.84      1315\n",
      "\n",
      "0.8448669201520913\n",
      "Confusion matrix, without normalization\n",
      "[[ 81   5  14   6   3   3   4   1   9  15]\n",
      " [  3 114   8   0   1   0   4   1   1   0]\n",
      " [  7   5  73   2   4   3   4   1   3   0]\n",
      " [  6   1   4 104   1   2   0   0  16   0]\n",
      " [  0   0   0   0 134   0   0   0   0   0]\n",
      " [  0   0   0   0   0 156   0   0   0   0]\n",
      " [  6   3   4   1   0   0 144   1   1   0]\n",
      " [  0   0   1   0   0   0   0  67   0   0]\n",
      " [  8   3   9   8   0   0   6   0 107   2]\n",
      " [  7   1   0   0   1   1   0   0   4 131]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.61      0.60       126\n",
      "           2       0.89      0.88      0.89       142\n",
      "           3       0.78      0.71      0.74       119\n",
      "           4       0.87      0.80      0.83       133\n",
      "           5       0.96      1.00      0.98       132\n",
      "           6       0.95      1.00      0.98       143\n",
      "           7       0.82      0.91      0.86       146\n",
      "           8       0.93      1.00      0.97        84\n",
      "           9       0.81      0.73      0.77       146\n",
      "          10       0.87      0.85      0.86       144\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1315\n",
      "   macro avg       0.85      0.85      0.85      1315\n",
      "weighted avg       0.85      0.85      0.85      1315\n",
      "\n",
      "0.847148288973384\n",
      "Confusion matrix, without normalization\n",
      "[[ 77   8   9   4   0   2   6   2   8  10]\n",
      " [  7 125   6   0   0   0   1   1   1   1]\n",
      " [ 12   3  84   3   3   3   4   3   2   2]\n",
      " [  4   0   3 106   1   1   6   0  11   1]\n",
      " [  0   0   0   0 132   0   0   0   0   0]\n",
      " [  0   0   0   0   0 143   0   0   0   0]\n",
      " [  1   2   5   0   0   0 133   0   2   3]\n",
      " [  0   0   0   0   0   0   0  84   0   0]\n",
      " [ 14   0   1   8   1   1  12   0 107   2]\n",
      " [ 16   2   0   1   0   0   1   0   1 123]]\n",
      "train...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.47      0.54       140\n",
      "           2       0.87      0.84      0.86       147\n",
      "           3       0.70      0.72      0.71       103\n",
      "           4       0.88      0.83      0.86       154\n",
      "           5       0.95      0.99      0.97       158\n",
      "           6       0.96      0.99      0.97       116\n",
      "           7       0.83      0.91      0.87       149\n",
      "           8       1.00      0.96      0.98        82\n",
      "           9       0.71      0.75      0.73       130\n",
      "          10       0.85      0.93      0.89       136\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1315\n",
      "   macro avg       0.84      0.84      0.84      1315\n",
      "weighted avg       0.83      0.84      0.84      1315\n",
      "\n",
      "0.8395437262357415\n",
      "Confusion matrix, without normalization\n",
      "[[ 66  10  13   3   4   1  11   0  16  16]\n",
      " [ 11 124   5   0   1   1   2   0   2   1]\n",
      " [  4   2  74   3   2   0   3   0  10   5]\n",
      " [ 10   0   3 128   1   2   0   0  10   0]\n",
      " [  0   0   0   1 157   0   0   0   0   0]\n",
      " [  0   0   0   0   1 115   0   0   0   0]\n",
      " [  1   5   4   2   0   0 136   0   1   0]\n",
      " [  2   1   0   0   0   0   0  79   0   0]\n",
      " [  4   1   6   8   0   0  12   0  98   1]\n",
      " [  6   0   1   0   0   1   0   0   1 127]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.55      0.55       115\n",
      "           2       0.93      0.86      0.89       134\n",
      "           3       0.68      0.68      0.68       115\n",
      "           4       0.85      0.84      0.84       166\n",
      "           5       0.97      0.99      0.98       153\n",
      "           6       0.98      0.99      0.99       140\n",
      "           7       0.88      0.88      0.88       137\n",
      "           8       0.92      0.97      0.94        68\n",
      "           9       0.78      0.82      0.80       146\n",
      "          10       0.87      0.86      0.86       141\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1315\n",
      "   macro avg       0.84      0.84      0.84      1315\n",
      "weighted avg       0.85      0.85      0.85      1315\n",
      "\n",
      "0.8463878326996198\n",
      "Confusion matrix, without normalization\n",
      "[[ 63   4   9   6   0   1   7   1  13  11]\n",
      " [  9 115   7   0   0   0   0   1   1   1]\n",
      " [ 12   3  78   6   2   1   4   3   2   4]\n",
      " [  7   0   5 139   0   0   1   0  14   0]\n",
      " [  0   0   0   0 152   0   0   0   1   0]\n",
      " [  0   0   0   0   0 139   0   0   1   0]\n",
      " [  7   0   5   1   0   0 121   0   2   1]\n",
      " [  1   0   1   0   0   0   0  66   0   0]\n",
      " [  7   0   3  10   2   0   3   1 119   1]\n",
      " [  9   1   6   2   0   1   1   0   0 121]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.48      0.53       137\n",
      "           2       0.91      0.90      0.91       141\n",
      "           3       0.77      0.72      0.75       120\n",
      "           4       0.82      0.81      0.82       139\n",
      "           5       0.90      1.00      0.95       130\n",
      "           6       0.99      0.99      0.99       144\n",
      "           7       0.84      0.95      0.89       131\n",
      "           8       0.98      0.97      0.98        65\n",
      "           9       0.75      0.74      0.75       148\n",
      "          10       0.83      0.88      0.85       160\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1315\n",
      "   macro avg       0.84      0.84      0.84      1315\n",
      "weighted avg       0.83      0.84      0.83      1315\n",
      "\n",
      "0.8387832699619772\n",
      "Confusion matrix, without normalization\n",
      "[[ 66   5  11  13   3   1   3   0  14  21]\n",
      " [  3 127   5   0   0   0   3   0   2   1]\n",
      " [ 12   4  87   2   6   0   1   1   3   4]\n",
      " [  6   0   3 113   1   0   2   0  13   1]\n",
      " [  0   0   0   0 130   0   0   0   0   0]\n",
      " [  0   0   0   0   1 143   0   0   0   0]\n",
      " [  3   1   0   1   0   0 124   0   2   0]\n",
      " [  0   1   0   0   0   0   0  63   0   1]\n",
      " [  9   1   1   9   3   0  14   0 110   1]\n",
      " [ 12   0   6   0   0   0   0   0   2 140]]\n"
     ]
    }
   ],
   "source": [
    "# 进行十次SVM_Tree测试\n",
    "for i in range(N):\n",
    "    # 以10%的比例进行交叉验证\n",
    "    # X_train, X_test, y_train, y_test = cross_validation.train_test_split(subfeatures, features_labels, test_size=0.1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subfeatures, labels, test_size=0.1)\n",
    "\n",
    "    # 进行训练\n",
    "    print('train...')\n",
    "    # 进行SVC训练 使用线性核\n",
    "    # clf = SVC(kernel='linear')                     # 高斯核 rbf\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(X_train.values, y_train.values.reshape(len(y_train)))\n",
    "\n",
    "    # 预试\n",
    "    print('test...')\n",
    "    c_test = clf.predict(X_test.values)\n",
    "\n",
    "    # print(y_test)\n",
    "    # print(c_test)\n",
    "\n",
    "    # 计算预测划分准确率\n",
    "    print('accurary...')\n",
    "    score = clf.score(X_test.values, y_test.values.reshape(len(y_test)))\n",
    "    print(classification_report(y_test, c_test))\n",
    "    avgscore = avgscore + score\n",
    "    recallscore = recallscore + recall_score(y_test, c_test, average=\"macro\")\n",
    "    precisionscore = precisionscore + precision_score(y_test, c_test, average=\"macro\")\n",
    "    print(score)\n",
    "\n",
    "    # 通过混淆矩阵进行结果标示\n",
    "    cm = confusion_matrix(y_test, c_test)\n",
    "    np.set_printoptions(threshold=10000)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(str(cm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgscore....\n",
      "0.8476045627376425\n",
      "False positive rate\n",
      "0.15519534073583263\n",
      "false negative rate\n",
      "0.1524216396430944\n"
     ]
    }
   ],
   "source": [
    "# 输出N次的平均准确率\n",
    "avgscore = avgscore / N\n",
    "recallscore = recallscore / N\n",
    "precisionscore = precisionscore / N\n",
    "\n",
    "print('avgscore....')\n",
    "print(avgscore)\n",
    "print('False positive rate')\n",
    "print(1-precisionscore)\n",
    "print('false negative rate')\n",
    "print(1-recallscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
