{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Summary of the file.\\n\\n    功能：\\n        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以KNN方法划分并获得准确率，通过混淆矩阵标示结果。\\n\\n    输出：\\n        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "''' Summary of the file.\n",
    "\n",
    "    功能：\n",
    "        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以KNN方法划分并获得准确率，通过混淆矩阵标示结果。\n",
    "\n",
    "    输出：\n",
    "        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验室的15000条恶意样本，分为十个家族\n",
    "\n",
    "# # GIST特征值\n",
    "subfeatures = pd.read_csv(r'E:\\test\\process_gist2\\gist_f_train_full.csv',header=None)\n",
    "labels = pd.read_csv(r'E:\\test\\process_gist2\\CNO_full.txt',header=None)\n",
    "\n",
    "# # text段代码提取二进制代码，生成图像的LBP特征\n",
    "# subfeatures=pd.read_csv('/home/stack/Data/Output/lbp_only_text/feature_train_full.csv',header=None)\n",
    "# labels = pd.read_csv('/home/stack/Data/Output/lbp_only_text/CNO_full.txt',header=None)\n",
    "\n",
    "# text段 GIST特征\n",
    "# #subfeatures=pd.read_csv(r'E:\\test\\process_gist\\gist_f_train_full.csv',header=None)\n",
    "# #labels = pd.read_csv(r'E:\\test\\process_gist\\CNO_full.txt',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           3         7        11       15        19        23        27   \\\n",
      "1      0.23956  0.237630  0.26381  0.35730  0.084427  0.080236  0.092387   \n",
      "3      0.18744  0.440850  0.18135  0.35175  0.063265  0.151440  0.060828   \n",
      "5      0.31370  0.401330  0.28956  0.36062  0.108090  0.140850  0.100460   \n",
      "7      0.33926  0.308770  0.34102  0.23674  0.116410  0.107190  0.115250   \n",
      "8      0.32031  0.239320  0.23991  0.34135  0.112280  0.082963  0.081881   \n",
      "9      0.16760  0.245140  0.10508  0.26727  0.059215  0.082231  0.034979   \n",
      "10     0.25577  0.327910  0.34899  0.30896  0.086576  0.113710  0.120750   \n",
      "11     0.40402  0.350880  0.28529  0.27655  0.139750  0.120110  0.097166   \n",
      "12     0.26433  0.293290  0.26679  0.28659  0.087935  0.101590  0.090306   \n",
      "13     0.40280  0.359690  0.36875  0.41921  0.138480  0.126010  0.128750   \n",
      "14     0.27751  0.079934  0.24908  0.10085  0.094805  0.028159  0.085137   \n",
      "15     0.31480  0.232990  0.29155  0.27720  0.110780  0.080156  0.101760   \n",
      "17     0.32580  0.281480  0.36490  0.30851  0.112370  0.098397  0.125840   \n",
      "18     0.40505  0.350870  0.28529  0.27533  0.140120  0.120110  0.097166   \n",
      "21     0.34668  0.296430  0.37054  0.23864  0.120360  0.102810  0.129360   \n",
      "23     0.32627  0.179140  0.23115  0.34916  0.113270  0.060430  0.080505   \n",
      "24     0.35351  0.369930  0.30373  0.29287  0.123340  0.130510  0.106950   \n",
      "25     0.18990  0.278110  0.32695  0.24038  0.066461  0.095366  0.112660   \n",
      "27     0.30605  0.205940  0.18302  0.28251  0.104960  0.071215  0.063274   \n",
      "28     0.41524  0.285140  0.22204  0.31553  0.143680  0.095703  0.074220   \n",
      "29     0.40425  0.350800  0.28520  0.27587  0.139840  0.120090  0.097135   \n",
      "31     0.19706  0.243620  0.28757  0.31960  0.067522  0.081592  0.101060   \n",
      "32     0.41160  0.321990  0.28863  0.36456  0.143660  0.111590  0.102220   \n",
      "33     0.25889  0.294550  0.33068  0.28831  0.089407  0.102620  0.115560   \n",
      "34     0.18989  0.278110  0.32671  0.24126  0.066460  0.095367  0.112570   \n",
      "35     0.45593  0.281450  0.21245  0.29098  0.156000  0.097756  0.072614   \n",
      "36     0.20941  0.326000  0.27696  0.18823  0.072749  0.112550  0.096013   \n",
      "37     0.35649  0.341980  0.31115  0.18712  0.121390  0.118760  0.109630   \n",
      "38     0.19792  0.320990  0.43340  0.23930  0.068789  0.111590  0.151030   \n",
      "39     0.30806  0.316280  0.32630  0.27064  0.107920  0.109340  0.114510   \n",
      "...        ...       ...      ...      ...       ...       ...       ...   \n",
      "14533  0.46258  0.235150  0.28923  0.42777  0.157330  0.082607  0.101560   \n",
      "14534  0.38386  0.278310  0.31486  0.23606  0.132700  0.093965  0.108140   \n",
      "14535  0.16492  0.313450  0.18284  0.27730  0.055584  0.106650  0.062045   \n",
      "14536  0.30400  0.244020  0.31968  0.36357  0.105350  0.082668  0.108130   \n",
      "14537  0.33063  0.308390  0.35920  0.35039  0.114370  0.105580  0.123340   \n",
      "14538  0.17280  0.361940  0.23742  0.20850  0.060491  0.125950  0.081468   \n",
      "14539  0.21129  0.377560  0.23655  0.26155  0.070894  0.131010  0.080132   \n",
      "14540  0.36042  0.274140  0.15265  0.44814  0.124670  0.095230  0.052730   \n",
      "14541  0.32718  0.264260  0.35576  0.25601  0.112520  0.090982  0.124750   \n",
      "14542  0.15066  0.097000  0.29147  0.13712  0.052388  0.033027  0.098898   \n",
      "14543  0.25714  0.288890  0.18688  0.35637  0.087883  0.099628  0.065582   \n",
      "14544  0.23473  0.317060  0.23479  0.28583  0.081888  0.106850  0.082523   \n",
      "14545  0.20726  0.085805  0.25649  0.36953  0.072252  0.029894  0.087491   \n",
      "14546  0.19134  0.313740  0.27836  0.27856  0.066907  0.106970  0.096664   \n",
      "14547  0.20726  0.085805  0.25649  0.36953  0.072252  0.029894  0.087491   \n",
      "14548  0.28021  0.221000  0.16258  0.24798  0.097627  0.073439  0.056734   \n",
      "14549  0.22535  0.342270  0.30017  0.35252  0.077384  0.118160  0.103390   \n",
      "14550  0.37156  0.282460  0.21856  0.30451  0.127280  0.097989  0.073348   \n",
      "14551  0.33058  0.282680  0.23087  0.38432  0.112430  0.099010  0.079154   \n",
      "14553  0.23473  0.317060  0.23479  0.28583  0.081888  0.106850  0.082523   \n",
      "14554  0.16492  0.313450  0.18284  0.27730  0.055584  0.106650  0.062045   \n",
      "14555  0.16492  0.313450  0.18284  0.27730  0.055584  0.106650  0.062045   \n",
      "14557  0.23473  0.317060  0.23479  0.28583  0.081888  0.106850  0.082523   \n",
      "14558  0.25315  0.344990  0.30165  0.32071  0.084959  0.120390  0.104240   \n",
      "14559  0.33531  0.236030  0.20859  0.43802  0.115980  0.080311  0.072415   \n",
      "14560  0.17280  0.361940  0.23742  0.20850  0.060491  0.125950  0.081468   \n",
      "14561  0.26254  0.313660  0.30621  0.31699  0.089559  0.109720  0.106520   \n",
      "14562  0.19682  0.316910  0.13378  0.26958  0.067680  0.110030  0.046450   \n",
      "14563  0.27122  0.390580  0.32077  0.16556  0.094821  0.136300  0.111360   \n",
      "14564  0.22497  0.304430  0.34050  0.24353  0.076041  0.105750  0.118650   \n",
      "\n",
      "            31        35        39   ...           475           479  \\\n",
      "1      0.124000  0.004294  0.003916  ...  2.619500e-06  2.962000e-06   \n",
      "3      0.122650  0.003085  0.007497  ...  1.983100e-06  1.438800e-06   \n",
      "5      0.124100  0.005385  0.007120  ...  1.695300e-06  1.547300e-06   \n",
      "7      0.080384  0.005759  0.005360  ...  1.732600e-06  2.189200e-06   \n",
      "8      0.119700  0.005676  0.004146  ...  1.802400e-06  1.941000e-06   \n",
      "9      0.091403  0.003015  0.003992  ...  2.257700e-06  1.023600e-06   \n",
      "10     0.106520  0.004228  0.005684  ...  1.121600e-06  1.855400e-06   \n",
      "11     0.093252  0.006970  0.005926  ...  1.242700e-06  7.426700e-07   \n",
      "12     0.098806  0.004225  0.005075  ...  2.373700e-06  2.974000e-06   \n",
      "13     0.144910  0.006859  0.006359  ...  1.039100e-06  9.599000e-07   \n",
      "14     0.034229  0.004672  0.001426  ...  3.733700e-06  4.033500e-06   \n",
      "15     0.097210  0.005620  0.003977  ...  5.278100e-07  1.224500e-06   \n",
      "17     0.107650  0.005587  0.004958  ...  1.121900e-06  5.795800e-07   \n",
      "18     0.092819  0.006989  0.005927  ...  1.250400e-06  7.504600e-07   \n",
      "21     0.080708  0.006026  0.005145  ...  4.325500e-07  3.449700e-07   \n",
      "23     0.121510  0.005671  0.002942  ...  3.772100e-07  5.252200e-07   \n",
      "24     0.103630  0.006202  0.006634  ...  6.991600e-07  5.836900e-07   \n",
      "25     0.083335  0.003357  0.004713  ...  4.290900e-07  5.596800e-07   \n",
      "27     0.095530  0.005194  0.003551  ...  1.810300e-06  1.183200e-06   \n",
      "28     0.106570  0.007161  0.004634  ...  1.468000e-06  1.484600e-06   \n",
      "29     0.092998  0.006976  0.005926  ...  1.249800e-06  7.566200e-07   \n",
      "31     0.109570  0.003337  0.003959  ...  1.875100e-06  2.034600e-06   \n",
      "32     0.127370  0.007222  0.005577  ...  2.083900e-06  2.476500e-06   \n",
      "33     0.100730  0.004463  0.005153  ...  2.617100e-06  3.515200e-06   \n",
      "34     0.083643  0.003357  0.004713  ...  4.341500e-07  5.665300e-07   \n",
      "35     0.099145  0.007695  0.004909  ...  2.251200e-06  2.193100e-06   \n",
      "36     0.064682  0.003644  0.005600  ...  3.691600e-07  2.996700e-07   \n",
      "37     0.064741  0.005959  0.005942  ...  3.227400e-06  4.511700e-06   \n",
      "38     0.083466  0.003447  0.005595  ...  9.960700e-07  3.757200e-07   \n",
      "39     0.094685  0.005449  0.005449  ...  7.909900e-07  5.631700e-07   \n",
      "...         ...       ...       ...  ...           ...           ...   \n",
      "14533  0.150210  0.007708  0.004188  ...  1.312600e-06  5.793800e-07   \n",
      "14534  0.080625  0.006611  0.004572  ...  1.671700e-06  1.259500e-06   \n",
      "14535  0.096504  0.002704  0.005239  ...  4.039400e-06  2.166700e-06   \n",
      "14536  0.126320  0.005265  0.004038  ...  1.347700e-06  5.598600e-07   \n",
      "14537  0.121360  0.005700  0.005205  ...  1.342800e-06  2.009600e-06   \n",
      "14538  0.072149  0.003057  0.006326  ...  3.958300e-07  8.130500e-07   \n",
      "14539  0.089222  0.003449  0.006547  ...  7.593300e-07  9.792000e-07   \n",
      "14540  0.154510  0.006217  0.004770  ...  4.418000e-07  1.900400e-07   \n",
      "14541  0.085949  0.005579  0.004512  ...  1.279500e-06  1.259900e-06   \n",
      "14542  0.046033  0.002626  0.001625  ...  1.732500e-06  1.543900e-06   \n",
      "14543  0.121140  0.004338  0.004957  ...  1.083300e-06  7.281400e-07   \n",
      "14544  0.098605  0.004123  0.005193  ...  2.136300e-06  2.719600e-06   \n",
      "14545  0.129050  0.003631  0.001503  ...  8.874500e-07  1.495700e-06   \n",
      "14546  0.096579  0.003379  0.005258  ...  1.132100e-06  9.824800e-07   \n",
      "14547  0.129050  0.003631  0.001503  ...  8.874500e-07  1.495700e-06   \n",
      "14548  0.083927  0.004905  0.003529  ...  1.088200e-06  2.225500e-06   \n",
      "14549  0.124480  0.003840  0.005883  ...  5.350200e-07  9.888200e-07   \n",
      "14550  0.103790  0.006282  0.004905  ...  1.800500e-06  1.699100e-06   \n",
      "14551  0.134880  0.005512  0.005004  ...  1.685300e-06  1.430700e-06   \n",
      "14553  0.098605  0.004123  0.005193  ...  2.136300e-06  2.719600e-06   \n",
      "14554  0.096504  0.002704  0.005239  ...  4.039400e-06  2.166700e-06   \n",
      "14555  0.096504  0.002704  0.005239  ...  4.039400e-06  2.166700e-06   \n",
      "14557  0.098605  0.004123  0.005193  ...  2.136300e-06  2.719600e-06   \n",
      "14558  0.112730  0.004115  0.006049  ...  1.437100e-06  1.692500e-06   \n",
      "14559  0.153130  0.005782  0.003945  ...  1.191700e-06  1.194100e-06   \n",
      "14560  0.072149  0.003057  0.006326  ...  3.958300e-07  8.130500e-07   \n",
      "14561  0.109670  0.004410  0.005534  ...  6.227800e-07  5.359200e-07   \n",
      "14562  0.091316  0.003368  0.005509  ...  1.655400e-06  1.181600e-06   \n",
      "14563  0.058040  0.004777  0.006854  ...  5.597300e-07  3.504700e-07   \n",
      "14564  0.082985  0.003703  0.005294  ...  1.515200e-06  1.428900e-06   \n",
      "\n",
      "            483       487       491       495       499       503       507  \\\n",
      "1      0.000342  0.000389  0.000527  0.000630  0.009008  0.009816  0.015357   \n",
      "3      0.000312  0.000413  0.000378  0.000252  0.009542  0.011749  0.010472   \n",
      "5      0.000142  0.000256  0.000336  0.000323  0.003542  0.007033  0.009642   \n",
      "7      0.000169  0.000233  0.000346  0.000448  0.004295  0.006251  0.009986   \n",
      "8      0.000369  0.000339  0.000340  0.000370  0.010563  0.009204  0.009186   \n",
      "9      0.000169  0.000418  0.000464  0.000250  0.004011  0.011534  0.014087   \n",
      "10     0.000154  0.000188  0.000260  0.000408  0.004217  0.006174  0.008846   \n",
      "11     0.000260  0.000220  0.000262  0.000174  0.007785  0.006966  0.008094   \n",
      "12     0.000234  0.000346  0.000492  0.000627  0.005725  0.009282  0.014921   \n",
      "13     0.000412  0.000310  0.000233  0.000213  0.012655  0.009775  0.007487   \n",
      "14     0.000583  0.000632  0.000721  0.000789  0.015723  0.017264  0.020019   \n",
      "15     0.000168  0.000117  0.000108  0.000227  0.004250  0.003410  0.003147   \n",
      "17     0.000095  0.000177  0.000224  0.000134  0.002305  0.004694  0.006539   \n",
      "18     0.000262  0.000222  0.000264  0.000175  0.007865  0.007022  0.008140   \n",
      "21     0.000105  0.000092  0.000080  0.000058  0.003404  0.002907  0.002126   \n",
      "23     0.000096  0.000039  0.000064  0.000092  0.002968  0.001530  0.001627   \n",
      "24     0.000106  0.000164  0.000143  0.000116  0.002696  0.004883  0.004272   \n",
      "25     0.000200  0.000086  0.000084  0.000097  0.005596  0.002545  0.002350   \n",
      "27     0.000238  0.000303  0.000345  0.000198  0.007807  0.009354  0.009594   \n",
      "28     0.000480  0.000288  0.000258  0.000262  0.013200  0.006768  0.006692   \n",
      "29     0.000262  0.000221  0.000264  0.000177  0.007836  0.007001  0.008146   \n",
      "31     0.000169  0.000302  0.000337  0.000378  0.005418  0.008246  0.008625   \n",
      "32     0.000332  0.000346  0.000403  0.000480  0.008974  0.009469  0.011219   \n",
      "33     0.000091  0.000288  0.000531  0.000718  0.002297  0.008221  0.015708   \n",
      "34     0.000200  0.000086  0.000084  0.000098  0.005598  0.002557  0.002375   \n",
      "35     0.000246  0.000380  0.000449  0.000445  0.006148  0.010814  0.012923   \n",
      "36     0.000246  0.000160  0.000089  0.000075  0.006866  0.004906  0.003067   \n",
      "37     0.000068  0.000276  0.000626  0.000901  0.001641  0.007178  0.017592   \n",
      "38     0.000356  0.000276  0.000178  0.000073  0.010053  0.007210  0.004579   \n",
      "39     0.000135  0.000119  0.000167  0.000129  0.004060  0.003813  0.005128   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "14533  0.000485  0.000314  0.000242  0.000121  0.013504  0.007639  0.006524   \n",
      "14534  0.000273  0.000343  0.000315  0.000219  0.008601  0.009957  0.008566   \n",
      "14535  0.000713  0.000891  0.000797  0.000374  0.021867  0.025944  0.023035   \n",
      "14536  0.000239  0.000280  0.000255  0.000111  0.006091  0.007525  0.006988   \n",
      "14537  0.000215  0.000267  0.000295  0.000406  0.007283  0.008664  0.009329   \n",
      "14538  0.000108  0.000130  0.000093  0.000167  0.003554  0.003912  0.003047   \n",
      "14539  0.000269  0.000179  0.000137  0.000186  0.007980  0.004931  0.003405   \n",
      "14540  0.000104  0.000170  0.000093  0.000035  0.003824  0.005447  0.002991   \n",
      "14541  0.000246  0.000249  0.000250  0.000233  0.006693  0.007089  0.007101   \n",
      "14542  0.000492  0.000402  0.000329  0.000280  0.014309  0.011411  0.008965   \n",
      "14543  0.000498  0.000358  0.000211  0.000130  0.013967  0.010191  0.005922   \n",
      "14544  0.000185  0.000325  0.000435  0.000550  0.004623  0.009369  0.012718   \n",
      "14545  0.000144  0.000082  0.000183  0.000300  0.004455  0.002943  0.005507   \n",
      "14546  0.000249  0.000248  0.000220  0.000181  0.006939  0.006954  0.006236   \n",
      "14547  0.000144  0.000082  0.000183  0.000300  0.004455  0.002943  0.005507   \n",
      "14548  0.000055  0.000035  0.000199  0.000430  0.001518  0.001056  0.005427   \n",
      "14549  0.000309  0.000104  0.000105  0.000195  0.008831  0.003160  0.003209   \n",
      "14550  0.000256  0.000314  0.000377  0.000382  0.006420  0.008067  0.011603   \n",
      "14551  0.000311  0.000340  0.000324  0.000259  0.008822  0.009675  0.009051   \n",
      "14553  0.000185  0.000325  0.000435  0.000550  0.004623  0.009369  0.012718   \n",
      "14554  0.000713  0.000891  0.000797  0.000374  0.021867  0.025944  0.023035   \n",
      "14555  0.000713  0.000891  0.000797  0.000374  0.021867  0.025944  0.023035   \n",
      "14557  0.000185  0.000325  0.000435  0.000550  0.004623  0.009369  0.012718   \n",
      "14558  0.000196  0.000190  0.000278  0.000302  0.005330  0.005772  0.007886   \n",
      "14559  0.000045  0.000155  0.000241  0.000221  0.002137  0.005048  0.007171   \n",
      "14560  0.000108  0.000130  0.000093  0.000167  0.003554  0.003912  0.003047   \n",
      "14561  0.000150  0.000149  0.000108  0.000093  0.003805  0.004022  0.002639   \n",
      "14562  0.000340  0.000356  0.000330  0.000260  0.008653  0.009418  0.009513   \n",
      "14563  0.000128  0.000092  0.000107  0.000058  0.004132  0.003000  0.002934   \n",
      "14564  0.000352  0.000326  0.000293  0.000271  0.009601  0.009107  0.008186   \n",
      "\n",
      "            511  \n",
      "1      0.019630  \n",
      "3      0.006301  \n",
      "5      0.009734  \n",
      "7      0.013389  \n",
      "8      0.010223  \n",
      "9      0.009006  \n",
      "10     0.013199  \n",
      "11     0.005886  \n",
      "12     0.019272  \n",
      "13     0.006710  \n",
      "14     0.022335  \n",
      "15     0.006102  \n",
      "17     0.004612  \n",
      "18     0.005941  \n",
      "21     0.001415  \n",
      "23     0.002437  \n",
      "24     0.003273  \n",
      "25     0.002413  \n",
      "27     0.004724  \n",
      "28     0.006653  \n",
      "29     0.005990  \n",
      "31     0.010219  \n",
      "32     0.013468  \n",
      "33     0.021334  \n",
      "34     0.002445  \n",
      "35     0.013002  \n",
      "36     0.002630  \n",
      "37     0.026248  \n",
      "38     0.002178  \n",
      "39     0.004273  \n",
      "...         ...  \n",
      "14533  0.004145  \n",
      "14534  0.005426  \n",
      "14535  0.009718  \n",
      "14536  0.003355  \n",
      "14537  0.011800  \n",
      "14538  0.005045  \n",
      "14539  0.005217  \n",
      "14540  0.000897  \n",
      "14541  0.006179  \n",
      "14542  0.007252  \n",
      "14543  0.003317  \n",
      "14544  0.016172  \n",
      "14545  0.008767  \n",
      "14546  0.004737  \n",
      "14547  0.008767  \n",
      "14548  0.012139  \n",
      "14549  0.005590  \n",
      "14550  0.012520  \n",
      "14551  0.006705  \n",
      "14553  0.016172  \n",
      "14554  0.009718  \n",
      "14555  0.009718  \n",
      "14557  0.016172  \n",
      "14558  0.007716  \n",
      "14559  0.005794  \n",
      "14560  0.005045  \n",
      "14561  0.002332  \n",
      "14562  0.008276  \n",
      "14563  0.001393  \n",
      "14564  0.007373  \n",
      "\n",
      "[12385 rows x 128 columns] \n",
      " 1         1\n",
      "3         1\n",
      "5         1\n",
      "7         1\n",
      "8         1\n",
      "9         1\n",
      "10        1\n",
      "11        1\n",
      "12        1\n",
      "13        1\n",
      "14        1\n",
      "15        1\n",
      "17        1\n",
      "18        1\n",
      "21        1\n",
      "23        1\n",
      "24        1\n",
      "25        1\n",
      "27        1\n",
      "28        1\n",
      "29        1\n",
      "31        1\n",
      "32        1\n",
      "33        1\n",
      "34        1\n",
      "35        1\n",
      "36        1\n",
      "37        1\n",
      "38        1\n",
      "39        1\n",
      "         ..\n",
      "14533    10\n",
      "14534    10\n",
      "14535    10\n",
      "14536    10\n",
      "14537    10\n",
      "14538    10\n",
      "14539    10\n",
      "14540    10\n",
      "14541    10\n",
      "14542    10\n",
      "14543    10\n",
      "14544    10\n",
      "14545    10\n",
      "14546    10\n",
      "14547    10\n",
      "14548    10\n",
      "14549    10\n",
      "14550    10\n",
      "14551    10\n",
      "14553    10\n",
      "14554    10\n",
      "14555    10\n",
      "14557    10\n",
      "14558    10\n",
      "14559    10\n",
      "14560    10\n",
      "14561    10\n",
      "14562    10\n",
      "14563    10\n",
      "14564    10\n",
      "Name: 512, Length: 12385, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 对全NaN列行进行清除\n",
    "result = pd.concat([subfeatures, labels], axis=1,ignore_index=True)\n",
    "# print(result)\n",
    "result=result.dropna(how='all',axis=1)\n",
    "result=result.dropna(how='any',axis=0)\n",
    "result=result.reindex()\n",
    "# print(result)\n",
    "subfeatures=result.iloc[:,:-1]\n",
    "labels=result.iloc[:,-1]\n",
    "print(subfeatures,'\\n',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.50      0.50       132\n",
      "           2       0.82      0.83      0.83       130\n",
      "           3       0.67      0.70      0.69       103\n",
      "           4       0.47      0.46      0.47       141\n",
      "           5       0.98      0.99      0.99       142\n",
      "           6       0.99      0.99      0.99        67\n",
      "           7       0.87      0.87      0.87       150\n",
      "           8       0.94      0.96      0.95        78\n",
      "           9       0.82      0.81      0.81       156\n",
      "          10       0.81      0.80      0.81       140\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1239\n",
      "   macro avg       0.79      0.79      0.79      1239\n",
      "weighted avg       0.78      0.78      0.78      1239\n",
      "\n",
      "0.7764326069410815\n",
      "Confusion matrix, without normalization\n",
      "[[ 66   6  10  22   0   1   5   2   6  14]\n",
      " [  3 108   4   9   0   0   1   1   2   2]\n",
      " [  7   1  72  13   1   0   2   0   5   2]\n",
      " [ 25  10  13  65   1   0   7   0  14   6]\n",
      " [  0   0   0   1 141   0   0   0   0   0]\n",
      " [  0   0   0   1   0  66   0   0   0   0]\n",
      " [  6   3   1   6   0   0 131   1   1   1]\n",
      " [  2   0   1   0   0   0   0  75   0   0]\n",
      " [  9   2   5  10   1   0   2   0 126   1]\n",
      " [ 12   1   1  11   0   0   2   1   0 112]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.46      0.45       110\n",
      "           2       0.81      0.76      0.78       138\n",
      "           3       0.74      0.67      0.71       107\n",
      "           4       0.48      0.51      0.50       149\n",
      "           5       0.99      1.00      0.99       134\n",
      "           6       0.94      0.99      0.97        99\n",
      "           7       0.89      0.92      0.90       147\n",
      "           8       0.94      0.98      0.96        83\n",
      "           9       0.87      0.78      0.82       155\n",
      "          10       0.78      0.80      0.79       117\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1239\n",
      "   macro avg       0.79      0.79      0.79      1239\n",
      "weighted avg       0.78      0.78      0.78      1239\n",
      "\n",
      "0.7804681194511703\n",
      "Confusion matrix, without normalization\n",
      "[[ 51   7   8  26   1   3   3   1   3   7]\n",
      " [ 11 105   4  10   0   0   2   0   4   2]\n",
      " [ 10   3  72  15   0   0   1   1   2   3]\n",
      " [ 26   9   9  76   1   2   8   1   7  10]\n",
      " [  0   0   0   0 134   0   0   0   0   0]\n",
      " [  0   0   0   1   0  98   0   0   0   0]\n",
      " [  4   1   1   4   0   0 135   1   0   1]\n",
      " [  0   1   0   0   0   0   0  81   0   1]\n",
      " [ 10   3   2  14   0   0   2   1 121   2]\n",
      " [  5   1   1  12   0   1   1   0   2  94]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.40      0.42       113\n",
      "           2       0.83      0.84      0.84       131\n",
      "           3       0.77      0.69      0.73       122\n",
      "           4       0.46      0.55      0.50       137\n",
      "           5       0.95      1.00      0.97       141\n",
      "           6       0.98      0.99      0.98        83\n",
      "           7       0.84      0.87      0.86       140\n",
      "           8       0.95      0.99      0.97        75\n",
      "           9       0.85      0.74      0.79       152\n",
      "          10       0.80      0.79      0.79       145\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1239\n",
      "   macro avg       0.79      0.79      0.78      1239\n",
      "weighted avg       0.78      0.77      0.78      1239\n",
      "\n",
      "0.774818401937046\n",
      "Confusion matrix, without normalization\n",
      "[[ 45   7   7  29   1   0   8   1   5  10]\n",
      " [  7 110   1   8   0   0   1   0   1   3]\n",
      " [  8   2  84  19   1   2   1   0   4   1]\n",
      " [ 20   6   8  76   5   0   6   1   8   7]\n",
      " [  0   0   0   0 141   0   0   0   0   0]\n",
      " [  0   0   1   0   0  82   0   0   0   0]\n",
      " [  3   0   3   8   0   0 122   1   0   3]\n",
      " [  0   1   0   0   0   0   0  74   0   0]\n",
      " [ 12   1   4  13   1   0   4   1 112   4]\n",
      " [  8   5   1  13   0   0   3   0   1 114]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.49      0.49       115\n",
      "           2       0.84      0.81      0.82       127\n",
      "           3       0.71      0.67      0.69       119\n",
      "           4       0.51      0.53      0.52       147\n",
      "           5       0.97      0.99      0.98       146\n",
      "           6       0.93      1.00      0.96        93\n",
      "           7       0.91      0.90      0.91       149\n",
      "           8       0.97      0.96      0.97        75\n",
      "           9       0.80      0.78      0.79       138\n",
      "          10       0.78      0.78      0.78       130\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1239\n",
      "   macro avg       0.79      0.79      0.79      1239\n",
      "weighted avg       0.78      0.78      0.78      1239\n",
      "\n",
      "0.7828894269572235\n",
      "Confusion matrix, without normalization\n",
      "[[ 56   7  13  25   1   0   0   1   5   7]\n",
      " [  6 103   2  10   0   0   1   0   2   3]\n",
      " [ 10   2  80  15   1   3   1   0   4   3]\n",
      " [ 21   6   7  78   3   2   4   1  14  11]\n",
      " [  0   0   0   0 145   0   1   0   0   0]\n",
      " [  0   0   0   0   0  93   0   0   0   0]\n",
      " [  3   0   2   6   0   0 134   0   2   2]\n",
      " [  0   1   1   1   0   0   0  72   0   0]\n",
      " [ 10   2   3  10   0   1   3   0 107   2]\n",
      " [  8   2   5   9   0   1   3   0   0 102]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.56      0.58       136\n",
      "           2       0.84      0.82      0.83       129\n",
      "           3       0.72      0.72      0.72       115\n",
      "           4       0.54      0.55      0.54       154\n",
      "           5       0.93      1.00      0.96       134\n",
      "           6       0.97      0.99      0.98        84\n",
      "           7       0.86      0.81      0.83       134\n",
      "           8       0.95      0.95      0.95        74\n",
      "           9       0.84      0.81      0.83       135\n",
      "          10       0.81      0.86      0.83       144\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1239\n",
      "   macro avg       0.80      0.81      0.80      1239\n",
      "weighted avg       0.79      0.79      0.79      1239\n",
      "\n",
      "0.7893462469733656\n",
      "Confusion matrix, without normalization\n",
      "[[ 76   7  12  18   2   0   5   1   7   8]\n",
      " [  2 106   2  12   1   0   1   0   1   4]\n",
      " [ 12   2  83  11   0   1   2   0   2   2]\n",
      " [ 20   5  12  84   6   2   7   1   9   8]\n",
      " [  0   0   0   0 134   0   0   0   0   0]\n",
      " [  1   0   0   0   0  83   0   0   0   0]\n",
      " [  2   2   2  11   0   0 108   1   1   7]\n",
      " [  0   1   0   2   0   0   0  70   0   1]\n",
      " [  8   1   3  10   1   0   1   1 110   0]\n",
      " [  4   2   2   9   0   0   2   0   1 124]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.48      0.49       137\n",
      "           2       0.78      0.80      0.79       125\n",
      "           3       0.70      0.66      0.68       111\n",
      "           4       0.47      0.47      0.47       144\n",
      "           5       0.94      1.00      0.97       127\n",
      "           6       0.98      0.99      0.98        87\n",
      "           7       0.82      0.91      0.87       145\n",
      "           8       0.93      0.84      0.88        77\n",
      "           9       0.85      0.81      0.83       132\n",
      "          10       0.83      0.81      0.82       154\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1239\n",
      "   macro avg       0.78      0.78      0.78      1239\n",
      "weighted avg       0.76      0.76      0.76      1239\n",
      "\n",
      "0.7643260694108152\n",
      "Confusion matrix, without normalization\n",
      "[[ 66   7  11  30   2   0   6   0   7   8]\n",
      " [  7 100   1  11   1   0   1   0   1   3]\n",
      " [ 11   4  73  11   1   0   5   0   2   4]\n",
      " [ 29   8  10  67   4   1   9   1   7   8]\n",
      " [  0   0   0   0 127   0   0   0   0   0]\n",
      " [  1   0   0   0   0  86   0   0   0   0]\n",
      " [  4   0   1   5   0   0 132   1   2   0]\n",
      " [  0   8   1   2   0   0   0  65   0   1]\n",
      " [  6   1   5   9   0   0   2   1 107   1]\n",
      " [ 10   0   3   9   0   1   5   2   0 124]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.49      0.52       139\n",
      "           2       0.80      0.84      0.82       117\n",
      "           3       0.73      0.71      0.72        96\n",
      "           4       0.52      0.53      0.52       149\n",
      "           5       0.95      0.99      0.97       128\n",
      "           6       0.95      1.00      0.98        84\n",
      "           7       0.86      0.89      0.88       159\n",
      "           8       0.94      0.96      0.95        81\n",
      "           9       0.82      0.76      0.79       143\n",
      "          10       0.78      0.81      0.79       143\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1239\n",
      "   macro avg       0.79      0.80      0.79      1239\n",
      "weighted avg       0.78      0.78      0.78      1239\n",
      "\n",
      "0.7812752219531881\n",
      "Confusion matrix, without normalization\n",
      "[[ 68   8   6  29   1   1   7   3   8   8]\n",
      " [  5  98   1   6   1   0   1   0   2   3]\n",
      " [  6   1  68  12   0   0   3   2   3   1]\n",
      " [ 22   9   9  79   4   1   5   0   6  14]\n",
      " [  0   0   0   1 127   0   0   0   0   0]\n",
      " [  0   0   0   0   0  84   0   0   0   0]\n",
      " [  7   0   2   4   0   0 142   0   2   2]\n",
      " [  0   0   0   0   0   1   0  78   0   2]\n",
      " [  6   4   5  13   1   0   3   0 108   3]\n",
      " [  7   2   2   9   0   1   4   0   2 116]]\n",
      "train...\n",
      "test...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.49      0.52       118\n",
      "           2       0.79      0.91      0.85       118\n",
      "           3       0.73      0.76      0.75       100\n",
      "           4       0.56      0.54      0.55       158\n",
      "           5       0.98      1.00      0.99       142\n",
      "           6       0.99      0.98      0.98        87\n",
      "           7       0.86      0.75      0.80       142\n",
      "           8       0.89      0.95      0.92        83\n",
      "           9       0.76      0.80      0.78       157\n",
      "          10       0.81      0.82      0.81       134\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1239\n",
      "   macro avg       0.79      0.80      0.80      1239\n",
      "weighted avg       0.78      0.79      0.78      1239\n",
      "\n",
      "0.7861178369652946\n",
      "Confusion matrix, without normalization\n",
      "[[ 58   4   6  24   0   0   7   2   7  10]\n",
      " [  0 107   2   6   0   0   0   1   0   2]\n",
      " [  6   1  76  12   0   0   0   1   3   1]\n",
      " [ 18  11  12  85   2   0   5   4  12   9]\n",
      " [  0   0   0   0 142   0   0   0   0   0]\n",
      " [  1   0   0   1   0  85   0   0   0   0]\n",
      " [  4   0   3   8   0   1 106   1  17   2]\n",
      " [  1   1   1   0   0   0   0  79   0   1]\n",
      " [  6   7   3  11   1   0   2   0 126   1]\n",
      " [  9   4   1   5   0   0   3   1   1 110]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.39      0.44       115\n",
      "           2       0.83      0.84      0.84       147\n",
      "           3       0.76      0.69      0.72       106\n",
      "           4       0.44      0.52      0.48       131\n",
      "           5       0.98      1.00      0.99       128\n",
      "           6       0.95      0.96      0.96       108\n",
      "           7       0.90      0.91      0.90       176\n",
      "           8       0.94      0.97      0.95        75\n",
      "           9       0.81      0.77      0.79       124\n",
      "          10       0.79      0.84      0.81       129\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1239\n",
      "   macro avg       0.79      0.79      0.79      1239\n",
      "weighted avg       0.79      0.79      0.79      1239\n",
      "\n",
      "0.7901533494753834\n",
      "Confusion matrix, without normalization\n",
      "[[ 45   9   5  39   0   2   3   2   5   5]\n",
      " [  5 124   2  10   0   0   2   0   1   3]\n",
      " [ 10   4  73  10   1   0   3   0   0   5]\n",
      " [ 13   7   9  68   2   2   6   1  15   8]\n",
      " [  0   0   0   0 128   0   0   0   0   0]\n",
      " [  1   0   2   0   0 104   0   0   0   1]\n",
      " [  1   2   2   6   0   0 160   1   1   3]\n",
      " [  1   0   1   0   0   0   0  73   0   0]\n",
      " [  6   2   1  12   0   0   2   1  96   4]\n",
      " [  7   2   1   8   0   1   2   0   0 108]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.47      0.51       159\n",
      "           2       0.84      0.87      0.85       135\n",
      "           3       0.73      0.67      0.70       110\n",
      "           4       0.47      0.63      0.54       129\n",
      "           5       0.98      1.00      0.99       119\n",
      "           6       0.98      1.00      0.99        90\n",
      "           7       0.89      0.86      0.87       144\n",
      "           8       0.88      0.87      0.88        78\n",
      "           9       0.82      0.76      0.79       144\n",
      "          10       0.77      0.79      0.78       131\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1239\n",
      "   macro avg       0.79      0.79      0.79      1239\n",
      "weighted avg       0.78      0.77      0.78      1239\n",
      "\n",
      "0.774818401937046\n",
      "Confusion matrix, without normalization\n",
      "[[ 74   7   8  37   0   0   7   6   7  13]\n",
      " [  9 117   1   4   1   0   1   0   1   1]\n",
      " [  8   3  74  13   0   0   1   0   5   6]\n",
      " [ 21   2   8  81   0   1   0   1   7   8]\n",
      " [  0   0   0   0 119   0   0   0   0   0]\n",
      " [  0   0   0   0   0  90   0   0   0   0]\n",
      " [  5   2   2   8   0   0 124   0   2   1]\n",
      " [  0   7   1   0   0   0   0  68   0   2]\n",
      " [  6   1   4  17   1   1   3   2 109   0]\n",
      " [  6   1   3  11   0   0   4   0   2 104]]\n"
     ]
    }
   ],
   "source": [
    "# 平均准确率归零\n",
    "avgscore = 0\n",
    "recallscore = 0\n",
    "precisionscore = 0\n",
    "N = 10\n",
    "\n",
    "# 进行十次KNN测试\n",
    "for i in range(N):\n",
    "    # 以10%的比例进行交叉验证\n",
    "    #X_train, X_test, y_train, y_test = cross_validation.train_test_split(subfeatures, features_labels, test_size=0.1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subfeatures, labels, test_size=0.1)\n",
    "\n",
    "    # 进行训练\n",
    "    print('train...')\n",
    "    # 进行KNN训练,距离为1\n",
    "    neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "    neigh.fit(X_train, y_train)\n",
    "\n",
    "    # 预试\n",
    "    print('test...')\n",
    "    c_test = neigh.predict(X_test)\n",
    "\n",
    "    # print(y_test)\n",
    "    # print(c_test)\n",
    "\n",
    "    # 计算预测划分准确率\n",
    "    print('accurary...')\n",
    "    score = neigh.score(X_test, y_test)\n",
    "    print(classification_report(y_test,c_test))\n",
    "    avgscore = avgscore + score\n",
    "    recallscore = recallscore + recall_score(y_test,c_test,average=\"macro\")\n",
    "    precisionscore = precisionscore + precision_score(y_test,c_test,average=\"macro\")\n",
    "    print(score)\n",
    "\n",
    "    # 通过混淆矩阵进行结果标示\n",
    "    cm = confusion_matrix(y_test, c_test)\n",
    "    np.set_printoptions(threshold=10000)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(str(cm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgscore....\n",
      "0.7800645682001615\n",
      "False positive rate\n",
      "0.20943242023385056\n",
      "false negative rate\n",
      "0.2083126031032564\n"
     ]
    }
   ],
   "source": [
    "# 输出N次的平均准确率\n",
    "avgscore = avgscore / N\n",
    "recallscore = recallscore / N\n",
    "precisionscore = precisionscore / N\n",
    "\n",
    "print('avgscore....')\n",
    "print(avgscore)\n",
    "print('False positive rate')\n",
    "print(1-precisionscore)\n",
    "print('false negative rate')\n",
    "print(1-recallscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
