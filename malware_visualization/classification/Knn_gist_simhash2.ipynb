{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Summary of the file.\\n\\n    功能：\\n        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以KNN方法划分并获得准确率，通过混淆矩阵标示结果。\\n\\n    输出：\\n        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "''' Summary of the file.\n",
    "\n",
    "    功能：\n",
    "        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以KNN方法划分并获得准确率，通过混淆矩阵标示结果。\n",
    "\n",
    "    输出：\n",
    "        结果的混淆矩阵，每次划分的准确率，十次的平均准确率\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实验室的15000条恶意样本，分为十个家族\n",
    "\n",
    "# # GIST特征值\n",
    "subfeatures = pd.read_csv(r'F:\\virtus_test\\Simhash_Gistv2\\gist_f_train_full.csv',header=None)\n",
    "labels = pd.read_csv(r'F:\\virtus_test\\Simhash_Gistv2\\CNO_full.txt',header=None)\n",
    "\n",
    "# # text段代码提取二进制代码，生成图像的LBP特征\n",
    "# subfeatures=pd.read_csv('/home/stack/Data/Output/lbp_only_text/feature_train_full.csv',header=None)\n",
    "# labels = pd.read_csv('/home/stack/Data/Output/lbp_only_text/CNO_full.txt',header=None)\n",
    "\n",
    "# text段 GIST特征\n",
    "# #subfeatures=pd.read_csv(r'E:\\test\\process_gist\\gist_f_train_full.csv',header=None)\n",
    "# #labels = pd.read_csv(r'E:\\test\\process_gist\\CNO_full.txt',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6    \\\n",
      "1      0.000025  0.000042  0.001947  0.000118  0.000027  0.000046  0.009525   \n",
      "3      0.021550  0.007681  0.031629  0.015364  0.017454  0.000460  0.030582   \n",
      "5      0.001903  0.000339  0.007303  0.000272  0.012126  0.000163  0.012807   \n",
      "8      0.002287  0.000333  0.014099  0.000369  0.011018  0.000194  0.011269   \n",
      "9      0.027110  0.006912  0.045522  0.023629  0.020981  0.008361  0.039535   \n",
      "10     0.037260  0.017591  0.055027  0.036390  0.027287  0.008496  0.043107   \n",
      "12     0.031269  0.007804  0.040050  0.018106  0.019535  0.000554  0.027900   \n",
      "14     0.001719  0.000278  0.007214  0.000267  0.005511  0.000162  0.012874   \n",
      "17     0.000027  0.000042  0.002761  0.000117  0.000028  0.000045  0.011897   \n",
      "21     0.012974  0.000319  0.007631  0.000222  0.013669  0.000194  0.020849   \n",
      "23     0.007849  0.000221  0.007033  0.007127  0.012704  0.000163  0.012380   \n",
      "24     0.012538  0.000630  0.016398  0.000351  0.018696  0.000278  0.011041   \n",
      "27     0.000063  0.000305  0.010203  0.000303  0.000061  0.000076  0.013000   \n",
      "28     0.000018  0.000032  0.001556  0.000090  0.000019  0.000036  0.006903   \n",
      "35     0.002254  0.007494  0.001996  0.000189  0.007194  0.000296  0.016105   \n",
      "36     0.001395  0.000114  0.000044  0.000046  0.010345  0.000154  0.000048   \n",
      "42     0.011227  0.000585  0.020101  0.009390  0.012406  0.000225  0.019210   \n",
      "43     0.012411  0.001144  0.021204  0.008284  0.016090  0.000305  0.035453   \n",
      "44     0.000869  0.000478  0.022434  0.008254  0.005531  0.000260  0.025060   \n",
      "48     0.031269  0.007804  0.040050  0.018106  0.019535  0.000554  0.027900   \n",
      "49     0.015528  0.000826  0.026785  0.012662  0.014631  0.000416  0.025233   \n",
      "50     0.001644  0.000156  0.002333  0.000138  0.015266  0.000227  0.017226   \n",
      "53     0.000099  0.000163  0.014600  0.000337  0.000066  0.000074  0.011649   \n",
      "55     0.022328  0.000721  0.027217  0.017388  0.012319  0.000380  0.024291   \n",
      "56     0.014728  0.000825  0.029814  0.019905  0.013842  0.000438  0.032417   \n",
      "57     0.000175  0.000093  0.008101  0.000196  0.014255  0.000212  0.000084   \n",
      "58     0.002058  0.000337  0.007349  0.025320  0.013468  0.000153  0.008734   \n",
      "63     0.013294  0.000647  0.020840  0.000471  0.016954  0.000271  0.018794   \n",
      "65     0.024476  0.007746  0.040585  0.019295  0.016583  0.001239  0.036150   \n",
      "71     0.022328  0.000721  0.027217  0.017388  0.012319  0.000380  0.024291   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "14711  0.000037  0.000068  0.003353  0.000117  0.000041  0.000073  0.017959   \n",
      "14713  0.000009  0.000020  0.001701  0.000060  0.000010  0.000022  0.006757   \n",
      "14714  0.000183  0.000086  0.010732  0.000187  0.007411  0.000159  0.016877   \n",
      "14715  0.006106  0.000158  0.001996  0.000238  0.000079  0.000088  0.009534   \n",
      "14716  0.000128  0.000068  0.002084  0.000103  0.010623  0.000178  0.014688   \n",
      "14717  0.007896  0.000158  0.001336  0.000156  0.000056  0.000060  0.008530   \n",
      "14718  0.000037  0.000068  0.003353  0.000117  0.000041  0.000073  0.017959   \n",
      "14719  0.000401  0.000100  0.010926  0.000198  0.007244  0.000196  0.017035   \n",
      "14720  0.000910  0.000107  0.003446  0.000103  0.006643  0.000129  0.015950   \n",
      "14721  0.000031  0.000069  0.010874  0.000187  0.000032  0.000060  0.016929   \n",
      "14722  0.000028  0.000057  0.002505  0.000110  0.000031  0.000067  0.011512   \n",
      "14723  0.000023  0.000048  0.010733  0.000164  0.000023  0.000037  0.011757   \n",
      "14724  0.000029  0.000066  0.011233  0.000183  0.000028  0.000052  0.015346   \n",
      "14725  0.000104  0.000068  0.009939  0.000166  0.008617  0.000146  0.012276   \n",
      "14727  0.000083  0.000060  0.002746  0.005884  0.000042  0.000066  0.018063   \n",
      "14728  0.000104  0.000068  0.009939  0.000166  0.008617  0.000146  0.012276   \n",
      "14729  0.000009  0.000020  0.001701  0.000060  0.000010  0.000022  0.006757   \n",
      "14731  0.000023  0.000049  0.010228  0.000164  0.000023  0.000038  0.013370   \n",
      "14732  0.000022  0.000046  0.002913  0.000090  0.000024  0.000046  0.014044   \n",
      "14734  0.000029  0.000066  0.011233  0.000183  0.000028  0.000052  0.015346   \n",
      "14735  0.006106  0.000158  0.001996  0.000238  0.000079  0.000088  0.009534   \n",
      "14736  0.006106  0.000158  0.001996  0.000238  0.000079  0.000088  0.009534   \n",
      "14738  0.000029  0.000066  0.011233  0.000183  0.000028  0.000052  0.015346   \n",
      "14739  0.000026  0.000052  0.002969  0.000093  0.000027  0.000053  0.014961   \n",
      "14740  0.000226  0.008405  0.003208  0.000116  0.000033  0.000069  0.016649   \n",
      "14741  0.000037  0.000068  0.003353  0.000117  0.000041  0.000073  0.017959   \n",
      "14742  0.000024  0.000041  0.001863  0.000112  0.000025  0.000045  0.009533   \n",
      "14743  0.000031  0.000398  0.001179  0.000112  0.000053  0.011253  0.007471   \n",
      "14744  0.000026  0.000053  0.003159  0.000095  0.000027  0.000053  0.016767   \n",
      "14745  0.000025  0.000053  0.009643  0.000165  0.000026  0.000046  0.014766   \n",
      "\n",
      "            7         8         9    ...       502       503       504  \\\n",
      "1      0.000160  0.000012  0.000009  ...  0.016020  0.000938  0.000007   \n",
      "3      0.005503  0.000508  0.000217  ...  0.025171  0.007387  0.000095   \n",
      "5      0.008677  0.000291  0.000037  ...  0.018321  0.013427  0.000050   \n",
      "8      0.007954  0.000272  0.000065  ...  0.020796  0.012178  0.000050   \n",
      "9      0.001357  0.007722  0.000400  ...  0.033002  0.004517  0.010100   \n",
      "10     0.013960  0.007929  0.000384  ...  0.036264  0.025158  0.009973   \n",
      "12     0.009331  0.008953  0.000270  ...  0.025152  0.014326  0.014281   \n",
      "14     0.007886  0.000191  0.000041  ...  0.016235  0.011912  0.000012   \n",
      "17     0.000162  0.000012  0.000010  ...  0.014783  0.000959  0.000008   \n",
      "21     0.000189  0.000292  0.000064  ...  0.020656  0.000676  0.000078   \n",
      "23     0.000157  0.000330  0.000049  ...  0.017546  0.000738  0.000058   \n",
      "24     0.000298  0.000489  0.000111  ...  0.020885  0.001118  0.000094   \n",
      "27     0.009455  0.000031  0.000027  ...  0.018099  0.014452  0.000011   \n",
      "28     0.000128  0.000008  0.000007  ...  0.013371  0.000755  0.000005   \n",
      "35     0.008505  0.000222  0.000088  ...  0.022849  0.011445  0.000015   \n",
      "36     0.000053  0.000126  0.000014  ...  0.000020  0.000078  0.000023   \n",
      "42     0.012841  0.000243  0.000120  ...  0.029459  0.015531  0.000072   \n",
      "43     0.007976  0.000371  0.000105  ...  0.030813  0.012423  0.000078   \n",
      "44     0.000244  0.000098  0.000038  ...  0.023137  0.000801  0.000019   \n",
      "48     0.009331  0.008953  0.000270  ...  0.025152  0.014326  0.014281   \n",
      "49     0.008019  0.000577  0.000164  ...  0.026013  0.012358  0.000108   \n",
      "50     0.000220  0.000204  0.000061  ...  0.027403  0.001258  0.000069   \n",
      "53     0.000176  0.000049  0.000046  ...  0.014465  0.000937  0.000017   \n",
      "55     0.008015  0.000420  0.000116  ...  0.025463  0.012252  0.000081   \n",
      "56     0.006539  0.000476  0.000141  ...  0.028232  0.014512  0.000102   \n",
      "57     0.000070  0.000088  0.000028  ...  0.000034  0.000072  0.000072   \n",
      "58     0.009537  0.000097  0.000034  ...  0.012322  0.015409  0.000052   \n",
      "63     0.007988  0.006846  0.000153  ...  0.025243  0.012183  0.009656   \n",
      "65     0.012703  0.000640  0.000250  ...  0.030975  0.017674  0.000151   \n",
      "71     0.008015  0.000420  0.000116  ...  0.025463  0.012252  0.000081   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "14711  0.000178  0.000021  0.000018  ...  0.015697  0.000870  0.000009   \n",
      "14713  0.000086  0.000005  0.000004  ...  0.008882  0.000516  0.000002   \n",
      "14714  0.000149  0.000106  0.000022  ...  0.022943  0.000805  0.000032   \n",
      "14715  0.000187  0.000060  0.000056  ...  0.016032  0.000941  0.000063   \n",
      "14716  0.000162  0.000071  0.000024  ...  0.021712  0.000961  0.000044   \n",
      "14717  0.000157  0.000042  0.000035  ...  0.014018  0.000885  0.000028   \n",
      "14718  0.000178  0.000021  0.000018  ...  0.015697  0.000870  0.000009   \n",
      "14719  0.000177  0.000244  0.000028  ...  0.015590  0.000766  0.000044   \n",
      "14720  0.000137  0.000091  0.000018  ...  0.021765  0.000840  0.000009   \n",
      "14721  0.000140  0.000017  0.000014  ...  0.017495  0.000715  0.000008   \n",
      "14722  0.000167  0.000014  0.000011  ...  0.013478  0.000859  0.000008   \n",
      "14723  0.000114  0.000012  0.000011  ...  0.017953  0.000748  0.000006   \n",
      "14724  0.000125  0.000015  0.000012  ...  0.020876  0.000710  0.000007   \n",
      "14725  0.000129  0.000063  0.000019  ...  0.018158  0.000748  0.000033   \n",
      "14727  0.000148  0.000022  0.000021  ...  0.018121  0.000757  0.000026   \n",
      "14728  0.000129  0.000063  0.000019  ...  0.018158  0.000748  0.000033   \n",
      "14729  0.000086  0.000005  0.000004  ...  0.008882  0.000516  0.000002   \n",
      "14731  0.000116  0.000012  0.000011  ...  0.017338  0.000742  0.000006   \n",
      "14732  0.000126  0.000013  0.000011  ...  0.018982  0.000760  0.000004   \n",
      "14734  0.000125  0.000015  0.000012  ...  0.020876  0.000710  0.000007   \n",
      "14735  0.000187  0.000060  0.000056  ...  0.016032  0.000941  0.000063   \n",
      "14736  0.000187  0.000060  0.000056  ...  0.016032  0.000941  0.000063   \n",
      "14738  0.000125  0.000015  0.000012  ...  0.020876  0.000710  0.000007   \n",
      "14739  0.000122  0.000014  0.000012  ...  0.021242  0.000713  0.000006   \n",
      "14740  0.000147  0.000018  0.000020  ...  0.018883  0.000775  0.000006   \n",
      "14741  0.000178  0.000021  0.000018  ...  0.015697  0.000870  0.000009   \n",
      "14742  0.000158  0.000011  0.000009  ...  0.015592  0.000927  0.000007   \n",
      "14743  0.000163  0.000025  0.000448  ...  0.016866  0.000939  0.000043   \n",
      "14744  0.000127  0.000014  0.000012  ...  0.020839  0.000714  0.000005   \n",
      "14745  0.000126  0.000014  0.000012  ...  0.013032  0.000672  0.000006   \n",
      "\n",
      "            505       506       507       508       509           510  \\\n",
      "1      0.000022  0.000023  0.000004  0.000004  0.000005  2.366100e-06   \n",
      "3      0.000306  0.015750  0.000713  0.001214  0.000688  1.730200e-03   \n",
      "5      0.000032  0.000081  0.000091  0.000038  0.000343  8.296800e-04   \n",
      "8      0.000242  0.013195  0.000653  0.000051  0.000371  1.036900e-03   \n",
      "9      0.000633  0.034223  0.015565  0.000984  0.000800  1.314600e-02   \n",
      "10     0.000743  0.023772  0.013804  0.001233  0.001019  1.522500e-02   \n",
      "12     0.000912  0.012303  0.004319  0.008147  0.000890  2.068500e-03   \n",
      "14     0.000085  0.005372  0.000351  0.000027  0.000355  9.422500e-03   \n",
      "17     0.000023  0.000024  0.000004  0.000005  0.000005  2.484600e-06   \n",
      "21     0.000048  0.000057  0.000050  0.000744  0.000299  8.195300e-04   \n",
      "23     0.000039  0.000057  0.000044  0.000635  0.000242  6.810400e-04   \n",
      "24     0.000270  0.011748  0.000381  0.001213  0.000706  1.151600e-03   \n",
      "27     0.000034  0.000052  0.000093  0.000042  0.000369  8.936000e-04   \n",
      "28     0.000017  0.000018  0.000003  0.000003  0.000003  1.726600e-06   \n",
      "35     0.000023  0.000124  0.000084  0.000007  0.000017  1.257900e-05   \n",
      "36     0.000005  0.000009  0.000020  0.000004  0.000007  7.661700e-06   \n",
      "42     0.000252  0.014079  0.000725  0.000939  0.000563  1.458200e-03   \n",
      "43     0.000100  0.008225  0.000522  0.000852  0.000664  1.238800e-03   \n",
      "44     0.000043  0.000078  0.000058  0.000067  0.000316  9.918200e-04   \n",
      "48     0.000912  0.012303  0.004319  0.008147  0.000890  2.068500e-03   \n",
      "49     0.000336  0.016882  0.000816  0.001287  0.000771  1.907900e-03   \n",
      "50     0.000258  0.012430  0.000341  0.000008  0.000024  2.074700e-05   \n",
      "53     0.000027  0.000043  0.000085  0.000054  0.000230  1.672800e-03   \n",
      "55     0.000261  0.012988  0.000716  0.000895  0.000606  1.768500e-03   \n",
      "56     0.000225  0.013904  0.000847  0.001223  0.000793  1.043100e-02   \n",
      "57     0.000013  0.000017  0.000060  0.000025  0.000097  8.880200e-04   \n",
      "58     0.000019  0.000045  0.000093  0.000034  0.000361  8.564700e-04   \n",
      "63     0.000475  0.009609  0.000278  0.001228  0.000713  1.481000e-03   \n",
      "65     0.000393  0.021465  0.000950  0.001300  0.000936  1.317900e-02   \n",
      "71     0.000261  0.012988  0.000716  0.000895  0.000606  1.768500e-03   \n",
      "...         ...       ...       ...       ...       ...           ...   \n",
      "14711  0.000044  0.000077  0.000011  0.000007  0.000009  4.544000e-06   \n",
      "14713  0.000010  0.000011  0.000002  0.000001  0.000002  9.790400e-07   \n",
      "14714  0.000034  0.000061  0.000012  0.000007  0.000009  4.152000e-05   \n",
      "14715  0.000068  0.000046  0.000016  0.001073  0.000192  6.476400e-05   \n",
      "14716  0.000033  0.000066  0.000016  0.000005  0.000008  5.757000e-06   \n",
      "14717  0.000055  0.000047  0.000013  0.000835  0.000356  5.474100e-05   \n",
      "14718  0.000044  0.000077  0.000011  0.000007  0.000009  4.544000e-06   \n",
      "14719  0.000040  0.000071  0.000015  0.000009  0.000011  4.206200e-05   \n",
      "14720  0.000032  0.000057  0.000010  0.000005  0.000009  5.233100e-06   \n",
      "14721  0.000035  0.000062  0.000010  0.000007  0.000009  4.460400e-05   \n",
      "14722  0.000030  0.000031  0.000005  0.000005  0.000006  2.908900e-06   \n",
      "14723  0.000026  0.000053  0.000008  0.000006  0.000007  4.435800e-05   \n",
      "14724  0.000031  0.000054  0.000009  0.000006  0.000008  4.447900e-05   \n",
      "14725  0.000029  0.000058  0.000012  0.000006  0.000008  4.535000e-05   \n",
      "14727  0.000033  0.000062  0.000014  0.000115  0.000039  9.013600e-05   \n",
      "14728  0.000029  0.000058  0.000012  0.000006  0.000008  4.535000e-05   \n",
      "14729  0.000010  0.000011  0.000002  0.000001  0.000002  9.790400e-07   \n",
      "14731  0.000026  0.000053  0.000008  0.000006  0.000007  4.417900e-05   \n",
      "14732  0.000028  0.000053  0.000007  0.000004  0.000005  2.916700e-06   \n",
      "14734  0.000031  0.000054  0.000009  0.000006  0.000008  4.447900e-05   \n",
      "14735  0.000068  0.000046  0.000016  0.001073  0.000192  6.476400e-05   \n",
      "14736  0.000068  0.000046  0.000016  0.001073  0.000192  6.476400e-05   \n",
      "14738  0.000031  0.000054  0.000009  0.000006  0.000008  4.447900e-05   \n",
      "14739  0.000030  0.000054  0.000008  0.000004  0.000006  3.141200e-06   \n",
      "14740  0.000037  0.000067  0.000009  0.000024  0.000067  1.800500e-05   \n",
      "14741  0.000044  0.000077  0.000011  0.000007  0.000009  4.544000e-06   \n",
      "14742  0.000021  0.000023  0.000004  0.000004  0.000004  2.283300e-06   \n",
      "14743  0.000253  0.000042  0.000008  0.000009  0.000007  3.090100e-06   \n",
      "14744  0.000031  0.000055  0.000008  0.000004  0.000006  3.167700e-06   \n",
      "14745  0.000031  0.000058  0.000009  0.000006  0.000007  4.376700e-05   \n",
      "\n",
      "            511  \n",
      "1      0.000003  \n",
      "3      0.000685  \n",
      "5      0.000136  \n",
      "8      0.000414  \n",
      "9      0.001094  \n",
      "10     0.001730  \n",
      "12     0.000935  \n",
      "14     0.000261  \n",
      "17     0.000004  \n",
      "21     0.000419  \n",
      "23     0.000273  \n",
      "24     0.000468  \n",
      "27     0.000153  \n",
      "28     0.000002  \n",
      "35     0.000007  \n",
      "36     0.000007  \n",
      "42     0.000701  \n",
      "43     0.000532  \n",
      "44     0.000366  \n",
      "48     0.000935  \n",
      "49     0.000776  \n",
      "50     0.000013  \n",
      "53     0.000782  \n",
      "55     0.000636  \n",
      "56     0.000722  \n",
      "57     0.000466  \n",
      "58     0.000368  \n",
      "63     0.000690  \n",
      "65     0.001148  \n",
      "71     0.000636  \n",
      "...         ...  \n",
      "14711  0.000005  \n",
      "14713  0.000001  \n",
      "14714  0.000033  \n",
      "14715  0.000467  \n",
      "14716  0.000007  \n",
      "14717  0.000056  \n",
      "14718  0.000005  \n",
      "14719  0.000033  \n",
      "14720  0.000005  \n",
      "14721  0.000035  \n",
      "14722  0.000004  \n",
      "14723  0.000036  \n",
      "14724  0.000035  \n",
      "14725  0.000035  \n",
      "14727  0.000659  \n",
      "14728  0.000035  \n",
      "14729  0.000001  \n",
      "14731  0.000035  \n",
      "14732  0.000003  \n",
      "14734  0.000035  \n",
      "14735  0.000467  \n",
      "14736  0.000467  \n",
      "14738  0.000035  \n",
      "14739  0.000003  \n",
      "14740  0.000008  \n",
      "14741  0.000005  \n",
      "14742  0.000003  \n",
      "14743  0.000004  \n",
      "14744  0.000003  \n",
      "14745  0.000036  \n",
      "\n",
      "[11974 rows x 512 columns] \n",
      " 1         1\n",
      "3         1\n",
      "5         1\n",
      "8         1\n",
      "9         1\n",
      "10        1\n",
      "12        1\n",
      "14        1\n",
      "17        1\n",
      "21        1\n",
      "23        1\n",
      "24        1\n",
      "27        1\n",
      "28        1\n",
      "35        1\n",
      "36        1\n",
      "42        1\n",
      "43        1\n",
      "44        1\n",
      "48        1\n",
      "49        1\n",
      "50        1\n",
      "53        1\n",
      "55        1\n",
      "56        1\n",
      "57        1\n",
      "58        1\n",
      "63        1\n",
      "65        1\n",
      "71        1\n",
      "         ..\n",
      "14711    10\n",
      "14713    10\n",
      "14714    10\n",
      "14715    10\n",
      "14716    10\n",
      "14717    10\n",
      "14718    10\n",
      "14719    10\n",
      "14720    10\n",
      "14721    10\n",
      "14722    10\n",
      "14723    10\n",
      "14724    10\n",
      "14725    10\n",
      "14727    10\n",
      "14728    10\n",
      "14729    10\n",
      "14731    10\n",
      "14732    10\n",
      "14734    10\n",
      "14735    10\n",
      "14736    10\n",
      "14738    10\n",
      "14739    10\n",
      "14740    10\n",
      "14741    10\n",
      "14742    10\n",
      "14743    10\n",
      "14744    10\n",
      "14745    10\n",
      "Name: 512, Length: 11974, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 对全NaN列行进行清除\n",
    "result = pd.concat([subfeatures, labels], axis=1,ignore_index=True)\n",
    "# print(result)\n",
    "result=result.dropna(how='all',axis=1)\n",
    "result=result.dropna(how='any',axis=0)\n",
    "result=result.reindex()\n",
    "# print(result)\n",
    "subfeatures=result.iloc[:,:-1]\n",
    "labels=result.iloc[:,-1]\n",
    "print(subfeatures,'\\n',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.70      0.67        56\n",
      "           2       0.93      0.92      0.93       128\n",
      "           3       0.91      0.82      0.86       100\n",
      "           4       1.00      1.00      1.00       136\n",
      "           5       0.99      0.90      0.94       148\n",
      "           6       0.91      0.99      0.95       171\n",
      "           7       0.93      0.95      0.94       167\n",
      "           8       0.96      1.00      0.98        48\n",
      "           9       0.93      0.87      0.90       119\n",
      "          10       0.92      0.98      0.95       125\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1198\n",
      "   macro avg       0.91      0.91      0.91      1198\n",
      "weighted avg       0.93      0.93      0.93      1198\n",
      "\n",
      "0.9273789649415692\n",
      "Confusion matrix, without normalization\n",
      "[[ 39   5   4   0   0   0   1   1   2   4]\n",
      " [  7 118   1   0   0   0   1   1   0   0]\n",
      " [  7   3  82   0   0   2   1   0   1   4]\n",
      " [  0   0   0 136   0   0   0   0   0   0]\n",
      " [  0   0   0   0 133  14   0   0   0   1]\n",
      " [  0   0   0   0   1 170   0   0   0   0]\n",
      " [  1   1   2   0   0   0 158   0   5   0]\n",
      " [  0   0   0   0   0   0   0  48   0   0]\n",
      " [  4   0   1   0   0   0   9   0 104   1]\n",
      " [  2   0   0   0   0   0   0   0   0 123]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.58      0.62        84\n",
      "           2       0.92      0.92      0.92       119\n",
      "           3       0.82      0.88      0.85        94\n",
      "           4       0.99      1.00      0.99       145\n",
      "           5       0.99      0.90      0.94       140\n",
      "           6       0.91      0.99      0.95       160\n",
      "           7       0.95      0.98      0.96       131\n",
      "           8       0.93      1.00      0.96        62\n",
      "           9       0.92      0.92      0.92       126\n",
      "          10       0.91      0.85      0.88       137\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1198\n",
      "   macro avg       0.90      0.90      0.90      1198\n",
      "weighted avg       0.91      0.91      0.91      1198\n",
      "\n",
      "0.9140233722871453\n",
      "Confusion matrix, without normalization\n",
      "[[ 49   8   9   1   0   0   1   2   4  10]\n",
      " [  5 110   0   0   0   0   1   1   2   0]\n",
      " [  6   2  83   0   0   0   1   0   1   1]\n",
      " [  0   0   0 145   0   0   0   0   0   0]\n",
      " [  0   0   0   0 126  14   0   0   0   0]\n",
      " [  0   0   1   0   0 159   0   0   0   0]\n",
      " [  0   0   1   0   0   0 128   0   2   0]\n",
      " [  0   0   0   0   0   0   0  62   0   0]\n",
      " [  2   0   2   1   1   0   4   0 116   0]\n",
      " [ 11   0   5   0   0   1   0   2   1 117]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.75      0.75        73\n",
      "           2       0.92      0.91      0.92       128\n",
      "           3       0.82      0.86      0.84        93\n",
      "           4       0.99      1.00      1.00       146\n",
      "           5       0.97      0.97      0.97       149\n",
      "           6       0.95      0.95      0.95       152\n",
      "           7       0.94      0.95      0.94       154\n",
      "           8       0.97      0.98      0.97        57\n",
      "           9       0.97      0.91      0.94       125\n",
      "          10       0.93      0.92      0.92       121\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1198\n",
      "   macro avg       0.92      0.92      0.92      1198\n",
      "weighted avg       0.93      0.93      0.93      1198\n",
      "\n",
      "0.9307178631051753\n",
      "Confusion matrix, without normalization\n",
      "[[ 55   3   6   1   0   0   1   0   1   6]\n",
      " [  7 117   1   0   0   0   2   0   1   0]\n",
      " [  3   3  80   0   1   1   3   2   0   0]\n",
      " [  0   0   0 146   0   0   0   0   0   0]\n",
      " [  0   0   0   0 145   4   0   0   0   0]\n",
      " [  0   0   1   0   4 145   0   0   0   2]\n",
      " [  3   2   1   0   0   0 146   0   1   1]\n",
      " [  1   0   0   0   0   0   0  56   0   0]\n",
      " [  3   2   2   0   0   0   4   0 114   0]\n",
      " [  2   0   6   0   0   2   0   0   0 111]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.70      0.66        69\n",
      "           2       0.94      0.93      0.94       143\n",
      "           3       0.90      0.82      0.86        97\n",
      "           4       1.00      1.00      1.00       145\n",
      "           5       0.96      0.92      0.94       144\n",
      "           6       0.91      0.97      0.94       148\n",
      "           7       0.92      0.99      0.95       141\n",
      "           8       0.93      0.95      0.94        42\n",
      "           9       0.96      0.93      0.94       142\n",
      "          10       0.96      0.91      0.93       127\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1198\n",
      "   macro avg       0.91      0.91      0.91      1198\n",
      "weighted avg       0.93      0.92      0.93      1198\n",
      "\n",
      "0.9248747913188647\n",
      "Confusion matrix, without normalization\n",
      "[[ 48   8   4   0   0   0   2   1   1   5]\n",
      " [  7 133   0   0   0   0   1   1   1   0]\n",
      " [  9   0  80   0   0   3   3   0   2   0]\n",
      " [  0   0   0 145   0   0   0   0   0   0]\n",
      " [  0   0   0   0 133  11   0   0   0   0]\n",
      " [  0   0   0   0   5 143   0   0   0   0]\n",
      " [  0   0   0   0   0   0 139   0   2   0]\n",
      " [  2   0   0   0   0   0   0  40   0   0]\n",
      " [  2   0   2   0   0   0   5   1 132   0]\n",
      " [  8   0   3   0   0   0   1   0   0 115]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.67      0.70        66\n",
      "           2       0.94      0.95      0.94       124\n",
      "           3       0.92      0.88      0.90       113\n",
      "           4       1.00      1.00      1.00       155\n",
      "           5       0.97      0.94      0.96       159\n",
      "           6       0.92      0.97      0.95       146\n",
      "           7       0.93      0.97      0.95       159\n",
      "           8       0.98      1.00      0.99        47\n",
      "           9       0.96      0.93      0.94       121\n",
      "          10       0.92      0.94      0.93       108\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1198\n",
      "   macro avg       0.93      0.92      0.93      1198\n",
      "weighted avg       0.94      0.94      0.94      1198\n",
      "\n",
      "0.9365609348914858\n",
      "Confusion matrix, without normalization\n",
      "[[ 44   4   5   0   0   0   3   1   3   6]\n",
      " [  5 118   1   0   0   0   0   0   0   0]\n",
      " [  4   1  99   0   2   3   1   0   1   2]\n",
      " [  0   0   0 155   0   0   0   0   0   0]\n",
      " [  0   0   0   0 150   9   0   0   0   0]\n",
      " [  0   0   0   0   3 142   0   0   0   1]\n",
      " [  2   2   0   0   0   0 154   0   1   0]\n",
      " [  0   0   0   0   0   0   0  47   0   0]\n",
      " [  0   1   1   0   0   0   7   0 112   0]\n",
      " [  4   0   2   0   0   0   1   0   0 101]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.58      0.62        67\n",
      "           2       0.90      0.93      0.91       121\n",
      "           3       0.87      0.75      0.81       100\n",
      "           4       1.00      1.00      1.00       131\n",
      "           5       0.58      0.98      0.73       143\n",
      "           6       0.96      0.41      0.58       160\n",
      "           7       0.92      0.88      0.90       140\n",
      "           8       0.93      0.97      0.95        59\n",
      "           9       0.86      0.94      0.90       138\n",
      "          10       0.89      0.91      0.90       139\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1198\n",
      "   macro avg       0.86      0.83      0.83      1198\n",
      "weighted avg       0.86      0.83      0.83      1198\n",
      "\n",
      "0.8347245409015025\n",
      "Confusion matrix, without normalization\n",
      "[[ 39   7   5   0   0   0   3   1   3   9]\n",
      " [  5 112   3   0   0   0   0   1   0   0]\n",
      " [  5   2  75   0   6   0   2   2   3   5]\n",
      " [  0   0   0 131   0   0   0   0   0   0]\n",
      " [  0   0   0   0 140   3   0   0   0   0]\n",
      " [  0   0   0   0  92  66   0   0   0   2]\n",
      " [  1   2   0   0   1   0 123   0  13   0]\n",
      " [  0   0   1   0   0   0   0  57   1   0]\n",
      " [  2   1   0   0   0   0   5   0 130   0]\n",
      " [  7   0   2   0   1   0   0   0   2 127]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.72      0.72        74\n",
      "           2       0.91      0.93      0.92       122\n",
      "           3       0.83      0.84      0.83        87\n",
      "           4       0.98      1.00      0.99       142\n",
      "           5       0.96      0.91      0.93       145\n",
      "           6       0.90      0.96      0.93       144\n",
      "           7       0.90      0.96      0.93       152\n",
      "           8       0.93      1.00      0.96        38\n",
      "           9       0.98      0.87      0.92       148\n",
      "          10       0.93      0.90      0.92       146\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1198\n",
      "   macro avg       0.90      0.91      0.91      1198\n",
      "weighted avg       0.92      0.91      0.91      1198\n",
      "\n",
      "0.9148580968280468\n",
      "Confusion matrix, without normalization\n",
      "[[ 53   5   3   2   0   1   3   0   2   5]\n",
      " [  6 113   0   0   0   0   1   2   0   0]\n",
      " [  3   2  73   0   0   3   1   0   1   4]\n",
      " [  0   0   0 142   0   0   0   0   0   0]\n",
      " [  0   0   2   0 132  10   1   0   0   0]\n",
      " [  0   0   1   0   4 138   0   0   0   1]\n",
      " [  1   1   3   0   0   0 146   1   0   0]\n",
      " [  0   0   0   0   0   0   0  38   0   0]\n",
      " [  3   2   3   1   0   0  10   0 129   0]\n",
      " [  7   1   3   0   2   1   0   0   0 132]]\n",
      "train...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.55      0.64        77\n",
      "           2       0.90      0.97      0.93       132\n",
      "           3       0.44      0.85      0.58        98\n",
      "           4       1.00      1.00      1.00       154\n",
      "           5       0.99      0.92      0.96       143\n",
      "           6       0.95      0.48      0.63       151\n",
      "           7       0.91      0.91      0.91       144\n",
      "           8       0.95      1.00      0.97        39\n",
      "           9       0.92      0.87      0.89       124\n",
      "          10       0.88      0.95      0.91       136\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      1198\n",
      "   macro avg       0.87      0.85      0.84      1198\n",
      "weighted avg       0.89      0.85      0.85      1198\n",
      "\n",
      "0.8497495826377296\n",
      "Confusion matrix, without normalization\n",
      "[[ 42  10   7   0   1   0   1   1   4  11]\n",
      " [  2 128   1   0   0   0   1   0   0   0]\n",
      " [  8   0  83   0   0   0   3   0   1   3]\n",
      " [  0   0   0 154   0   0   0   0   0   0]\n",
      " [  0   0   8   0 132   3   0   0   0   0]\n",
      " [  0   0  78   0   0  72   0   0   0   1]\n",
      " [  0   4   4   0   0   0 131   0   5   0]\n",
      " [  0   0   0   0   0   0   0  39   0   0]\n",
      " [  1   1   3   0   0   0   8   1 108   2]\n",
      " [  2   0   4   0   0   1   0   0   0 129]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.72      0.72        64\n",
      "           2       0.90      0.94      0.92       108\n",
      "           3       0.86      0.82      0.84        91\n",
      "           4       0.99      1.00      1.00       159\n",
      "           5       0.97      0.93      0.95       167\n",
      "           6       0.92      0.98      0.95       161\n",
      "           7       0.90      0.95      0.92       132\n",
      "           8       0.96      0.98      0.97        48\n",
      "           9       0.95      0.88      0.92       141\n",
      "          10       0.95      0.95      0.95       127\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1198\n",
      "   macro avg       0.91      0.91      0.91      1198\n",
      "weighted avg       0.93      0.93      0.93      1198\n",
      "\n",
      "0.9265442404006677\n",
      "Confusion matrix, without normalization\n",
      "[[ 46   3   6   1   0   1   2   0   1   4]\n",
      " [  4 101   1   0   0   0   1   1   0   0]\n",
      " [  5   1  75   0   1   4   2   1   1   1]\n",
      " [  0   0   0 159   0   0   0   0   0   0]\n",
      " [  0   0   2   0 155   9   1   0   0   0]\n",
      " [  0   0   0   0   4 157   0   0   0   0]\n",
      " [  1   3   0   0   0   0 125   0   3   0]\n",
      " [  0   1   0   0   0   0   0  47   0   0]\n",
      " [  4   2   2   0   0   0   8   0 124   1]\n",
      " [  3   1   1   0   0   0   0   0   1 121]]\n",
      "train...\n",
      "test...\n",
      "accurary...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.64      0.65        64\n",
      "           2       0.93      0.93      0.93       136\n",
      "           3       0.87      0.85      0.86       102\n",
      "           4       1.00      1.00      1.00       167\n",
      "           5       0.62      0.97      0.75       144\n",
      "           6       0.90      0.45      0.60       145\n",
      "           7       0.90      0.94      0.92       158\n",
      "           8       0.90      1.00      0.95        38\n",
      "           9       0.93      0.86      0.89       116\n",
      "          10       0.97      0.91      0.94       128\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1198\n",
      "   macro avg       0.87      0.85      0.85      1198\n",
      "weighted avg       0.88      0.86      0.85      1198\n",
      "\n",
      "0.8580968280467446\n",
      "Confusion matrix, without normalization\n",
      "[[ 41   6   5   0   2   2   3   1   2   2]\n",
      " [  5 127   2   0   0   0   0   2   0   0]\n",
      " [  6   0  87   0   3   0   4   0   2   0]\n",
      " [  0   0   0 167   0   0   0   0   0   0]\n",
      " [  0   0   0   0 139   4   1   0   0   0]\n",
      " [  0   0   0   0  80  65   0   0   0   0]\n",
      " [  1   3   1   0   0   0 148   1   4   0]\n",
      " [  0   0   0   0   0   0   0  38   0   0]\n",
      " [  3   1   1   0   1   0   9   0 100   1]\n",
      " [  7   0   4   0   0   1   0   0   0 116]]\n"
     ]
    }
   ],
   "source": [
    "# 平均准确率归零\n",
    "avgscore = 0\n",
    "recallscore = 0\n",
    "precisionscore = 0\n",
    "N = 10\n",
    "\n",
    "# 进行十次KNN测试\n",
    "for i in range(N):\n",
    "    # 以10%的比例进行交叉验证\n",
    "    #X_train, X_test, y_train, y_test = cross_validation.train_test_split(subfeatures, features_labels, test_size=0.1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(subfeatures, labels, test_size=0.1)\n",
    "\n",
    "    # 进行训练\n",
    "    print('train...')\n",
    "    # 进行KNN训练,距离为1\n",
    "    neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "    neigh.fit(X_train, y_train)\n",
    "\n",
    "    # 预试\n",
    "    print('test...')\n",
    "    c_test = neigh.predict(X_test)\n",
    "\n",
    "    # print(y_test)\n",
    "    # print(c_test)\n",
    "\n",
    "    # 计算预测划分准确率\n",
    "    print('accurary...')\n",
    "    score = neigh.score(X_test, y_test)\n",
    "    print(classification_report(y_test,c_test))\n",
    "    avgscore = avgscore + score\n",
    "    recallscore = recallscore + recall_score(y_test,c_test,average=\"macro\")\n",
    "    precisionscore = precisionscore + precision_score(y_test,c_test,average=\"macro\")\n",
    "    print(score)\n",
    "\n",
    "    # 通过混淆矩阵进行结果标示\n",
    "    cm = confusion_matrix(y_test, c_test)\n",
    "    np.set_printoptions(threshold=10000)\n",
    "    np.set_printoptions(precision=2)\n",
    "    print('Confusion matrix, without normalization')\n",
    "    print(str(cm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgscore....\n",
      "0.901752921535893\n",
      "False positive rate\n",
      "0.10132662404481363\n",
      "false negative rate\n",
      "0.10647692406175602\n"
     ]
    }
   ],
   "source": [
    "# 输出N次的平均准确率\n",
    "avgscore = avgscore / N\n",
    "recallscore = recallscore / N\n",
    "precisionscore = precisionscore / N\n",
    "\n",
    "print('avgscore....')\n",
    "print(avgscore)\n",
    "print('False positive rate')\n",
    "print(1-precisionscore)\n",
    "print('false negative rate')\n",
    "print(1-recallscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
