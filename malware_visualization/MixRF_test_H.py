from sklearn.ensemble import RandomForestClassifier as RF
from sklearn import cross_validation
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
from collections import Counter
import pandas as pd
import numpy as np

''' Summary of the file.

    功能：
        实现特征的混合分类方法：
        构造的公式是对随机森林分类后概率矩阵的相加比例：
            proba = proba1*(P) + proba2*(1-P)
        上面，proba1是由GIST特征算出来的分类概率矩阵，proba2是由lbp或dense sift算出来的分类概率矩阵。
        这个权重P是一个不确定的值，不同的混合方法会有不同的最优值。
        最后根据合成的这个概率矩阵proba获得每个样本的分类情况
        
        读取特征值文件和标签文件，通过交叉验证获得训练集和测试集，以随机森林方法划分并获得准确率，通过混淆矩阵标示结果。

    输出：
        结果的混淆矩阵，每次划分的准确率，N次的平均准确率
'''


def getCopySplit(subfeatures, labels, X_train, X_test, y_train, y_test):
    """
    Function: 产生一个与已有交叉验证集合相对应的交叉验证的测试集和训练集，但来自于不同的特征集，同时试集和训练集具有的编号对应
    :param subfeatures: 用于交叉验证的另一个特征群的特征文件
    :param labels: 用于交叉验证的另一个特征群的标签文件
    :param X_train: 此前已经产生的训练集的特征
    :param X_test: 此前已经产生的测试集的特征
    :param y_train: 此前产生的训练集的标签
    :param y_test: 此前产生的测试集的标签
    :return: 与之前交叉对应相对应而不同特征集的交叉验证结果
    """
    X_train2 = subfeatures.loc[X_train.index]
    X_test2 = subfeatures.loc[X_test.index]
    y_train2 = labels.loc[y_train.index]
    y_test2 = labels.loc[y_test.index]

    return (X_train2, X_test2, y_train2, y_test2)


# # 读取两组数据，每组数据含有两种特征。每组数据进行混合分类

# # Test 1 (Gist + lbp)
# # GIST特征值
# subfeatures = pd.read_csv('/usr/local/MATLAB/Data/Gist_feature/gist_f_train_full.csv',header=None)
# labels = pd.read_csv('/usr/local/MATLAB/Data/Gist_feature/CNO_full.txt',header=None)
# # LBP特征值
# subfeatures2 = pd.read_csv('/usr/local/MATLAB/Data/lbp_only_features/feature_train_full.csv',header=None)
# labels2 = pd.read_csv('/usr/local/MATLAB/Data/lbp_only_features/CNO_full.txt',header=None)

# # Test 2 (Gist + dense sift)
# # GIST特征值
# subfeatures = pd.read_csv('/usr/local/MATLAB/Data/Gist_feature/gist_f_train_full.csv',header=None)
# labels = pd.read_csv('/usr/local/MATLAB/Data/Gist_feature/CNO_full.txt',header=None)
# # Dense sift Kmeans100 CenterHist
# subfeaturesTemp = pd.read_csv('/usr/local/MATLAB/Data/dense_sift_Kmeans100_hist100/dense_sift_hist100_full.csv',header=None)
# labelsTemp = pd.read_csv('/usr/local/MATLAB/Data/dense_sift_Kmeans100_hist100/dense_sift_ClassNo_full.csv',header=None)
# # 由于dense sift的特征在文件中的编号方法为字典排序，不是数值排序，这里先进行排序的重置。
# subfeaturesTemp[-1] = labelsTemp[0]
# subfeatures2 = subfeaturesTemp.sort_values([-1],ascending=True).ix[:,0:99]
# labels2 = subfeaturesTemp.sort_values([-1],ascending=True)[-1]
# subfeatures2.index = range(len(subfeatures2))
# labels2.index = range(len(labels2))

# # Test 3 (Gist + lbp) 新的数据集
# GIST特征值
subfeatures = pd.read_csv('/usr/local/MATLAB/Data/Gist_Only_New/gist_f_train_full.csv',header=None)
labels = pd.read_csv('/usr/local/MATLAB/Data/Gist_Only_New/CNO_full.txt',header=None)
# LBP特征值
subfeatures2 = pd.read_csv('/usr/local/MATLAB/Data/Lbp_Only_New/feature_train_full.csv',header=None)
labels2 = pd.read_csv('/usr/local/MATLAB/Data/Lbp_Only_New/CNO_full.txt',header=None)

# # # Test 4 (Gist + dense sift) 新的数据集
# # GIST特征值
# subfeatures = pd.read_csv('/usr/local/MATLAB/Data/Gist_Only_New/gist_f_train_full.csv',header=None)
# labels = pd.read_csv('/usr/local/MATLAB/Data/Gist_Only_New/CNO_full.txt',header=None)
# # Dense sift Kmeans100 CenterHist
# subfeaturesTemp = pd.read_csv('/usr/local/MATLAB/Data/dense_sift_Kmeans100/dense_sift_hist100_full.csv',header=None)
# labelsTemp = pd.read_csv('/usr/local/MATLAB/Data/dense_sift_Kmeans100/dense_sift_ClassNo_full.csv',header=None)
# # 由于dense sift的特征在文件中的编号方法为字典排序，不是数值排序，这里先进行排序的重置。
# subfeaturesTemp[-1] = labelsTemp[0]
# subfeatures2 = subfeaturesTemp.sort_values([-1],ascending=True).ix[:,0:99]
# labels2 = subfeaturesTemp.sort_values([-1],ascending=True)[-1]
# subfeatures2.index = range(len(subfeatures2))
# labels2.index = range(len(labels2))

# 临时修改标签(不变序的数据)
# m = [5,8]       # 旧数据集的家族更改方案
m = [11,12,30,31]       # 新数据集的家族更改方案
first = 0
N = 0

for i in range(len(labels)):
    if(labels[0].get(i) in m) and (labels[0].get(i) != first):
        N = N + 1;
        first = labels[0].get(i)
    labels.ix[i] = labels.ix[i] - N;

# 平均准确率归零 Gist混合比例 实验次数
avgscore = 0
P = 0.5
N = 30


# 进行N次随机森林测试
for i in range(N):
    # 以10%的比例进行交叉验证
    X_train, X_test, y_train, y_test = train_test_split(subfeatures, labels, test_size=0.1)
    X_train2, X_test2, y_train2, y_test2 = getCopySplit(subfeatures2, labels, X_train, X_test, y_train, y_test)
    print("ROUND:",i)

    # 进行训练
    print('train...')
    # 进行随机森林训练,30课树，不限制进程数，为两个混合的特征集各自产生一个随机森林
    srf = RF(n_estimators=30, n_jobs=-1)
    srf.fit(X_train, y_train)
    srf2 = RF(n_estimators=30, n_jobs=-1)
    srf2.fit(X_train2, y_train2)

    # 预试
    print("test...")
    # 构建混合的随机森林概率矩阵
    proba1 = srf.predict_proba(X_test)
    proba2 = srf2.predict_proba(X_test2)
    proba = proba1*(P) + proba2*(1-P)
    # print("Proda")  # 输出随机森林混合概率矩阵
    # print(proba)

    predictions = np.zeros((proba.shape[0]))    # 记录预测分类结果
    Max_allClo = proba.max(axis=1)  # 每个样本分类概率的最大值
    # print("Max_allClo:")  # 输出每个样本分类概率的最大值
    # print(Max_allClo)

    # 将每一个样本分类概率的最大值和随机森林概率矩阵中的概率相比，取得最大概率的分类并记录
    for k in range(proba.shape[0]):
        for n in range(proba.shape[1]):
            if proba[k][n] == Max_allClo[k]:
                predictions[k] = n+1
                # continue

    # print("predictions")  # 输出分类结果
    # print(predictions)

    # 计算预测划分准确率
    score = accuracy_score(y_test, predictions,sample_weight=None)
    print(score)

    # 累加分数
    avgscore = avgscore + score

    # 通过混淆矩阵进行结果标示
    cm = confusion_matrix(y_test, predictions)
    np.set_printoptions(threshold=np.nan)
    np.set_printoptions(precision=2)
    print('Confusion matrix, without normalization')
    print(str(cm))

# 输出十次的平均准确率
avgscore = avgscore / N
print('avgscore....')
print(avgscore)
